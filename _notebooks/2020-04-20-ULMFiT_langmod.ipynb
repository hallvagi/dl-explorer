{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Fine-tuning a Norwegian language model for ULMFiT\"\n",
    "> \"In this post we'll load a pretrained language model and fine tune it on our target corpus of TV and movie reviews.\"\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [NLP, fastai, LSTM, ULMFiT]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous post we explored the [Norec](https://github.com/ltgoslo/norec) Norwegian language corpus of film and TV reviews. In this post I want to use the [ULMFiT](https://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.html) method to predict the sentiment of the reviews based on the text. ULMFiT has three main steps:\n",
    "1. Train a language model on a large general purpose corpus such as Wikipedia\n",
    "1. Fine-tune the language model on the text your are working with - the style is most likely different than a Wikipedia article\n",
    "1. Use the encoder of the fine-tuned language to transform text to feature vectors, and finally add a linear classifier on top to predict the class of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we'll focus on step 1 and 2, the language model and the fine-tuning. Training a language model from scratch is a bit of work. First you have to get the data to train it, and the training will also take a long time. Luckily the the fast.ai [language model zoo](https://forums.fast.ai/t/language-model-zoo-gorilla/14623) already lists a pretrained language model for Norwegian. Note that this is a ULMFiT model zoo, so we expect to find weights for a [AWD-LSTM](https://arxiv.org/abs/1708.02182). See [this post](https://hallvagi.github.io/dl-explorer/nlp/fastai/rnn/lstm/2020/04/17/AWD_LSTM.html) to better understand how to customize an AWD-LSTM with fastai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first grab the dataset from a [previous post](https://hallvagi.github.io/dl-explorer/nlp/fastai/2020/04/06/get-data.html). It's available as a csv from github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>split</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>html/train/000000.html</td>\n",
       "      <td>6</td>\n",
       "      <td>Rome S02</td>\n",
       "      <td>train</td>\n",
       "      <td>positive</td>\n",
       "      <td>Den andre og siste sesongen av Rome er ute på DVD i Norge. Om du så sesong 1, vet du at du har noe stort i vente. Har du aldri sett Rome før, stikk ut og kjøp begge sesongene. Dette er nemlig en av verdens beste tv-serier, og etter å ha sett de fire første episodene av sesong 2, konstaterer jeg at kvaliteten ser ut til å holde seg på et nesten overraskende høyt nivå! Sesong 2 starter nøyaktig der sesong 1 sluttet. Julius Cæsar ligger myrdet i Senatet og Lucius Vorenus hulker over liket av Neobie. Så blir historien enda mørkere. Marcus Antonius tar over styringen av Roma, men utfordres fra ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>html/train/000001.html</td>\n",
       "      <td>6</td>\n",
       "      <td>Twin Peaks - definitive gold box edition</td>\n",
       "      <td>train</td>\n",
       "      <td>positive</td>\n",
       "      <td>Tv-serien Twin Peaks, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen. Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men Twin Peaks sikrer dem evig liv. Serien handler om et mordmysterium i den lille byen Twin Peaks, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>html/train/000002.html</td>\n",
       "      <td>6</td>\n",
       "      <td>The Wire (sesong 1-4)</td>\n",
       "      <td>train</td>\n",
       "      <td>positive</td>\n",
       "      <td>I neste uke kommer sesong 5 av tv-serien ”The Wire” på DVD. 2008 har for meg vært sterkt preget av denne serien. Hjemme hos oss begynte vi med sesong 1 i vår. Da hadde jeg i lengre tid hørt panegyriske lovord om serien fra både venner og media. Vi ble også fanget av skildringene av purk og skurk i Baltimore, og pløyde oss igjennom alt til og med sesong 4 på sensommeren. Jeg vil ikke gå så langt som å kalle det ”verdens beste serie”, som noen har gjort, men det er ingen tvil om at dette er noe av det bedre som er blitt vist på tv! Serien forteller om en gruppe politietterforskere som samles...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 filename  rating                                     title  \\\n",
       "0  html/train/000000.html       6                                  Rome S02   \n",
       "1  html/train/000001.html       6  Twin Peaks - definitive gold box edition   \n",
       "2  html/train/000002.html       6                     The Wire (sesong 1-4)   \n",
       "\n",
       "   split sentiment  \\\n",
       "0  train  positive   \n",
       "1  train  positive   \n",
       "2  train  positive   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \n",
       "0  Den andre og siste sesongen av Rome er ute på DVD i Norge. Om du så sesong 1, vet du at du har noe stort i vente. Har du aldri sett Rome før, stikk ut og kjøp begge sesongene. Dette er nemlig en av verdens beste tv-serier, og etter å ha sett de fire første episodene av sesong 2, konstaterer jeg at kvaliteten ser ut til å holde seg på et nesten overraskende høyt nivå! Sesong 2 starter nøyaktig der sesong 1 sluttet. Julius Cæsar ligger myrdet i Senatet og Lucius Vorenus hulker over liket av Neobie. Så blir historien enda mørkere. Marcus Antonius tar over styringen av Roma, men utfordres fra ...  \n",
       "1  Tv-serien Twin Peaks, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen. Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men Twin Peaks sikrer dem evig liv. Serien handler om et mordmysterium i den lille byen Twin Peaks, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funn...  \n",
       "2  I neste uke kommer sesong 5 av tv-serien ”The Wire” på DVD. 2008 har for meg vært sterkt preget av denne serien. Hjemme hos oss begynte vi med sesong 1 i vår. Da hadde jeg i lengre tid hørt panegyriske lovord om serien fra både venner og media. Vi ble også fanget av skildringene av purk og skurk i Baltimore, og pløyde oss igjennom alt til og med sesong 4 på sensommeren. Jeg vil ikke gå så langt som å kalle det ”verdens beste serie”, som noen har gjort, men det er ingen tvil om at dette er noe av det bedre som er blitt vist på tv! Serien forteller om en gruppe politietterforskere som samles...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/hallvagi/dl-explorer/master/uploads/norec.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained weights we want to use is located in this [repo](https://github.com/mollerhoj/Scandinavian-ULMFiT). There is some information listed here:\n",
    "* The weights were trained on 90% of all text in the corresponding language wikipedia as per 3. July 2018. The remaining 10% was used for validation.\n",
    "* Norwegian: Trained on 80,284,231 tokens, and validated on 8,920,387 tokens. We achieve a perplexity of 26.31\n",
    "\n",
    "And file descriptions:\n",
    "* enc.h5 Contains the weights in 'Hierarchical Data Format'\n",
    "* enc.pth Contains the weights in 'Pytorch model format'\n",
    "* itos.pkl (Integers to Strings) contains the vocabulary mapping from ids (0 - 30000) to strings\n",
    "\n",
    "It looks like we will need the enc.pth (fastai is built on top of PyTorch) and the vocabulary (itos.pkl). But how do we actually load the model? The repo doesn't really specify this part, so let's see if we can figure it out. First we'll download and extract the data to a desired location and have a look at the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('~/.fastai/data/norec') # choses a path of your liking!\n",
    "os.makedirs(path/'models', exist_ok=True)\n",
    "model_url = 'https://www.dropbox.com/s/lwr5kvbxri1gvv9/norwegian.zip'\n",
    "!wget {model_url} -O {path/'models/norwegian.zip'} -q\n",
    "!unzip -q {path/'models/norwegian.zip'} -d {path/'models'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5) [Path('models/norwegian_wgts.h5'),Path('models/norwegian_enc.pth'),Path('models/norwegian.zip'),Path('models/norwegian_enc.h5'),Path('models/norwegian_itos.pkl')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.BASE_PATH = path # paths are printed relative to the BASE_PATH\n",
    "(path/'models').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first file we want to check out is the norwegian_itos.pkl. This the vocabulary of the model, that is, the words and tokens it's able to recognize. *itos* means integer-to-string. The index of a particular token in the list is the key to that token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['_unk_', '_pad_', '.', 'i', ',', 'og', '\\n\\n', 'av', 'som', 'en'],\n",
       " ['learning', 'initiativtager', 'forskningsleder', 'devils', 'graeme'],\n",
       " 30002)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path/'models/norwegian_itos.pkl', 'rb') as f:\n",
    "    itos = pickle.load(f)\n",
    "itos[:10], itos[-5:], len(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first token, i.e. index 0, is \\_unk_ or unknown. The other tokens in the first part of the list are common words such as 'i' (in) and 'og' (and). Among the final tokens there are even some English words. This is not really surprising since Norwegian has \"borrowed\" several words from English. It seems, however, that the special tokens for unknown and padding (\\_unk_ and \\_pad_) are different than the fastai defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj']\n"
     ]
    }
   ],
   "source": [
    "print(defaults.text_spec_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will this cause issues later?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, let's have a look at the weights. We'll load it with pyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['encoder.weight', 'encoder_dp.emb.weight', 'rnns.0.weight_hh_l0_raw', 'rnns.0.module.weight_ih_l0', 'rnns.0.module.weight_hh_l0', 'rnns.0.module.bias_ih_l0', 'rnns.0.module.bias_hh_l0', 'rnns.1.weight_hh_l0_raw', 'rnns.1.module.weight_ih_l0', 'rnns.1.module.weight_hh_l0', 'rnns.1.module.bias_ih_l0', 'rnns.1.module.bias_hh_l0', 'rnns.2.weight_hh_l0_raw', 'rnns.2.module.weight_ih_l0', 'rnns.2.module.weight_hh_l0', 'rnns.2.module.bias_ih_l0', 'rnns.2.module.bias_hh_l0'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = torch.load(path/'models/norwegian_enc.pth')\n",
    "enc.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a dictionary with keys, and the keys are the names of the layers of the model. We can see that is has an embedding layer (named 'encoder'), and three RNNs(LSTMs more precisely) with various descriptions. We recognize the three layer LSTM from the [AWD-LSTM](https://arxiv.org/abs/1708.02182) and the [ULMFiT](https://arxiv.org/abs/1801.06146) paper. So we must make sure that the model we set up matches matches this. Let's have a look at the dimensions of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.weight  \t torch.Size([30002, 400])\n",
      "encoder_dp.emb.weight  \t torch.Size([30002, 400])\n",
      "rnns.0.weight_hh_l0_raw  \t torch.Size([4600, 1150])\n",
      "rnns.0.module.weight_ih_l0  \t torch.Size([4600, 400])\n",
      "rnns.0.module.weight_hh_l0  \t torch.Size([4600, 1150])\n",
      "rnns.0.module.bias_ih_l0  \t torch.Size([4600])\n",
      "rnns.0.module.bias_hh_l0  \t torch.Size([4600])\n",
      "rnns.1.weight_hh_l0_raw  \t torch.Size([4600, 1150])\n",
      "rnns.1.module.weight_ih_l0  \t torch.Size([4600, 1150])\n",
      "rnns.1.module.weight_hh_l0  \t torch.Size([4600, 1150])\n",
      "rnns.1.module.bias_ih_l0  \t torch.Size([4600])\n",
      "rnns.1.module.bias_hh_l0  \t torch.Size([4600])\n",
      "rnns.2.weight_hh_l0_raw  \t torch.Size([1600, 400])\n",
      "rnns.2.module.weight_ih_l0  \t torch.Size([1600, 1150])\n",
      "rnns.2.module.weight_hh_l0  \t torch.Size([1600, 400])\n",
      "rnns.2.module.bias_ih_l0  \t torch.Size([1600])\n",
      "rnns.2.module.bias_hh_l0  \t torch.Size([1600])\n"
     ]
    }
   ],
   "source": [
    "for k,v in enc.items():\n",
    "    print(k,\" \\t\", v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the hidden size is different than the fastai default of 1152, but apart from that everything looks fine. Let's save a few weights from the embedding layer to compare with our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5711, 0.2321, 0.2601, 0.9425, 0.0901])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights = enc['encoder.weight'][0][:5]\n",
    "sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the weights into our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to make sure that our data loader uses our custom vocabulary instead of doing tokenization on its own, so we pass `text_vocab = itos`. We also set `is_lm = True` since we want a language model and not a classifier. We use the basic factory method, since we have no need of customization at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_unk_ _unk_ film : i likhet med _unk_ alejandro _unk_ gonzález _unk_ _unk_ « _unk_ » er _unk_ olivier _unk_ _unk_ ' « _unk_ _unk_ maria » et forsøk på å _unk_ spørsmålet om hva som skjer med _unk_ i en tid der de ambisiøse _unk_ _unk_ til tv - skjermen og amerikanske _unk_ dominerer både _unk_ og _unk_ . _unk_ men der « _unk_ » er en high _unk_ - film</td>\n",
       "      <td>_unk_ film : i likhet med _unk_ alejandro _unk_ gonzález _unk_ _unk_ « _unk_ » er _unk_ olivier _unk_ _unk_ ' « _unk_ _unk_ maria » et forsøk på å _unk_ spørsmålet om hva som skjer med _unk_ i en tid der de ambisiøse _unk_ _unk_ til tv - skjermen og amerikanske _unk_ dominerer både _unk_ og _unk_ . _unk_ men der « _unk_ » er en high _unk_ - film med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>på først : _unk_ fordi den franske byen _unk_ calais ligger ved kysten på det stedet hvor _unk_ den engelske kanal er på sitt smaleste , har den blitt invadert av illegale innvandrere fra land som _unk_ afghanistan og _unk_ irak , som etter en _unk_ ferd fra hjemlandet vil forsøke å krysse kanalen og nå _unk_ storbritannia . _unk_ ofte skjuler de seg i lastebiler , noen forsøker å svømme over</td>\n",
       "      <td>først : _unk_ fordi den franske byen _unk_ calais ligger ved kysten på det stedet hvor _unk_ den engelske kanal er på sitt smaleste , har den blitt invadert av illegale innvandrere fra land som _unk_ afghanistan og _unk_ irak , som etter en _unk_ ferd fra hjemlandet vil forsøke å krysse kanalen og nå _unk_ storbritannia . _unk_ ofte skjuler de seg i lastebiler , noen forsøker å svømme over .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>et stort og _unk_ glimt i øyet . _unk_ de spiller aldri hovedrollen selv . _unk_ det er noen år siden herrene _unk_ nils _unk_ _unk_ og _unk_ ronny _unk_ kristoffersen gjorde « _unk_ » . _unk_ nå er de tilbake igjen . _unk_ godt er det . i ni programmer følger vi _unk_ nils og _unk_ ronny gjennom _unk_ colombia , _unk_ gaza , _unk_ england og usas _unk_ _unk_ _unk_</td>\n",
       "      <td>stort og _unk_ glimt i øyet . _unk_ de spiller aldri hovedrollen selv . _unk_ det er noen år siden herrene _unk_ nils _unk_ _unk_ og _unk_ ronny _unk_ kristoffersen gjorde « _unk_ » . _unk_ nå er de tilbake igjen . _unk_ godt er det . i ni programmer følger vi _unk_ nils og _unk_ ronny gjennom _unk_ colombia , _unk_ gaza , _unk_ england og usas _unk_ _unk_ _unk_ _unk_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm = TextDataLoaders.from_df(df, text_col='text', text_vocab=itos, is_lm=True, valid_pct=0.1)\n",
    "dls_lm.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good. We can recognize our \\_unk_ token for example. We also see that the label column, that is the \\\"text_\" column, is offset by 1 token from the input. This makes sense since the goal of a language model is to predict the next word in a sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to configure the AWD-LSTM architecture. Let's have a look at the default config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emb_sz': 400,\n",
       " 'n_hid': 1152,\n",
       " 'n_layers': 3,\n",
       " 'pad_token': 1,\n",
       " 'bidir': False,\n",
       " 'output_p': 0.1,\n",
       " 'hidden_p': 0.15,\n",
       " 'input_p': 0.25,\n",
       " 'embed_p': 0.02,\n",
       " 'weight_p': 0.2,\n",
       " 'tie_weights': True,\n",
       " 'out_bias': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_lm_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these look good, but we will change the n_hid to 1150. Note also the pad_token=1. This is the index of the padding token, and from our itos above we see that `itos[1] = _pad_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "awd_lstm_lm_config['n_hid'] = 1150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass the config to our learner object. Notice that we set `pretrained=False`, we want to load our own weights. The final `.to_fp16()` means that the model is trained with mixed precision (16 bit floating point) which can often speed up training quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm = language_model_learner(dls_lm, \n",
    "                               arch=AWD_LSTM, \n",
    "                               metrics=[accuracy, Perplexity()], \n",
    "                               path=path, \n",
    "                               config=awd_lstm_lm_config, \n",
    "                               pretrained=False).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary now looks correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(30002, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(30002, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=30002, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_lm.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of our model have been initialized randomly, so they should not match at the moment. Let's compare our sample weights from the above section with those from our language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0005, -0.0117,  0.0255,  0.0342,  0.0455]),\n",
       " tensor([0.5711, 0.2321, 0.2601, 0.9425, 0.0901]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_lm.model.state_dict()['0.encoder.weight'][0][:5].cpu(), sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we should be able to load the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai2.text.learner.LMLearner at 0x7f518b3bfc10>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_lm.load_encoder(path/'models/norwegian_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! We can also see that the weights match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5711, 0.2321, 0.2601, 0.9425, 0.0901]),\n",
       " tensor([0.5711, 0.2321, 0.2601, 0.9425, 0.0901]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_lm.model.state_dict()['0.encoder.weight'][0][:5].cpu(), sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But are we able to predict any useful text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'xxunk' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d3bf635eccab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hovedstaden i Norge er'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/fastai2/fastai2/text/learner.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text, n_words, no_unk, temperature, min_p, no_bar, decoder, only_last_word)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mno_unk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munk_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mno_bar\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'xxunk' is not in list"
     ]
    }
   ],
   "source": [
    "learn_lm.predict('Hovedstaden i Norge er') # the captical of norway is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the problem now? We see that `predict()` by default has `no_unk=True`. The error message tells us that the library tries to get the index of the UNK token. The UNK token is as we noted earlier different in our `itos` (vocabulary) than that what the library expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxunk', '_unk_')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNK, itos[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not really a problem for our model. The models only sees the underlying numbers and indexes, and they are still correct. But if we want to use most of the convenience functions of the fastai2 library, we either have to customise the code, or simpler still, change the vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace special tokens in vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's look through our itos and see if we can find any special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj']\n"
     ]
    }
   ],
   "source": [
    "print(defaults.text_spec_tok) # fastai defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for tokens that contains an underscore **\\_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unk_', '_pad_', 't_up', 'tk_rep', 'formula_1']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_toks = [token for token in itos if '_' in token]\n",
    "_toks[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then for tokens that begin with an **x**. We use a simple regex to check for x in the beginning of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xfld', 'xbos', 'x', 'xii', 'xi']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_toks = [token for token in itos if re.match(r'^x', token) != None]\n",
    "x_toks[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'_unk_', '_pad_', 'xfld', 'xbos' seems pretty obvious. But I'm less sure of eg. 't_up' and 'tk_rep'. So we replace a bit conservatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_unk_', '_pad_', 'xfld', 'xbos']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_replace = _toks[:2]+x_toks[:2]\n",
    "to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk', 'xxpad', 'xxup', 'xxbos']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_with = defaults.text_spec_tok[:2]+defaults.text_spec_tok[7:8]+defaults.text_spec_tok[2:3]\n",
    "replace_with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we loop trough our itos and replace the selected tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok_remove, tok_insert in zip(to_replace, replace_with):\n",
    "    idx = itos.index(tok_remove)\n",
    "    itos[idx] = tok_insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that we did things correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 31, 32]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = [itos.index(token) for token in replace_with]\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk', 'xxpad', 'xxup', 'xxbos']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[itos[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model: final cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make yet another version of our dataloader and language learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<fastai2.text.learner.LMLearner at 0x7f511dc73b50>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls_lm = TextDataLoaders.from_df(df, text_col='text', text_vocab=itos, is_lm=True, valid_pct=0.1)\n",
    "learn_lm = language_model_learner(dls_lm, arch=AWD_LSTM, metrics=[accuracy, Perplexity()], \n",
    "                               path=path, config=awd_lstm_lm_config, pretrained=False).to_fp16()\n",
    "learn_lm.load_encoder(path/'models/norwegian_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['hovedstaden i norge er en by med stor politisk status fra 1891 til 1955 . \\n \\n 1 havene \\n\\n havene ( engelsk \" american mountains \" , engelsk : \" mountain peninsula \" eller \" t_up ngc \" eller \" øvre',\n",
       " 'hovedstaden i norge er \\n \\n 1 hans - herman \\n\\n hans - günther ( født 28 . desember 1812 i rostock , død 6 . desember 1888 i leipzig ) var en tysk økonom , som var general og generaldirektør for',\n",
       " 'hovedstaden i norge er oppkalt etter ham . i tillegg er det ett av norges høyeste byer og omegn . i norge er det også en rekke andre land : \\n\\n historie . \\n det er i dag i norge ikke lenger en kommunal']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = \"Hovedstaden i Norge er\" # the capital of norway is\n",
    "preds = [learn_lm.predict(TEXT, 40, temperature=0.75) for _ in range(3)]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that kind of makes sense. LSTMs are less capable at generating text than the more complex transformer architectures, but our concern in this particular case is how well we eventually do sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to fine tune our language model. We'll use the \"standard\" training regime from the documentation and the fast.ai courses. That is 1 epoch where only the linear layers in the head of the model are trainable, and finally 10 epochs with all layers unfrozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.02089296132326126, lr_steep=1.3182567499825382e-06)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZCcd33n8fe3r7kPHaPTsuRDFsaAbVl2cFwhZiEEO4C5ijWQIhwVYyBks6mkQipVSSW7JKlcu4AreA0JgQ2wCeJYAwZM2JgYiIklbONTlizbaCzNLU1Pz/Td3/3jeUYaj0ejGWme7qenP6+qrul++ul+vmpJ/Znf8fwec3dERKR1JRpdgIiINJaCQESkxSkIRERanIJARKTFKQhERFqcgkBEpMWlGl3Acq1fv9537NjR6DJERJrK/v37x9x9YKHnmi4IduzYwb59+xpdhohIUzGzZ0/3nLqGRERanIJARKTFKQhERFqcgkBEpMUpCEREWpyCQESkxSkIRESawHcfG+bQyFQk760gEBGJOXfng5/fz979z0Xy/goCEZGYK5RrlKtOb0c05wArCEREYm6qUAagtz0dyfsrCEREYi47GwQdCgIRkZY0ma8A0NOuriERkZaUVdeQiEhrmyoELYI+DRaLiLSmbD5oEfSoRSAi0prUNSQi0uKy+QrppNGejuYrW0EgIhJzU4Uyve1pzCyS91cQiIjEXLZQiWzqKCgIRERiL5svR3YyGSgIRERib7ZrKCqRBYGZ7TKzB+fcsmb2W/P2MTP7uJkdMrOfmtnuqOoREWlW2UIlsgXnACJ7Z3c/AFwBYGZJ4Dngq/N2uwHYGd5+Dvhk+FNERELZfJmetiZsEczzKuApd3923vabgM954D6g38w216kmEZGmkC2UI20R1CsIbga+uMD2rcCROY8Hw23PY2a3mNk+M9s3OjoaUYkiIvFTqtQolGvNOUYwy8wywBuALy309ALb/AUb3O9w9z3uvmdgYGClSxQRia2piJeghvq0CG4AfuLuwws8Nwhsm/P4POBoHWoSEWkK2UK0S1BDfYLg7SzcLQRwJ/CucPbQy4FJdz9Wh5pERJpC1FcngwhnDQGYWSfwS8D752y7FcDdbwfuAm4EDgEzwHuirEdEpNlkw4vSRNk1FGkQuPsMsG7ettvn3HfgQ1HWICLSzE5dprK5u4ZEROQsRX0tAlAQiIjE2uzVyXqbfLBYRETOUrZQJmHQlVEQiIi0pGy+THdbikQimmsRgIJARCTWggXnohsfAAWBiEisRb0ENSgIRERiLZuPdglqUBCIiMRatlCOdOooKAhERGJtqlBR15CISCsLrlesriERkZZUrTlTRbUIRERaVq4OS1CDgkBEJLaydbgoDSgIRERiK1uHaxGAgkBEJLZOXYtAXUMiIi2pHlcnAwWBiEhsZU8uQa0gEBFpSbMXpVHXkIhIi5odLO5uUxCIiLSkqUKFrkySVDLar2oFgYhITAXLS0Q7PgAKAhGR2MrW4VoEoCAQEYmtqUIl8uUlQEEgIhJb2YK6hkREWlo2X6FXLQIRkdalFoGISAtzd40RiIi0splSlWrNNWtIRKRV1etaBKAgEBGJpZNLUKtFICLSmk7MlADoa/YWgZn1m9leM3vCzB43s2vnPX+9mU2a2YPh7Q+jrEdEpFlMTAdBsLYrE/mxoh6O/hjwbXd/q5llgM4F9rnX3V8XcR0iIk1lPAyCdd1NHARm1gu8Ang3gLuXgFJUxxMRWU1mWwRrOqMPgii7hi4ERoHPmNkDZvZpM+taYL9rzewhM/uWmV0WYT0iIk1jPFektz1FJhX9UG6UR0gBu4FPuvuVwDTwkXn7/ATY7u6XA58AvrbQG5nZLWa2z8z2jY6ORliyiEg8jE+XWNfdVpdjRRkEg8Cgu/84fLyXIBhOcvesu+fC+3cBaTNbP/+N3P0Od9/j7nsGBgYiLFlEJB4mpkt1GSiGCIPA3YeAI2a2K9z0KuCxufuY2SYzs/D+NWE941HVJCLSLOoZBFHPGvow8PlwxtBh4D1mdiuAu98OvBX4gJlVgDxws7t7xDWJiMTe+HSJK7b11+VYkQaBuz8I7Jm3+fY5z98G3BZlDSIizaZWcyamS3WZOgo6s1hEJHayhTLVmrO2q/kHi0VE5CycPJms2QeLRUTk7NRzeQlQEIiIxM54rn7LS4CCQEQkdsaniwCs0xiBiEhrmghbBGu6ol+CGhQEIiKxMz5doqctRVsqWZfjKQhERGJmYrrE2jqND4CCQEQkdsani3WbOgoKAhGR2BnPlep2MhkoCEREYmdiuqQWgYhIq3J3js9ojEBEpGVlCxXKVVeLQESkVY3nwpPJ1CIQEWlNp9YZ0mCxiEhLqvfKo6AgEBGJlXqvPAoKAhGRWFEQiIi0uLFcke62FO3p+qwzBAoCEZFYmZgu1bU1AAoCEZFYURCIiLS48Vx9l5cABYGISKyoRSAi0sLcPViCurt+J5OBgkBEJDamivVfZwgUBCIisTF7rWJ1DYmItKjZ5SXquQQ1KAhERGJjogHrDMESg8DMLjKztvD+9Wb2m2bWH21pIiKt5dQS1PEcLP4yUDWzi4G/Ay4AvhBZVSIiLWhsNgji2CIAau5eAd4E/E93/6/A5ujKEhFpPUPZAv2d6bquMwRLD4Kymb0d+DXgG+G29JleZGb9ZrbXzJ4ws8fN7Np5z5uZfdzMDpnZT81s9/LKFxFZPYYmi2zqba/7cZcaBO8BrgU+6u5Pm9kFwD8u4XUfA77t7i8CLgcen/f8DcDO8HYL8Mkl1iMisuoMZfNs6qt/EKSWspO7Pwb8JoCZrQF63P3PF3uNmfUCrwDeHb5HCSjN2+0m4HPu7sB9YQtis7sfW9afQkRkFRiaLPLSrX11P+5SZw3dY2a9ZrYWeAj4jJn9zRlediEwGu77gJl92sy65u2zFTgy5/FguE1EpKWUKjXGckU2xrhrqM/ds8Cbgc+4+1XAq8/wmhSwG/iku18JTAMfmbePLfA6n7/BzG4xs31mtm90dHSJJYuINI+RqQJArMcIUma2GXgbpwaLz2QQGHT3H4eP9xIEw/x9ts15fB5wdP4bufsd7r7H3fcMDAws8fAiIs1jOBsGQQPGCJYaBH8CfAd4yt3vN7MLgYOLvcDdh4AjZrYr3PQq4LF5u90JvCucPfRyYFLjAyLSio5NNi4IljpY/CXgS3MeHwbesoSXfhj4vJllgMPAe8zs1vA9bgfuAm4EDgEzBLOTRERaztBk47qGlhQEZnYe8AngOoI+/B8A/8XdBxd7nbs/COyZt/n2Oc878KHlFCwishoNTRZoTyfo6zjjKVorbqldQ58h6MbZQjCr5+vhNhERWQFD2QKbetsxW2gOTbSWGgQD7v4Zd6+Et38ANGorIrJChrOFhowPwNKDYMzMftXMkuHtV4HxKAsTEWklxyYLDRkfgKUHwXsJpo4OAceAt6KBXRGRFVGrOSPZIhvj3CJw95+5+xvcfcDdN7j7GwlOLhMRkXM0MVOiVK2xOeYtgoX89opVISLSwoYaeA4BnFsQ1H9oW0RkFZo9q7gR6wzBuQXBC9YEEhGR5Zs9q3hzX0dDjr/oCWVmNsXCX/gGNKZiEZFVZjhbIGGwvru+l6ictWgQuHtPvQoREWlVxyYLDPS0kUqeSyfN2WvMUUVE5KTgZLLGdbIoCEREGmxossCm3raGHV9BICLSYEOThYYNFIOCQESkoXLFClPFSsOmjoKCQESkoU6dTKauIRGRlnTyEpW96hoSEWlJjV5eAhQEIiINNZRt3CUqZykIREQaaGiyQF9Hmo5MsmE1KAhERBro6Ik8mxvYLQQKAhGRhnpyZIqLN3Q3tAYFgYhIg0wXKxyZyLNrY2OXdVMQiIg0yMGRHACXbFIQiIi0pCeHpgDUIhARaVUHhqdoTyfYtrazoXUoCEREGuTJ4Sl2bughmWjslX8VBCIiDXJgaIpLGtwtBAoCEZGGOD5dYmSqyK5NjZ06CgoCEZGGeHI4GChWi0BEpEXNBsGuBk8dBQWBiEhDHBieoqct1dDF5mYpCEREGuDJoRyXbOrBrLEzhiDiIDCzZ8zsYTN70Mz2LfD89WY2GT7/oJn9YZT1iIjEgbtzYDgeM4YAUnU4xivdfWyR5+9199fVoQ4RkVgYnSoymS+za2PjZwyBuoZEROruwOyMoRgMFEP0QeDA3Wa238xuOc0+15rZQ2b2LTO7bKEdzOwWM9tnZvtGR0ejq1ZEpA4OxGSNoVlRdw1d5+5HzWwD8F0ze8Ld/23O8z8Btrt7zsxuBL4G7Jz/Ju5+B3AHwJ49ezzimkVEIvXk8BTruzOs625rdClAxC0Cdz8a/hwBvgpcM+/5rLvnwvt3AWkzWx9lTSIijXZgOBebgWKIMAjMrMvMembvA68BHpm3zyYL506Z2TVhPeNR1SQi0mi1mnMwRjOGINquoY3AV8Pv+RTwBXf/tpndCuDutwNvBT5gZhUgD9zs7ur6EZFV6+hknplSlZ0xmTEEEQaBux8GLl9g++1z7t8G3BZVDSIicTN7VbKdG+LTItD0URGROjo0PBsE8WkRKAhEROro4EgwY2hNV6bRpZykIBARqaODIzkujlFrABQEIiJ14+4cGs7FanwAFAQiInUznC0yVazEasYQKAhEROrm4EiwtIS6hkREWtTB4fhNHQUFgYhI3RwcydHfmWZ9d3xmDIGCQESkbg6NTLFzQ3csrko2l4JARKQO3D2cOhqvbiFQEIiI1MX4dIkTM+VYnVE8S0EgIlIHJweKYzZ1FBQEIiJ1cSicOhq3GUOgIBARqYuDIzl62lJs7I3HVcnmUhCIiNTBweEcF2+M34whUBCIiNTFwZFcLAeKQUEgIhK549MlxnLFWI4PgIJARCRy+549DsBlW3sbXMnCFAQiIhH7/pMjdGWS7Nm+ttGlLEhBICISIXfnngOj/PzF68mk4vmVG8+qRERWicNj0wwez/OLlww0upTTUhCIiETongOjAAoCEZFWdc+BES4a6GLb2s5Gl3JaCgIRkYjkS1V+/PQE1+/a0OhSFqUgEBGJyH2HxylValy/K77dQqAgEBGJzPefHKUjneTqHfGcNjpLQSAiEpF7Doxw7UXraE8nG13KohQEIiIReGZsmmfGZ2I9W2iWgkBEJAL/8vgwQOzHB0BBICISia8+8BwvO6+P7eu6Gl3KGSkIRERW2IGhKR49muXNV25tdClLEmkQmNkzZvawmT1oZvsWeN7M7ONmdsjMfmpmu6OsR0SkHr7ywCCphPH6y7c0upQlSdXhGK9097HTPHcDsDO8/RzwyfCniEhTqtacrz3wHNfvGmBdd/wuS7mQRncN3QR8zgP3Af1mtrnBNYmInLV/f2qc4WyRN115XqNLWbKog8CBu81sv5ndssDzW4Ejcx4Phtuex8xuMbN9ZrZvdHT0rAqZLlb4+kNHcfezer2IyFJ85SeD9LSneNWl8V5WYq6og+A6d99N0AX0ITN7xbznF7qK8wu+qd39Dnff4+57BgbObirWtx4Z4sNffID94ZWCRERW2nSxwrcfHeJ1L9sc+5PI5oo0CNz9aPhzBPgqcM28XQaBbXMenwccjaKWG16yic5Mkr37B6N4exERvvPoEDOlKm/e3TzdQhBhEJhZl5n1zN4HXgM8Mm+3O4F3hbOHXg5MuvuxKOrpaktxw0s2882fHiNfqkZxCBFpYdWa86l7n2b7uk72bF/T6HKWJcoWwUbgB2b2EPAfwDfd/dtmdquZ3RrucxdwGDgEfAr4YIT18JartjJVrHD3Y0NRHkZEWtA/3X+Ex49l+d1f3oXZQr3e8RXZ9FF3PwxcvsD22+fcd+BDUdUw38svWMfW/g727h/kpiua40QPEYm/yXyZv7r7ANdcsJZfeWnzTXxs9PTRukokjLfs3soPDo1xbDLf6HJEZJX4+PcOcnymxB+9/sVN1xqAFgsCgLdcdR7uwTogIiLn6tBIjs/+6Bluvvp8LtvS1+hyzko9ziyOle3rurh6xxr27h/kA7940bLTe7pY4cjxGQYn8hQrNVJJI500ylVncqbMiXyJbL5CqVqjVKlRc+fiDd28dGsfl27ubaopZSKyuFrN+eOvP0pHJsnvvOaSRpdz1louCADeetV5/N6XH+Y7jw7x2pcs3p83Xaxw78Ex/uXxYb7/5CijU8Uzvn/CoC2VJJ00HJgqVABIJYwt/R1s7mtnS38HG3rbGOhuY113hr6ONLUa1NwpVWsMZ4scO5FneKrIlv52rjp/Dbu3r2F9hKesV6o1potVpopl3KE9naQ9naAzkyKZaL7mrkjU7rj3MPceHOO/vfElTbOcxEKs2c603bNnj+/b94L165ZlqlDmdZ/4Ac+Oz/DqSzfykRtexPZ1nTx6NMv9T0/wxNAUxybzDE0WGDyep1St0deR5hcvGeBFm3vYtqaTbWs76UgnKVdrVGpOKmH0d6bp78zQlUmebGm4O0PZAg8dmeTh505wZCLPsck8R08UGJ0qUqrWTltnezrBhp52jk3mKVeDv6c1nWl6O9L0tqfp70yzvruNgZ421ndn6O/M0N+RDkLFoVCuki9XKVdruIPjGEYqaaQSCUrVGo8eneThwUkePZplMl9esI5kwtjU286W/nbWdmWYKVWZKlSYLlZIJRN0pBN0ZJKs6cywqbedTX3t9HWkSSbs5C2VCI6ZTNjzTiPsbU+d/DMkzJiYLjExXeLETJlcMTjGTLkK7mCGEQRqMhH8OdyhVKlRqtaoVJ1UMjymGWZgBMdrSyXCW5JEwnB3HEia0dWWpDOToiOdxAmmAbo7qWSCdNLIpBKkEwmSyeDPkUkmSCWf36vq7hQrNZIJI51suR7XlvTjw+O849M/5rWXbeK2d1wZ+7EBM9vv7nsWfK4VgwCCL8m//+HT/O2/PkW+XCWTTJAvB+cXzH7pbe7rYNvaTq7fNcCe7Wte8J//XLk7U8UKY1NFsoUKCYOEBV9wG3va6e9MY2YUylUeeW6S/c8eZ/B4nmyhTDZf5vhMmbFckZGpIqXK6QNlMZlkgks393DZ1j429LTR056mpy0FFnxGhXKVEzNljk0WeO5EnuPTJTrbUvS2p+jKpKjU/GTgTEyXODaZp1A+u1qaSU97iv7ONJ3pFJP5MhMzpZN/B8mE0Z4KwjFoVSXpyiTp7UjTE35uqWSCZAJSiQS9HWn6O4Jgd4eZcpV8qUIqkWBtV4Y1XRl621MkzEiY4Tj5UpWZ8DZ7zHTS6EgHx+nrSLOm6/m/lMjKGcsVufFj99LVluLO37iOnvZ0o0s6IwXBIsZyRT5172GK5RpX71jL1TvWsKG3fcXevx7cnVyxwomZMidmykzmyyQTFn4RJUgnExhgFvwmXKk55Wrw2+sF67toS63cuIW7k81XyBbKVGtO1Z1qzalUw5+1UyHhQDZfZixXYnSqSM2dtV2Z4MuvM0NXW5LuthQdmSRG8AWIQ9WD9ytXayQs+I09k0qQSlhwzFrwZ/SwHncoV2sUKzUK5Sq18J+8WfDb/0ypykyxQr5cJRG2JBIWvFepUqNYrVENW341D/YPPusSM6Uq/Z1p1nRm6O1IU6s5hUqVQrl2MiAL5Sq5YpWpMMCni9WTn0u5UiNXqhDVf8P2dOJki2tjT9Ba29DbxvruoBW5rquNnvYUbekkbalEcH8F/z2sRsVKlff+w/3se+Y4X/3gdbx4S2+jS1oSBYFIjFVrTjZf5kS+TMKgIxN0VVWqtZNdZVOFCrUw1BzozCTDWwqzIOgq1SCkJvOzLcYSY7kiY7kSI1MFRrJFhrKFk2NWCzGDbWs6uXhDNzvWdbGmM01fZ9DC2Njbztb+Djb2tpNJtWb310i2wPv/cT8P/OwEf/HWl/G2PdvO/KKYWCwIWnKwWCROkgljTdgFNF9/Z4YLV/iStzOlCuO5UyExXaxQrFQpVmqM5UocHs1xaCTHfYfHT3Y9zWUGW/o6uGRjNzs39nDRQBfb1naybU0nm/vaV7wLNS4ePHKC9//vfWTzFf72nbu5sQlPHDsdBYFIi+nMpOhcm2Lb2s4z7lusVMnmK0zmSwxNFjk6mee543meHpvm4EiOHz41/rzxqUwqwWVberly2xquOL+fnWHLoiPTvN1N08UKn7r3MH97z1Ns6GnjKx/8eS7d3BzdQUulriEROWuVao2jJwocOT7DkYkZDo3keGjwBD8dnKQ4JyA29bZz4UAXFw10c/GGbnZu6ObSzb0LtoLiolSp8U/3/4yPfe8gY7kSN750E//9jS9lbYxrXoy6hkQkEqlkgvPXdXL+uue3LsrVGk8OT/H02DRPj07z9Pg0h0en+doDzzFVPDVGsbmvnRdt6mHH+i52rOti+7pgavbW/o6GnHw5VShz78ExvvPoEP/viRGmChWuuWAtn3rXi7jy/OZaUXQ5FAQisuLSyQSXbel7wZIL7s7oVJEDw1M8djTL48eyHBjO8eOnJ14wHrGhp43z13ayPQyI89d2sqW/g61rOtjY03bOYxGT+TLPjE3z1GiOB4+cYN8zx3liKEvNg/N1XnvZJl5/+RZ+Yef6VT8FV11DItJw7s5orsiz40EX0+DxPEcmZnh2Yoafjc8wlC284DVtqcTzZk7NnkDYGZ6z0duepj2dCM+/gHLNmcgFs7BGc0Umpksn36szk+TK8/u5avtafv6idZGcN9Ro6hoSkVgzMzb0tLOhp52rd6x9wfP5UpXnTuR57kSeoyfyjGSLzJQqJ0+qmz1TfPY8j2y+zODxGUqV2sntyYSxtjPD9nWd7N6+hh3rOtmxvosL13dxwfquVffFvxwKAhGJvY5Mkos3BAPNsvJaNwJFRARQEIiItDwFgYhIi1MQiIi0OAWBiEiLUxCIiLQ4BYGISItTEIiItLimW2LCzEaBZ8OHfcDkIvfnb0sDY8s85Nz3WMpz87cttcbZn+uXWWO96pvdps8wXvU1Q41xr+9calxsW9w+w+3uvvDVLYJL+TXnDbhjsfvztwH7zuUYS3lu/ral1jjn57JqrFd9+gzjWV8z1Bj3+s6lxjPUGqvPcLFbs3cNff0M90/3/NkeYynPzd+21BrjXt+ZjrUYfYZnPs5izvS6uNcY9/pO9/xSajzTtuWI+jM8rabrGjoXZrbPT7P6XlzEvca41wfxrzHu9UH8a4x7fdAcNc5q9hbBct3R6AKWIO41xr0+iH+Nca8P4l9j3OuD5qgRaLEWgYiIvFCrtQhERGQeBYGISItTEIiItDgFQcjMfsHMbjezT5vZjxpdz0LMLGFmHzWzT5jZrzW6nvnM7Hozuzf8HK9vdD0LMbMuM9tvZq9rdC0LMbNLw89vr5l9oNH1LMTM3mhmnzKz/2tmr2l0PfOZ2YVm9ndmtrfRtcwK/919Nvzc3tnoeuZbFUFgZn9vZiNm9si87a81swNmdsjMPrLYe7j7ve5+K/AN4LNxrBG4CdgKlIHBGNbnQA5oj2l9AL8H/PNK1raSNbr74+G/w7cBKz71cIVq/Jq7/zrwbuA/x7C+w+7+vpWsayHLrPXNwN7wc3tD1LUt23LOfIvrDXgFsBt4ZM62JPAUcCGQAR4CXgy8lODLfu5tw5zX/TPQG8cagY8A7w9fuzeG9SXC120EPh/D+l4N3EzwBfa6OP4dh695A/Aj4B1xrTF83V8Du2Nc34r+HznHWn8fuCLc5wtR1nU2t1Vx8Xp3/zcz2zFv8zXAIXc/DGBm/we4yd3/DFiwW8DMzgcm3T0bxxrNbBAohQ+rcatvjuNAW9zqM7NXAl0E/zHzZnaXu9fiVGP4PncCd5rZN4EvrFR9K1WjmRnw58C33P0ncauvXpZTK0EL+TzgQWLYE7MqguA0tgJH5jweBH7uDK95H/CZyCp6oeXW+BXgE2b2C8C/RVlYaFn1mdmbgV8G+oHboi0NWGZ97v4HAGb2bmBsJUNgEcv9DK8n6EZoA+6KtLJTlvvv8MMEras+M7vY3W+PsjiW/xmuAz4KXGlmvx8GRr2crtaPA7eZ2a9w9ktQRGY1B4EtsG3Rs+fc/Y8iquV0llWju88QhFW9LLe+rxCEVb0s++8YwN3/YeVLOa3lfob3APdEVcxpLLfGjxN8sdXLcusbB26NrpxFLViru08D76l3MUsVuybKChoEts15fB5wtEG1nE7ca1R95041nru41zdXM9V60moOgvuBnWZ2gZllCAYJ72xwTfPFvUbVd+5U47mLe31zNVOtpzR6tHqFRu+/CBzj1LTK94XbbwSeJBjF/wPVqPpUY7xrjHt9zVrrmW5adE5EpMWt5q4hERFZAgWBiEiLUxCIiLQ4BYGISItTEIiItDgFgYhIi1MQyKpgZrk6H+/TZvbiFXqvqpk9aGaPmNnXzaz/DPv3m9kHV+LYIqCL18sqYWY5d+9ewfdLuXtlpd7vDMc6WbuZfRZ40t0/usj+O4BvuPtL6lGfrH5qEciqZWYDZvZlM7s/vF0Xbr/GzH5kZg+EP3eF299tZl8ys68Dd1twxbV7LLha2BNm9vlwCWbC7XvC+zkLrhz3kJndZ2Ybw+0XhY/vN7M/WWKr5d8JVrDEzLrN7Htm9hMze9jMbgr3+XPgorAV8Zfhvr8bHuenZvbHK/gxSgtQEMhq9jHgf7j71cBbgE+H258AXuHuVwJ/CPzpnNdcC/yau/+n8PGVwG8RXMPgQuC6BY7TBdzn7pcTLA/+63OO/7Hw+GdceMzMksCrOLU2TQF4k7vvBl4J/HUYRB8BnnL3K9z9dy24XOROgrXwrwCuMrNXnOl4IrNW8zLUIq8GXhz+Eg/Qa2Y9QB/wWTPbSbCccXrOa77r7hNzHv+Huw8CmNmDwA7gB/OOUyK4OhbAfuCXwvvXAm8M738B+KvT1Nkx5733A98Ntxvwp+GXeo2gpbBxgde/Jrw9ED7uJgiGelyzQlYBBYGsZgngWnfPz91oZp8A/tXd3xT2t98z5+npee9RnHO/ysL/Z8p+arDtdPssJu/uV5hZH0GgfIhgvf93AgPAVe5eNrNnCK4HPZ8Bf+bu/2uZxxUB1DUkq9vdwG/MPjCzK8K7fcBz4f13R3j8+wi6pCBYjnhR7j4J/CbwO2aWJqhzJFyp3fEAAADRSURBVAyBVwLbw12ngJ45L/0O8F4zmx1w3mpmG1bozyAtQEEgq0WnmQ3Ouf02wZfqnnAA9TFOXbXqL4A/M7MfElxsPCq/Bfy2mf0HsBmYPNML3P0Bggue3wx8nqD+fQStgyfCfcaBH4bTTf/S3e8m6Hr6dzN7GNjL84NCZFGaPioSETPrJOj2cTO7GXi7u990pteJ1JvGCESicxXBBcsNOAG8t8H1iCxILQIRkRanMQIRkRanIBARaXEKAhGRFqcgEBFpcQoCEZEWpyAQEWlx/x8AUugixm9UfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_lm.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learning rate of 1e-2 seem to be a safe choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.189573</td>\n",
       "      <td>3.936918</td>\n",
       "      <td>0.315999</td>\n",
       "      <td>51.260372</td>\n",
       "      <td>02:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_lm.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will unfreeze all layers and train for 10 epochs at a reduced learning rate. The idea is that once we unfreeze the lower layers of our model we should make smaller changes to avoid catastrophic forgetting. We will go for a leraning rate of 1e-3. One can always test if longer training improves results, but in this case we will simply assumes that 10 epochs is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.0003019951749593019, lr_steep=6.309573450380412e-07)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_lm.unfreeze()\n",
    "learn_lm.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.997230</td>\n",
       "      <td>3.822397</td>\n",
       "      <td>0.327288</td>\n",
       "      <td>45.713673</td>\n",
       "      <td>02:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.954266</td>\n",
       "      <td>3.777179</td>\n",
       "      <td>0.330856</td>\n",
       "      <td>43.692589</td>\n",
       "      <td>02:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.878042</td>\n",
       "      <td>3.725606</td>\n",
       "      <td>0.337251</td>\n",
       "      <td>41.496380</td>\n",
       "      <td>02:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.814440</td>\n",
       "      <td>3.686008</td>\n",
       "      <td>0.340614</td>\n",
       "      <td>39.885315</td>\n",
       "      <td>02:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.785215</td>\n",
       "      <td>3.656684</td>\n",
       "      <td>0.344700</td>\n",
       "      <td>38.732697</td>\n",
       "      <td>02:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.729483</td>\n",
       "      <td>3.635671</td>\n",
       "      <td>0.346845</td>\n",
       "      <td>37.927280</td>\n",
       "      <td>02:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.698734</td>\n",
       "      <td>3.618694</td>\n",
       "      <td>0.348383</td>\n",
       "      <td>37.288837</td>\n",
       "      <td>02:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.654537</td>\n",
       "      <td>3.607326</td>\n",
       "      <td>0.350624</td>\n",
       "      <td>36.867321</td>\n",
       "      <td>02:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.635801</td>\n",
       "      <td>3.602736</td>\n",
       "      <td>0.351033</td>\n",
       "      <td>36.698505</td>\n",
       "      <td>02:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.618312</td>\n",
       "      <td>3.601928</td>\n",
       "      <td>0.351133</td>\n",
       "      <td>36.668865</td>\n",
       "      <td>02:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_lm.fit_one_cycle(10, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We're only training for about 30 minutes on a single GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the model and the encoder for future use. The `save()` method save objects to path/'models' by default. The encoder will be used in our classifier in the next post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.save('finetuned_model')\n",
    "learn_lm.save_encoder('finetuned_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the model any good at predicting text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['xxunk denne filmen er et godt eksempel på hvordan serien er blitt laget med en amerikansk komedie . i filmen står det en tydelig og effektiv følelse av at en serie med tretten mennesker , som har mistet kontrollen over de to første episodene , har fått en',\n",
       " 'xxunk denne filmen er et godt eksempel på hva som skjer når den kommer i nærheten av en tsunami . i verste fall skjer det med den virkelige verden , hvor det som skjer , er det som gjør at den blir rammet av et stadig forvirrende ,',\n",
       " 'xxunk denne filmen er et godt eksempel på at det er vanskelig å forestille seg hva som egentlig foregår . i visse scener er dette en underholdende og ytterst underholdende film , i norsk forstand . JON GORE « the THAT IN']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = \"Denne filmen er et godt eksempel på\" # this film is a good example of\n",
    "preds = [learn_lm.predict(TEXT, 40, temperature=0.75) for _ in range(3)]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is by no means as impressive as the recent transformer models, but the model certainly understands language fairly well. Also, our particular use case isn't really text generation, but sentiment classification. Transformers only do marginally better than ULMFiT according to [Papers with code](https://paperswithcode.com/sota/sentiment-analysis-on-imdb) on the similar IMDB classification task. Classification will be the topic of an upcoming post."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "319.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
