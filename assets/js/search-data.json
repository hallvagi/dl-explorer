{
  
    
        "post0": {
            "title": "Exploring attention",
            "content": "from fastai2.vision.all import * import altair as alt from itertools import product . Finding a proper case study for attention . I have been looking into attention and transformers lately, and wanted to explore it with an example data set - solving a problem is the best way to learn in my opinion. Most use cases I come across, however, seem to be huge language models, which are just too complex to be a good starting point for learning. One day this popped up in my twitter feed, and seemed like just the right thing for further examination: . To illustrate attention mechanisms, I made a toy task seq2seq task and implemented an attention layer from scratch. It worked beautifully (thread) . &mdash; François Fleuret (@francoisfleuret) May 19, 2020 . The code below creates the actual data which we will investigate in this post. The code is written by François Fleuret, which was kind to let met reuse the code. I&#39;ve modified it slightly so that it fits the notebook format. You can find the original code here. . #collapse # Author: François Fleuret seq_height_min, seq_height_max = 1.0, 25.0 seq_width_min, seq_width_max = 5.0, 11.0 seq_length = 100 def positions_to_sequences(tr = None, bx = None, noise_level = 0.3): st = torch.arange(seq_length).float() st = st[None, :, None] tr = tr[:, None, :, :] bx = bx[:, None, :, :] xtr = torch.relu(tr[..., 1] - torch.relu(torch.abs(st - tr[..., 0]) - 0.5) * 2 * tr[..., 1] / tr[..., 2]) xbx = torch.sign(torch.relu(bx[..., 1] - torch.abs((st - bx[..., 0]) * 2 * bx[..., 1] / bx[..., 2]))) * bx[..., 1] x = torch.cat((xtr, xbx), 2) # u = x.sign() u = F.max_pool1d(x.sign().permute(0, 2, 1), kernel_size = 2, stride = 1).permute(0, 2, 1) collisions = (u.sum(2) &gt; 1).max(1).values y = x.max(2).values return y + torch.rand_like(y) * noise_level - noise_level / 2, collisions def generate_sequences(nb, group_by_locations=False): # Position / height / width tr = torch.empty(nb, 2, 3) tr[:, :, 0].uniform_(seq_width_max/2, seq_length - seq_width_max/2) tr[:, :, 1].uniform_(seq_height_min, seq_height_max) tr[:, :, 2].uniform_(seq_width_min, seq_width_max) bx = torch.empty(nb, 2, 3) bx[:, :, 0].uniform_(seq_width_max/2, seq_length - seq_width_max/2) bx[:, :, 1].uniform_(seq_height_min, seq_height_max) bx[:, :, 2].uniform_(seq_width_min, seq_width_max) if group_by_locations: a = torch.cat((tr, bx), 1) v = a[:, :, 0].sort(1).values[:, 2:3] mask_left = (a[:, :, 0] &lt; v).float() h_left = (a[:, :, 1] * mask_left).sum(1) / 2 h_right = (a[:, :, 1] * (1 - mask_left)).sum(1) / 2 valid = (h_left - h_right).abs() &gt; 4 else: valid = (torch.abs(tr[:, 0, 1] - tr[:, 1, 1]) &gt; 4) &amp; (torch.abs(tr[:, 0, 1] - tr[:, 1, 1]) &gt; 4) input, collisions = positions_to_sequences(tr, bx) if group_by_locations: a = torch.cat((tr, bx), 1) v = a[:, :, 0].sort(1).values[:, 2:3] mask_left = (a[:, :, 0] &lt; v).float() h_left = (a[:, :, 1] * mask_left).sum(1, keepdim = True) / 2 h_right = (a[:, :, 1] * (1 - mask_left)).sum(1, keepdim = True) / 2 a[:, :, 1] = mask_left * h_left + (1 - mask_left) * h_right tr, bx = a.split(2, 1) else: tr[:, :, 1:2] = tr[:, :, 1:2].mean(1, keepdim = True) bx[:, :, 1:2] = bx[:, :, 1:2].mean(1, keepdim = True) targets, _ = positions_to_sequences(tr, bx) valid = valid &amp; ~collisions tr = tr[valid] bx = bx[valid] input = input[valid][:, None, :] targets = targets[valid][:, None, :] if input.size(0) &lt; nb: input2, targets2, tr2, bx2 = generate_sequences(nb - input.size(0)) input = torch.cat((input, input2), 0) targets = torch.cat((targets, targets2), 0) tr = torch.cat((tr, tr2), 0) bx = torch.cat((bx, bx2), 0) return input, targets, tr, bx . . We use the above code to create our data set: 25 000 and 1000 train and test samples, respectively, of 1D sequences with a length of 100: . torch.manual_seed(42) train_input, train_targets, train_tr, train_bx = generate_sequences(25000) test_input, test_targets, test_tr, test_bx = generate_sequences(1000) train_input.shape, train_targets.shape, test_input.shape, test_targets.shape . (torch.Size([25000, 1, 100]), torch.Size([25000, 1, 100]), torch.Size([1000, 1, 100]), torch.Size([1000, 1, 100])) . Our input data is simply a series of floats: . print(train_input[0,0,:5].tolist()) . [0.13880640268325806, 0.0006390661001205444, 0.0356530100107193, -0.10253779590129852, -0.014928251504898071] . Let&#39;s also plot a pattern to better understand what our task is going to be: . #collapse def plot_pattern(inp, target, input_type=&#39;input&#39;, ax=None, include_targ = True, legend=True): if not ax: fix, ax = plt.subplots() inp = to_np(tensor(inp).squeeze()) target = to_np(tensor(target).squeeze()) ax.plot(inp, label=input_type) if include_targ: ax.plot(target, label=&#39;target&#39;) if legend: ax.legend(); . . plot_pattern(train_input[123], train_targets[123]) . The target output should be the average height of the input for corresponding geometric shape:The two yellow squares represent the average height of the blue squares, and similary for the triangles. . Basic cnn model . Our first attempt at solving this task will be a regular (1d) convnet. We will make it with the fastai library. The architecture and training will be identical to the one François used in his example (see tweet above). . Dataloaders . In fastai the dataloader holds both the input and the target data. So let&#39;s combine our input and target data to a combined training dataset. This step makes the creation of the dataloader a bit easier. . train = torch.cat((train_input, train_targets), dim=-1) train.shape . torch.Size([25000, 1, 200]) . We also want to normalize our data, so we&#39;ll create a normalize helper function. . mean, std = train.mean(), train.std() def norm(x, m, s): return (x-m)/s normalize = partial(norm, m=mean, s=std) . Finally we make our dataloader with the datablock api. Think of this as an assembly line which we pass our data source through: . dls = DataBlock( get_x = lambda row: row[:, :100], # the first 100 cols of a row hold our training data get_y = lambda row: row[:, 100:], # and the final 100 cols in the row holds our target data splitter=RandomSplitter(valid_pct=0.2), batch_tfms=normalize ).dataloaders(source=train, bs=128) . Next we will grab a batch from the dataloader, and expect to find the data normalized: . xb, yb = next(iter(dls.train)) xb.shape, yb.shape, xb.mean().item(), xb.std().item() . (torch.Size([128, 1, 100]), torch.Size([128, 1, 100]), 0.004318716004490852, 1.0494749546051025) . We get a batch of 128 items both for our input and target, with zero mean and unit variance. We can also plot an item from the our batch to verify that everything is ok: . plot_pattern(xb[0], yb[0]) . Create a learner . This is the same model that Francois used in his example: . #collapse def get_model_cnn(ks, stride=1): return sequential( nn.Conv1d(1, 64, kernel_size=ks, stride=stride, padding=ks//2), nn.ReLU(), nn.Conv1d(64, 64, kernel_size=ks, stride=stride, padding=ks//2), nn.ReLU(), nn.Conv1d(64, 64, kernel_size=ks, stride=stride, padding=ks//2), nn.ReLU(), nn.Conv1d(64, 64, kernel_size=ks, stride=stride, padding=ks//2), nn.ReLU(), nn.Conv1d(64, 1, kernel_size=ks, stride=stride, padding=ks//2), ) . . basic_conv = get_model_cnn(ks=5) learn_cnn = Learner(dls, model=basic_conv, loss_func=MSELossFlat()) . Our cnn model has about 62 000 parameters: . learn_cnn.summary() . Sequential (Input shape: [&#39;128 x 1 x 100&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv1d 128 x 64 x 100 384 True ________________________________________________________________ ReLU 128 x 64 x 100 0 False ________________________________________________________________ Conv1d 128 x 64 x 100 20,544 True ________________________________________________________________ ReLU 128 x 64 x 100 0 False ________________________________________________________________ Conv1d 128 x 64 x 100 20,544 True ________________________________________________________________ ReLU 128 x 64 x 100 0 False ________________________________________________________________ Conv1d 128 x 64 x 100 20,544 True ________________________________________________________________ ReLU 128 x 64 x 100 0 False ________________________________________________________________ Conv1d 128 x 1 x 100 321 True ________________________________________________________________ Total params: 62,337 Total trainable params: 62,337 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f47ac2085f0&gt; Loss function: FlattenedLoss of MSELoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Training . We&#39;ll train our model for 50 epochs with a flat learning rate of 1e-3. It&#39;s a small model, so training won&#39;t take very long. We could definitively train for longer without waiting all day. Fastai prints out the metrics from every epoch by default, so we&#39;ll use the no_logging() context manager to make the output a bit more compact: . with learn_cnn.no_logging(): learn_cnn.fit(50, 1e-3) . learn_cnn.recorder.plot_loss(skip_start=100) . Our model seems to be learning something, but how good are the predictions? . Check a few predictions . Let&#39;s check a few predictions from the test set. We&#39;ll add the test set to our data loader first. Note that the test_dl() method expects a numpy ndarray, so we transform our input tensors with to_np() before passing it: . test_dl = learn_cnn.dls.test_dl(to_np(test_input)) preds_cnn, _ = learn_cnn.get_preds(dl=test_dl) preds_cnn.shape . torch.Size([1000, 1, 100]) . Then we grab a few random indexes to plot. We&#39;ll be reusing these for other models later: . idxs = [84, 701, 27, 493] . The predictions aren&#39;t particulary good at this point: . fig, axs = plt.subplots(2,2, figsize=(12,9)) for ax, idx in zip(axs.flatten(), idxs): plot_pattern(preds_cnn[idx], normalize(test_targets[idx]), input_type=&#39;prediction&#39;, ax=ax) . Sanity check our model . We didn&#39;t get great results from the above convnet. Is the task too hard or is our model broken in some way? A standard way of checking the health of a model is to try to overfit a single batch. If the model can&#39;t do this something funky is going on. . Let&#39;s take 20 items and see if we&#39;re ablet to overfit that. We&#39;ll use the standard PyTorch training loop: . model = get_model_cnn(ks=5) optimizer = Adam(model.parameters(), lr=1e-3) mse_loss = MSELossFlat() inp, target = normalize(train[:20,:,:100]), normalize(train[:20,:,100:]) for epoch in range(5001): optimizer.zero_grad() output = model(inp) loss = mse_loss(output, target) if epoch%1000==0: print(f&#39;epoch: {epoch}, loss: {loss.item()}&#39;) loss.backward() optimizer.step() . epoch: 0, loss: 0.9717220664024353 epoch: 1000, loss: 0.016162781044840813 epoch: 2000, loss: 0.006243061739951372 epoch: 3000, loss: 0.003895669477060437 epoch: 4000, loss: 0.0020476507488638163 epoch: 5000, loss: 0.0005493065691553056 . The loss is slowly approaching zero, so I it seems that our model is working. But it needs a crazy number of epochs to overfit just 20 items, so doing well on the entire data set would be very difficult. There are many things we do to improve the model - make it deeper or wider, or perhaps increasing the kernel size would help? But there is also this thing called attention! . Attention . Attention has become a central concept in deep learning with the rise of the transformer architecture. There are several good resources that explains the concept. I particularly like the following: . The Rasa white board video series on attention. Higly reccomended! | Jay Allamar&#39;s blog has several posts on attentions and transformers | Yannick Kilcher has lot&#39;s of videos on deep learning papers, including a playlist for NLP | Lilian Weng has a nice blog with a few posts on attention and transformers | Peter Bloem has a nice from-scratch implementation of the transformer in PyTorch | The annotated Transformer by Harvard NLP, and the Attention is All You Need paper | The annotated GPT-2 by Aman Aroa | The rest of the post will assume a basic understanding of the concept of attention (specifically self-attention). Below is the implementation of an attention layer and the proposed model architecture - very similar to the above cnn architecture except for a new AttentionLayer. This code is once again written by François, but I added a few comments on intermediate tensor shapes. Let&#39;s go through it step by step. . #collapse class AttentionLayer(nn.Module): def __init__(self, in_channels, out_channels, key_channels): super(AttentionLayer, self).__init__() self.conv_Q = nn.Conv1d(in_channels, key_channels, kernel_size = 1, bias = False) self.conv_K = nn.Conv1d(in_channels, key_channels, kernel_size = 1, bias = False) self.conv_V = nn.Conv1d(in_channels, out_channels, kernel_size = 1, bias = False) def forward(self, x): # x.shape = [bs x in_channels x seq_len] Q = self.conv_Q(x) # [bs x key_channels x seq_len] K = self.conv_K(x) # [bs x key_channels x seq_len] V = self.conv_V(x) # [bs x out_channels x seq_len] A = Q.permute(0, 2, 1).matmul(K).softmax(2) # [bs x seq_len x seq_len] x = A.matmul(V.permute(0, 2, 1)).permute(0, 2, 1) # [bs x out_channels x seq_len] return x def __repr__(self): return self._get_name() + &#39;(in_channels={}, out_channels={}, key_channels={})&#39;.format( self.conv_Q.in_channels, self.conv_V.out_channels, self.conv_K.out_channels ) def attention(self, x): Q = self.conv_Q(x) K = self.conv_K(x) return Q.permute(0, 2, 1).matmul(K).softmax(2) def get_model_attention(ks, stride=1): return sequential( nn.Conv1d(1, 64, kernel_size=ks, stride=stride, padding=ks//2), nn.ReLU(), nn.Conv1d(64, 64, kernel_size=ks, stride=stride, padding=ks//2), nn.ReLU(), AttentionLayer(in_channels=64, out_channels=64, key_channels=96), ## New attention layer nn.ReLU(), nn.Conv1d(64, 64, kernel_size=ks, stride=stride, padding=ks//2), nn.ReLU(), nn.Conv1d(64, 1, kernel_size=ks, stride=stride, padding=ks//2) ) . . def __(init)__(): . Our class has a nn.Module superclass, so first we initialize the class properly. . We proceed to initialize our Q, K and V matrices. Note that conv_K and conv_Q have identical dimensions: in_channels x key_channels. We&#39;ll look into why this is a requirement later in the post. conv_V, however, is different: in_channels x out_channels. Q, K and V often have the same shape in other implementations, see e.g. the fast.ai nlp course implementation, but this depends on the overall architecture that the attention layer is part of. In this particular case our attention layer is part of a specific architecture where the number of out channels need to conform to the expected number of input channels in the next layer. Finally, if Q, K and V do have identical shape we can also stack them in a single matrix and thereby improve performance. . Why are Q, K and V conv layers? . Note that we use Conv1d layers with kernel_size=1. This means that a conv-layer is identical to a linear layer. Let&#39;s take some dummy data to compare a Conv1d(ks=1) and a regular nn.Linear() and see if we can change the number of channels of our input from 8 to 12: . # bs x n_channels x seq_len x = torch.rand(16,8,50) x.shape . torch.Size([16, 8, 50]) . If we pass x through a conv layer with kernel_size=1 and no bias, we get our desired output: . nn.Conv1d(in_channels=8, out_channels=12, kernel_size=1, bias=False)(x).shape . torch.Size([16, 12, 50]) . We can achieve the same with a linear layer. But the linear layer expects input where the final dimension represents the input&#39;s channel dimension. And similar for the output. To compare with our conv layer we need to permute the data in each step: . out = nn.Linear(in_features=8, out_features=12, bias=False)(x.permute(0,2,1)) out.permute(0,2,1).shape . torch.Size([16, 12, 50]) . So using a Conv1d with ks=1 is the same as a linear layer, but seems to avoid a few annoying permutations. Huggingface&#39;s implementation of attention in GPT-2 uses a conv1D for Q, K and V for example. . Why don&#39;t we scale the attention scores? . There are many variations of attention, see Lillian Weng&#39;s post for an overview. Our implementation is a so called dot product attention. But the Attention is All You Need paper uses a scaled dot product attention instead. This seem to have become the norm in subsequent transformers, and the argument is that scaling will improve the gradient signal especially for long input sequences. In this particular example it probably doesn&#39;t matter much anyway since we have a fairly simple architecture and small input data. . def forward() . In the forward method we calculate the matrix product of our conv_q, conv_k and conv_v matrices with our input x. Since these are conv-layers we can just feed x to them, and the superclass&#39; forward method will do the matrix multiplication automatically. We proceed to calculate the Attention, A by doing a dot-product with each Q and K pair of the sequence and finally doing a softmax of the final dimension (2 in this case) to normalize the attention scores. But why do we need to permute k before the matrix multiplication? Let&#39;s walk through an example step by step. . First we create sample Q, K and V conv layers: . conv_Q = nn.Conv1d(4, 8, kernel_size=1, bias=False) conv_K = nn.Conv1d(4, 8, kernel_size=1, bias=False) conv_V = nn.Conv1d(4, 16, kernel_size=1, bias=False) conv_K.weight.shape, conv_Q.weight.shape, conv_V.weight.shape . (torch.Size([8, 4, 1]), torch.Size([8, 4, 1]), torch.Size([16, 4, 1])) . X is our mock input data. it has a batch size of 1 (keep it simple), a channel size of 8, and a sequence length of 50: . x = torch.rand((1, 4, 50)) x.shape . torch.Size([1, 4, 50]) . Then we calculate our Q, K and V matrices by feeding x to them: . qx = conv_Q(x) kx = conv_K(x) vx = conv_V(x) qx.shape, kx.shape, vx.shape . (torch.Size([1, 8, 50]), torch.Size([1, 8, 50]), torch.Size([1, 16, 50])) . Now we want to get the dot product of each key and value pair for every item in the sequence for the entire batch. If we ignore the batch dimension, qx and kx are shaped like 8x50 @ 8x50. If we permute the final dimension of the former we instead get 50x8 @ 8x50, which are valid dimensions for a matrix multiplication. As expected the output is seq_len x seq_len: . qx.permute(0, 2, 1).matmul(kx).shape . torch.Size([1, 50, 50]) . Note that if we instead permute the last matrix we get the wrong result! . qx.matmul(kx.permute(0, 2, 1)).shape . torch.Size([1, 8, 8]) . Finally we calculate our attention scores by normalizing with a softmax along the seq_len dimension, in this case the second dimension: . A = qx.permute(0, 2, 1).matmul(kx).softmax(2) A.shape . torch.Size([1, 50, 50]) . We can verify that the softmax normalized our attention scores to 1 for each item in the sequence: . A.sum(-1).mean().item() . 1.0 . Finally we want to multiply A with our learned values vx, but once again our matrices don&#39;t align: . A.shape, vx.shape . (torch.Size([1, 50, 50]), torch.Size([1, 16, 50])) . When we permute vx and do the matrix multiply with A we get the contextualized values for our input sequence. But the seq_len dimension has been switched: . A.matmul(vx.permute(0, 2, 1)).shape . torch.Size([1, 50, 16]) . We have to permute once more to get our desired result. We also see that the resulting shape is independent of the 8 key_channels in Q and K: . A.matmul(vx.permute(0, 2, 1)).permute(0, 2, 1).shape . torch.Size([1, 16, 50]) . Note that the shape of our contextualized x is different than our input x: . x.shape . torch.Size([1, 4, 50]) . This is simply due to the fact that our attention layer is the middle layer of a particular architecture and the number of in_channels and out_channels have to match the characteristics of the architecture. In many self-attention implementations, we need to return a contextualized input with identical shape as the original input. . permute vs transpose . The code permutes the matrices several times. This is necessary to align the matrices properly for matrix multiplication, and is similar to transposing. Note that we can transpose by listing dimensions explicitly or relatively. The latter is perhaps a bit more robust to changing input data types, but the former is maybe easier to read: . tmp = torch.rand(3,4,5) tmp.permute(0,2,1).shape, tmp.permute(0, -1,-2).shape . (torch.Size([3, 5, 4]), torch.Size([3, 5, 4])) . Transpose gives us the same result: . tmp.transpose(1,2).shape, tmp.transpose(-1, -2).shape . (torch.Size([3, 5, 4]), torch.Size([3, 5, 4])) . def __repr__() . This method simply gives the module a string representation for printing. We can se the result in learn.model() below, which prints the layers of the model. . def attention() . This method outputs the same as the A in the forward. This gives us a convenient way to inspect the attention scores for a particular input x. . CNN + attention . We make a new model and train it in a similar manner as our original cnn implementation above. Note that it has slightly fewer parameters than our original cnn, 58k vs 62k. . attention_model = get_model_attention(ks=5) learn_att = Learner(dls, model=attention_model, loss_func=MSELossFlat()) learn_att.summary() . Sequential (Input shape: [&#39;128 x 1 x 100&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv1d 128 x 64 x 100 384 True ________________________________________________________________ ReLU 128 x 64 x 100 0 False ________________________________________________________________ Conv1d 128 x 64 x 100 20,544 True ________________________________________________________________ ReLU 128 x 64 x 100 0 False ________________________________________________________________ Conv1d 128 x 96 x 100 6,144 True ________________________________________________________________ Conv1d 128 x 96 x 100 6,144 True ________________________________________________________________ Conv1d 128 x 64 x 100 4,096 True ________________________________________________________________ ReLU 128 x 64 x 100 0 False ________________________________________________________________ Conv1d 128 x 64 x 100 20,544 True ________________________________________________________________ ReLU 128 x 64 x 100 0 False ________________________________________________________________ Conv1d 128 x 1 x 100 321 True ________________________________________________________________ Total params: 58,177 Total trainable params: 58,177 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f47ac2085f0&gt; Loss function: FlattenedLoss of MSELoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . learn_att.model . Sequential( (0): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,)) (1): ReLU() (2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,)) (3): ReLU() (4): AttentionLayer(in_channels=64, out_channels=64, key_channels=96) (5): ReLU() (6): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,)) (7): ReLU() (8): Conv1d(64, 1, kernel_size=(5,), stride=(1,), padding=(2,)) ) . Train model . Let&#39;s train our new attention model for a similar number of epochs and the same learning rate as our basic_conv model: . with learn_att.no_logging(): learn_att.fit(50, 1e-3) . learn_att.recorder.plot_loss(skip_start=100) . Wow! The loss is much lower this time around! . Check a few predictions . We&#39;ll reuse the idxs and the test_dl dataloader to inspect a few predictions. The predictions from the attention model are much better than our previous cnn-model: . preds_att, _ = learn_att.get_preds(dl=test_dl) preds_att.shape fig, axs = plt.subplots(2,2, figsize=(12,9)) for ax, idx in zip(axs.flatten(), idxs): plot_pattern(preds_cnn[idx], normalize(test_targets[idx]), input_type=&#39;prediction cnn&#39;, ax=ax, include_targ=False, legend=False) plot_pattern(preds_att[idx], normalize(test_targets[idx]), input_type=&#39;prediction cnn+attention&#39;, ax=ax, legend=False) handles, labels = ax.get_legend_handles_labels() fig.legend(handles, labels, loc=&#39;lower center&#39;, ncol=3); . Check attention matrix . Another fascinating thing with attention is that we can inspect the attention matrix. This means we can see which parts of the input sequence that the model thinks is most important for it&#39;s contextualized output sequence. In order to get it we have to: . Run a batch from the test set through the a part of the model up to the attention layer | Grab the attention layer and run it&#39;s attention-method to get the attention matrix | We can inspect the individual parts of our model with the .childeren() method, and also slice the model into separate parts: . list(attention_model.children()) . [Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,)), ReLU(), Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,)), ReLU(), AttentionLayer(in_channels=64, out_channels=64, key_channels=96), ReLU(), Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,)), ReLU(), Conv1d(64, 1, kernel_size=(5,), stride=(1,), padding=(2,))] . Let&#39;s grab the first part and the attention layer: . base_model = attention_model[0:4] attention_layer = attention_model[4] base_model, attention_layer . (Sequential( (0): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,)) (1): ReLU() (2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,)) (3): ReLU() ), AttentionLayer(in_channels=64, out_channels=64, key_channels=96)) . Now we pass our normalized test_input through the first part of the model, ensuring it&#39;s on the same device as our model: . base_output = base_model(normalize(test_input).to(&#39;cuda&#39;)) base_output.shape . torch.Size([1000, 64, 100]) . We then take the output and run it through the attention() method from our attention layer: . attention = attention_layer.attention(base_output) attention.shape . torch.Size([1000, 100, 100]) . As expected we get a 100x100 attention matrix for each item in the test set. Let&#39;s check the attention scores of the first id in idxs from our predictions above: . sample = attention[idxs[0]] idxs[0], sample.shape . (84, torch.Size([100, 100])) . In order to plot this we&#39;ll convert the data into so called long format. That means reshaping the data from a 100x100 matrix to a 100*100 x 3 matrix which has one observation per row. We&#39;ll add this to a dataframe. This step makes plotting a bit easier. . df = pd.DataFrame(list(product(range(100), range(100))), columns=[&#39;input&#39;, &#39;output&#39;]) df[&#39;attention&#39;] = to_np(sample.reshape(-1,1).squeeze()) df.head() . input output attention . 0 0 | 0 | 0.005647 | . 1 0 | 1 | 0.017142 | . 2 0 | 2 | 0.019656 | . 3 0 | 3 | 0.011353 | . 4 0 | 4 | 0.010529 | . Let&#39;s first have a look at the particular task we try to solve: . plot_pattern(test_input[idxs[0]], test_targets[idxs[0]]) . And then the attention-matrix from the model&#39;s prediction of that sample: . alt.data_transformers.disable_max_rows() alt.Chart(df).mark_rect().encode( x = alt.X(&#39;input:O&#39;, axis=alt.Axis(values=list(range(0, 100,10)))), y = alt.Y(&#39;output:O&#39;, axis=alt.Axis(values=list(range(0, 100,10))), sort=&#39;descending&#39;), color=alt.Color(&#39;attention:Q&#39;, scale=alt.Scale(scheme=&#39;viridis&#39;)) ).properties(height=500, width=500) . Attention is high (yellowish) when the output sequence pays the most attention to the input sequence. There is a solid pattern of rectangles to rectangles and triangles to triangles! . LSTM . After the arrival of the fancier transformer, LSTMs seem kind of old school. But on certain tasks they do almost as well as transformer based models, IMDb being one such case. Anyway, let&#39;s write a custom module for a fairly vanilla LSTM. I really recommend the chapter on LSTMs in Deep Learning for Coders with fastai and PyTorch, also available on github, for understanding LSTMs. The implementation below is similar to the one in the book. Also, checkout the LSTM documentation from PyTorch. . We will go for a 1 layer LSTM with a single linear layer to produce the output. We also need standard LSTM particulars such as model resetting and gradient truncation - check the fastai book for details. Finally, note that the fast.ai Module is very similar to nn.module, but without the need for the boilerplate super().__init__(): . class LSTM(Module): def __init__(self, dim_in, n_out, n_hidden, n_layers): a_in, b_in = dim_in # input = [a_in, b_in], in our case [1,100] self.lstm = nn.LSTM(b_in, n_hidden, n_layers) # n_layered lstm self.h_o = nn.Linear(n_hidden, n_out) # hidden to output self.h = [torch.zeros(n_layers, a_in, # initialize hidden and cell state in a list n_hidden, device=&#39;cuda&#39;) for _ in range(2)] def forward(self, x): # x=[bs,1,100], e.g. 128 elements of [1,100] data res,h = self.lstm(x, self.h) # the resulting output and (hn, cn) in list form. Res=[bs,1,n_hidden] self.h = [h_.detach() for h_ in h] # truncate the gradients return self.h_o(res) # run res thru self.h_o, return predictions [128, 1, 100] def reset(self): # reset hidden and cell before training/validation, and after epoch for h in self.h: h.zero_() . Let&#39;s test our model with a single batch to see that everything is working: . xb, yb = dls.train.one_batch() model = LSTM(dim_in=(1,100), n_out=100, n_hidden=128, n_layers=1) xb.shape, model.to(&#39;cuda&#39;)(xb).shape . (torch.Size([128, 1, 100]), torch.Size([128, 1, 100])) . Training the model . We&#39;ll create a learner in the usual way, but notice that we pass a ModelReseter callback (ModelReseter??) to the learner. It will call our LSTM-module&#39;s reset() automatically. . learn_lstm = Learner(dls, model, loss_func=MSELossFlat(), cbs=ModelReseter) . The model has more than twice the number of parameters (130 k) compared to our previous models: . learn_lstm.summary() . LSTM (Input shape: [&#39;128 x 1 x 100&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ LSTM [&#39;128 x 1 x 128&#39;, &#34; 117,760 True ________________________________________________________________ Linear 128 x 1 x 100 12,900 True ________________________________________________________________ Total params: 130,660 Total trainable params: 130,660 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f47ac2085f0&gt; Loss function: FlattenedLoss of MSELoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback - ModelReseter . We&#39;ll train the model for the same number of epochs and with the same learning rate as above. The loss is much worse than our attention model, but is clearly improving: . with learn_lstm.no_logging(): learn_lstm.fit(50, 1e-3) . learn_lstm.recorder.plot_loss(skip_start=1000) . Check a few predictions . Let&#39;s check our usual suspects (idxs) and compare the LSTM with our other models: . preds_lstm, _ = learn_lstm.get_preds(dl=test_dl) preds_lstm.shape . torch.Size([1000, 1, 100]) . The plot is getting a bit busy. But it&#39;s clear that the attention model is the winner. Also, note that the LSTM has much more oscillations than our other models. I&#39;m not sure why this is. . fig, axs = plt.subplots(2,2, figsize=(12,9)) for ax, idx in zip(axs.flatten(), idxs): plot_pattern(preds_lstm[idx], normalize(test_targets[idx]), input_type=&#39;prediction LSTM&#39;, ax=ax, legend=False) plot_pattern(preds_att[idx], normalize(test_targets[idx]), input_type=&#39;prediction attention&#39;, ax=ax, include_targ=False, legend=False) plot_pattern(preds_cnn[idx], normalize(test_targets[idx]), input_type=&#39;prediction cnn&#39;, ax=ax, include_targ=False, legend=False) handles, labels = ax.get_legend_handles_labels() fig.legend(handles, labels, loc=&#39;lower center&#39;, ncol=4); . Final thoughts . Attention worked really well in this example. After adding an attention layer we outclassed our base cnn model, even though our attention model had fewer parameters! The trusty old LSTM fared a bit better than the base cnn, but was not nearly as good as our attention model. I have to admit, though, that the LSTM model may need a bit more love during training, and the simple setup we used might not have allowed it to shine. . Self attention is really the heart of most transformer based models. The transformer uses multi-headed attention, but that is just a duplication of our self attention layer, with an extra linear layer to transform the output to appropriate shape. The full transformer features additional linear layers, skip connections and normalization layers, but nothing too fancy. . There is however one concept which we haven&#39;t covered, namely positional encoding. Transformers have mainly been used for nlp tasks so far. And when our input data is text, matters order (omg I&#39;m clever). Since self attention is fundamentally permutation invariant it will struggle to cope if our input data is ordered. In the next post we&#39;ll modify our example data to include ordering, and see if we can make a model that solves this harder task too - spoiler: we can! .",
            "url": "https://hallvagi.github.io/dl-explorer/fastai/attention/lstm/2020/06/29/Attention.html",
            "relUrl": "/fastai/attention/lstm/2020/06/29/Attention.html",
            "date": " • Jun 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Classification of movie review sentiment using ULMFiT",
            "content": "from fastai2.text.all import * . Load data and language model . In this post we&#39;ll try to predict movie reviews from a Norwegian language dataset explored in this post. It&#39;s taken a couple of steps to get here: . first we explored the underlying AWD-LSTM architecture | then we fine-tuned a pretrained Norwegian language model | and tried to interpret what the model learn with dimensionality reduction techniques. | . In this post we want to do actual classification, the final step of the ULMFiT method using fastai2 deep learning library. . First we&#39;ll grab the dataframe with reviews and labels from this post (available directly from github), the vocabulary from the language model, and the encoder from the fine-tuned language model (see this post). . df = pd.read_csv(&#39;https://raw.githubusercontent.com/hallvagi/dl-explorer/master/uploads/norec.csv&#39;) df.head(3) . filename rating title split sentiment text . 0 html/train/000000.html | 6 | Rome S02 | train | positive | Den andre og siste sesongen av Rome er ute på DVD i Norge. Om du så sesong 1, vet du at du har noe stort i vente. Har du aldri sett Rome før, stikk ut og kjøp begge sesongene. Dette er nemlig en av verdens beste tv-serier, og etter å ha sett de fire første episodene av sesong 2, konstaterer jeg at kvaliteten ser ut til å holde seg på et nesten overraskende høyt nivå! Sesong 2 starter nøyaktig der sesong 1 sluttet. Julius Cæsar ligger myrdet i Senatet og Lucius Vorenus hulker over liket av Neobie. Så blir historien enda mørkere. Marcus Antonius tar over styringen av Roma, men utfordres fra ... | . 1 html/train/000001.html | 6 | Twin Peaks - definitive gold box edition | train | positive | Tv-serien Twin Peaks, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen. Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men Twin Peaks sikrer dem evig liv. Serien handler om et mordmysterium i den lille byen Twin Peaks, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funn... | . 2 html/train/000002.html | 6 | The Wire (sesong 1-4) | train | positive | I neste uke kommer sesong 5 av tv-serien ”The Wire” på DVD. 2008 har for meg vært sterkt preget av denne serien. Hjemme hos oss begynte vi med sesong 1 i vår. Da hadde jeg i lengre tid hørt panegyriske lovord om serien fra både venner og media. Vi ble også fanget av skildringene av purk og skurk i Baltimore, og pløyde oss igjennom alt til og med sesong 4 på sensommeren. Jeg vil ikke gå så langt som å kalle det ”verdens beste serie”, som noen har gjort, men det er ingen tvil om at dette er noe av det bedre som er blitt vist på tv! Serien forteller om en gruppe politietterforskere som samles... | . We are mainly interested in the text, split and sentiment columns. Let&#39;s take a quick check for missing values: . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 8613 entries, 0 to 8612 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 filename 8613 non-null object 1 rating 8613 non-null int64 2 title 8613 non-null object 3 split 8613 non-null object 4 sentiment 8613 non-null object 5 text 8557 non-null object dtypes: int64(1), object(5) memory usage: 403.9+ KB . It seems there are a few missing values in our text columns, so let&#39;s get rid of them and reset the index: . df = df.dropna() df = df.reset_index(drop=True) . We&#39;ll load the language model vocabulary lm_itos.pkl that we made in a previous post: . path = Path(&#39;~/.fastai/data/norec/&#39;) Path.BASE_PATH = path (path/&#39;models&#39;).ls() . (#8) [Path(&#39;models/finetuned_model.pth&#39;),Path(&#39;models/norwegian_wgts.h5&#39;),Path(&#39;models/norwegian_enc.pth&#39;),Path(&#39;models/lm_itos.pkl&#39;),Path(&#39;models/finetuned_encoder.pth&#39;),Path(&#39;models/norwegian.zip&#39;),Path(&#39;models/norwegian_enc.h5&#39;),Path(&#39;models/norwegian_itos.pkl&#39;)] . with open(path/&#39;models/lm_itos.pkl&#39;, &#39;rb&#39;) as f: itos = pickle.load(f) itos[:10], itos[-5:], len(itos) . ([&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;.&#39;, &#39;i&#39;, &#39;,&#39;, &#39;og&#39;, &#39; n n&#39;, &#39;av&#39;, &#39;som&#39;, &#39;en&#39;], [&#39;learning&#39;, &#39;initiativtager&#39;, &#39;forskningsleder&#39;, &#39;devils&#39;, &#39;graeme&#39;], 30002) . We&#39;ll load the encoder, finetuned_encoder.pth, at a later stage when our classifier is set up. . Setup dataloader . Before we can make a model we need a dataloader. The dataloader is responsible for keeping track of the data, labels and dataset splits among other. The data loader will then feed batches to our model during training. We will test the data block api, instead of a simpler factory method. The data block represents the mid-level of the fastai api and is a flexible way to set up our data loader. For the ickiest datasets, one might have to drop down the lowest level of the api. . A data block is basically an assembly line that we will send our data through. That mean we set up a string of functions that will take our data frame and extract the relevant information the data loader needs: . define which types (block) our dependent and independent variables are. In our case that is text and category. | create a tokenizer and a vocabulary. We need to turn our text in to a numerical representation according to some vocabulary. | define how to get the text: in our case we read it from the dataframe column &#39;text&#39; | define how to get the labels: in our case we&#39;ll read it form the dataframe column &#39;sentiment&#39; | split the data in a train, validation and test dataset. In our case we have to locate the indexes of rows in the column &#39;split&#39; depending on the value of split: &#39;train&#39;, &#39;dev&#39; or &#39;test&#39;. | . Note: In fastai and kaggle lingo the test set is an unlabeled dataset on which we test our model. In our case, though, we also have labels for the test set. So we can assess the performance on both the validation and test set. . First let&#39;s set up a basic function that splits the data according to which split it belongs to. The datablock expects a set of indexes for the training, validation and test dataset: . def split(df): train_idx = df.loc[df[&#39;split&#39;] == &#39;train&#39;].index.to_list() valid_idx = df.loc[df[&#39;split&#39;] == &#39;dev&#39;].index.to_list() test_idx = df.loc[df[&#39;split&#39;] == &#39;test&#39;].index.to_list() return L(train_idx), L(valid_idx), L(test_idx) . If we pass our df through this function it simply returns the indexes of the various splits: . split(df) . ((#6863) [0,1,2,3,4,5,6,7,8,9...], (#876) [136,137,138,140,141,142,143,144,145,146...], (#818) [183,184,186,187,188,189,190,191,192,193...]) . We can use the built-in class &#39;ColReader&#39; to read the actual texts and labels. The ColReader class has a __call__ method that reads a particular column from a data frame that is passed: . reader = ColReader(cols=&#39;sentiment&#39;) reader(df)[:3] . 0 positive 1 positive 2 positive Name: sentiment, dtype: object . We now have all we need to set up our data block: . . Note: The seq_len has to be the same as we trained our language model with! . reviews = DataBlock(blocks=(TextBlock.from_df(text_cols=&#39;text&#39;, vocab=itos, seq_len=72), CategoryBlock), get_x=ColReader(&#39;text&#39;), get_y=ColReader(&#39;sentiment&#39;), splitter=split ) . A grate way of debugging your data block with is the summary(df) method. It&#39;s takes you through all the steps in the pipeline. The output is very long though, so I left it out in this notebook. . #reviews.summary(df) . Finally let&#39;s create the actual data loader from our source dataframe: . dls = reviews.dataloaders(df, bs=64) dls.show_batch(max_n=4) . text category . 0 xxbos html , body { border : xxunk ; } — xxunk det enkle er grunnlaget for all n xxunk , sa xxunk xxunk xxunk xxunk til xxunk xxunk xxunk xxunk i 1923 . xxunk filmen « xxunk xxunk xxunk n &amp; xxunk igor xxunk xxunk » , som handler om xxunk xxunk korte xxunk med den n russiske komponisten xxunk igor xxunk xxunk , baserer seg på samme dekret . xxunk xxunk selv n skjærer som en mørk xxunk inn i scenene , med sine smale , svarte antrekk . xxunk store n deler av filmen foregår på xxunk xxunk landsted , dit den velstående xxunk n inviterer den fattige komponisten og familien hans for at han skal få jobbe , og n de svarte og hvite linjene i xxunk xxunk xxunk xxunk seg xxunk gjennom regissør n xxunk jan xxunk xxunk bilder slik xxunk xxunk selv skjærer gjennom | positive | . 1 xxbos xxunk vakre , xxunk xxunk christine xxunk brown sover trygt i sin xxunk xxunk , med kjæresten ved sin side . xxunk ei flue flyr inn gjennom vinduet , summer xxunk gjennom rommet og lander på xxunk , før den setter kursen mot vår blonde xxunk . xxunk musikken er xxunk og xxunk . xxunk xxunk likeså og xxunk inn i xxunk browns ene xxunk . xxunk og ut igjen fra det andre . xxunk før den forsvinner inn i xxunk hennes og vekker henne til hennes livs verste mareritt , ei xxunk , xxunk xxunk , som xxunk over henne med brune xxunk , xxunk for å bite henne til døde . xxup scenen xxup fra « drag xxunk me to xxunk hell » representerer det meste xxunk sam xxunk xxunk står for som filmskaper . xxunk den er xxunk , xxunk og preget av humor . xxunk | positive | . 2 xxbos xxunk lawrence of xxunk arabia er en film for de store lerret . xxunk den bør helst oppleves i en xxunk , fortrinnsvis i sitt opprinnelige 70 mm format . xxunk men siden det er en xxunk i 2012 , er den nye xxunk blu - ray - utgivelsen det nest beste . xxunk den er faktisk dobbelt restaurert . xxunk den digitale xxunk er nemlig basert på en xxunk xxunk fra 1988 , da den også ble rekonstruert til sin opprinnelige lengde . xxunk og på xxunk blu - ray har xxunk lawrence of xxunk arabia en fantastisk klarhet og dybde som forsterker følelsen av xxunk xxunk . xxunk det er helt utrolig at en 50 år gammel film kan se så bra ut ! xxunk david xxunk xxunk mesterverk skildrer en mann som lar sin indre kamp komme til uttrykk i ytre handlinger . xxunk når en | positive | . 3 xxbos xxunk 3 | • xxunk dagbladets reporter xxunk i xxunk cannes . xxunk les hans rapport her . xxunk xxup film : xxunk vi har lest om xxunk og n skandale . xxunk om sex , blod , xxunk og vold . xxunk om xxunk av xxunk og n xxunk xxunk som xxunk blod . xxunk om journalister som har xxunk og n kritikere som har slaktet og xxunk om hverandre . xxunk xxunk for en tid tilbake befant regissør von xxunk trier seg i en dyp n depresjon . xxunk den svært xxunk dansken har fortalt om psykiske problemer før , n men ifølge ham selv var tidligere depresjoner ingen ting sammenliknet med mørket n som rammet ham nå . i lang tid var han ikke i stand til å gjøre annet enn å ligge n og xxunk tomt ut i lufta . xxunk som en form for | positive | . Note that our data block reviews is specific to the source we plan to use it with. If we instead pass the underlying numpy values of the dataframe to it, the reader and splitter functions won&#39;t work: . #reviews.dataloaders(df.values) . Inspect a batch . What does our data actually look like? Grabbing a batch and inspecting it is often a useful way of understanding what the data actually looks like for our model: . xb, yb = dls.one_batch() xb.shape, yb.shape . (torch.Size([64, 2990]), torch.Size([64])) . The dependent variable xb represents the numericalized text of the reviews. We have a batch size of 64 and the longest text in this particular batch has a length of 2990 tokens - but this will vary from batch to batch. We can get the vocab from our data loader, dls, to check what the numbers represent. We have two vocabs, one for the text and one for the labels of our data: . tokens, classes = dls.vocab tokens[:5], classes . ([&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;.&#39;, &#39;i&#39;, &#39;,&#39;], (#2) [&#39;negative&#39;,&#39;positive&#39;]) . Let&#39;s first grab a few numericalized tokens from our batch: . nums = xb[0][10:15].cpu() nums . tensor([2631, 0, 18, 2770, 10]) . We can use the fastai L-class to look up those indexes in the vocab. Remember that token 2631 is simply the token at index 2631 in our vocab: . tokens[2631] . &#39;—&#39; . L(tokens)[nums] . (#5) [&#39;—&#39;,&#39;xxunk&#39;,&#39;det&#39;,&#39;enkle&#39;,&#39;er&#39;] . . Note: L is a fastai version of the python List datatype with some added functionality such as slicing from a list of indexes . Our dependent variable is as expected, either positive or negative. Once again we can use the vocab to get the actual classes. 1 is positive and 0 is negative: . yb[:5].cpu(), L(classes)[yb[:5]] . (tensor([1, 1, 0, 1, 1]), (#5) [&#39;positive&#39;,&#39;positive&#39;,&#39;negative&#39;,&#39;positive&#39;,&#39;positive&#39;]) . Padding . The longest text in the batch decides the shape of the enitre batch. What happens with the shorter texts? They are padded with our padding token, in our case token id 1: . tokens[1] . &#39;xxpad&#39; . If we look at the final text in the batch we see that it&#39;s padded on both sides: . xb[-1].cpu(), len(xb[-1]) . (tensor([1, 1, 1, ..., 1, 1, 1]), 2990) . We can count the padding of each text in the batch: . (xb==1).sum(dim=1).cpu() . TensorText([ 0, 94, 404, 1179, 1352, 1374, 1376, 1388, 1405, 1434, 1499, 1513, 1563, 1580, 1615, 1617, 1617, 1634, 1687, 1717, 1730, 1743, 1763, 1793, 1794, 1813, 1851, 1870, 1893, 1893, 1895, 1900, 1951, 1980, 1984, 1986, 1991, 1996, 2006, 2010, 2014, 2014, 2023, 2032, 2055, 2067, 2072, 2079, 2081, 2082, 2091, 2091, 2117, 2118, 2128, 2131, 2136, 2140, 2143, 2144, 2147, 2159, 2164, 2165]) . We see the longest review is placed first, and the batch has progressively shorter texts. . Creating our model . Setting up the model is pretty straight forward, but since our language model had a hidden size of 1150, we have to change the default in the config dictionary. Note also that we pass pretrained=False. We&#39;ll load our pretrained encoder manually instead. . awd_lstm_clas_config[&#39;n_hid&#39;] = 1150 learn = text_classifier_learner(dls, arch=AWD_LSTM, metrics=accuracy, config=awd_lstm_clas_config, pretrained=False).to_fp16() learn.load_encoder(path/&#39;models/finetuned_encoder&#39;) . &lt;fastai2.text.learner.TextLearner at 0x7fb1d16c5350&gt; . Inspecting the model . When looking at the model we can see that the first layers of the model are frozen, i.e. the trainable column says False. It&#39;s only the randomly initialized linear classification layers that are trainable. This makes sense. We first want to calibrate these layers as much as possible before we proceed to fine tune the language model. We&#39;ll come back to freezing and unfreezing of the model later in the post. . learn.summary() . SequentialRNN (Input shape: [&#39;64 x 2990&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ RNNDropout 64 x 38 x 400 0 False ________________________________________________________________ RNNDropout 64 x 38 x 1150 0 False ________________________________________________________________ RNNDropout 64 x 38 x 1150 0 False ________________________________________________________________ BatchNorm1d 64 x 1200 2,400 True ________________________________________________________________ Dropout 64 x 1200 0 False ________________________________________________________________ Linear 64 x 50 60,000 True ________________________________________________________________ ReLU 64 x 50 0 False ________________________________________________________________ BatchNorm1d 64 x 50 100 True ________________________________________________________________ Dropout 64 x 50 0 False ________________________________________________________________ Linear 64 x 2 100 True ________________________________________________________________ Total params: 62,600 Total trainable params: 62,600 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fb1fc4a0f80&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group number 4 Callbacks: - ModelReseter - RNNRegularizer - ModelToHalf - TrainEvalCallback - Recorder - ProgressCallback - MixedPrecision . We also recognize most of the other dimensions in the architecture: . batch size of 64 | embedding size of 400 | hidden size of LSTMs is 1150 | 1200 and 50 output channels from the linear classifier layers | but why is seq_len 38? Didn&#39;t we set it to 72 for the data loader? We&#39;ll investigate this later in the post. | . learn.model . SequentialRNN( (0): SentenceEncoder( (module): AWD_LSTM( (encoder): Embedding(30002, 400, padding_idx=1) (encoder_dp): EmbeddingDropout( (emb): Embedding(30002, 400, padding_idx=1) ) (rnns): ModuleList( (0): WeightDropout( (module): LSTM(400, 1150, batch_first=True) ) (1): WeightDropout( (module): LSTM(1150, 1150, batch_first=True) ) (2): WeightDropout( (module): LSTM(1150, 400, batch_first=True) ) ) (input_dp): RNNDropout() (hidden_dps): ModuleList( (0): RNNDropout() (1): RNNDropout() (2): RNNDropout() ) ) ) (1): PoolingLinearClassifier( (layers): Sequential( (0): LinBnDrop( (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): Dropout(p=0.2, inplace=False) (2): Linear(in_features=1200, out_features=50, bias=False) (3): ReLU(inplace=True) ) (1): LinBnDrop( (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): Dropout(p=0.1, inplace=False) (2): Linear(in_features=50, out_features=2, bias=False) ) ) ) ) . Looking inside our model . Before we do the actual classification, let&#39;s have a look at what the model actually does at each step. . What does the SentenceEncoder do? . In short, the SentenceEncoder takes text and outputs a vector representation of it. Let&#39;s split the model into the SentenceEncoder and the classifier and inspect both: . enc, lin = learn.model.children() enc . SentenceEncoder( (module): AWD_LSTM( (encoder): Embedding(30002, 400, padding_idx=1) (encoder_dp): EmbeddingDropout( (emb): Embedding(30002, 400, padding_idx=1) ) (rnns): ModuleList( (0): WeightDropout( (module): LSTM(400, 1150, batch_first=True) ) (1): WeightDropout( (module): LSTM(1150, 1150, batch_first=True) ) (2): WeightDropout( (module): LSTM(1150, 400, batch_first=True) ) ) (input_dp): RNNDropout() (hidden_dps): ModuleList( (0): RNNDropout() (1): RNNDropout() (2): RNNDropout() ) ) ) . enc.summary(xb) . SentenceEncoder (Input shape: [&#39;64 x 2990&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ RNNDropout 64 x 38 x 400 0 False ________________________________________________________________ RNNDropout 64 x 38 x 1150 0 False ________________________________________________________________ RNNDropout 64 x 38 x 1150 0 False ________________________________________________________________ Total params: 0 Total trainable params: 0 Total non-trainable params: 0 . Here we see a seq_len of 38 again. This is an artifact of the model summary which is specific to the batch. The LSTM processes a batch in chunks of 72 - the actual seq_len. The summary shows the final chunk, where it simply happens to be 38 items left: . 2990%72 . 38 . What comes out of the encoder if we pass it a batch of data? . enc_x = enc(xb) [e.shape for e in enc_x] . [torch.Size([64, 1406, 400]), torch.Size([64, 1406])] . The SentenceEncoder outputs two things, and if we check the source code, SentenceEncoder??, we find that it returns: return outs,mask. . The first output is our encoded text. The shape out the output is bs, len, embedding size. But where does the 1406 size come from? This is also specific to the batch we pass in. The get_text_classifier is called with a default max_len of 72*20 = 1440. So the SentenceEncoder encodes the final sequence of tokens up to a maximum length of 1440. It also makes sure to include the last sequence of the batch: . SentenceEncoder?? . Our batch has size 2990, much longer than the maximum. So we will fill up 19 chunks of seq_len 72 and then add the remainder of 38 of the final chunk as the last part of the encoded sentence: . 2990%72 . 38 . So 19 full chunks and the remainder gives us: . 72*19 + 38 . 1406 . The mask output is a padding mask, that tells us where the padding tokens are in the processed text: . enc_x[1].sum(1).cpu() . tensor([ 0, 22, 44, 27, 56, 6, 8, 20, 37, 66, 59, 1, 51, 68, 31, 33, 33, 50, 103, 133, 146, 159, 179, 209, 210, 229, 267, 286, 309, 309, 311, 316, 367, 396, 400, 402, 407, 412, 422, 426, 430, 430, 439, 448, 471, 483, 488, 495, 497, 498, 507, 507, 533, 534, 544, 547, 552, 556, 559, 560, 563, 575, 580, 581]) . If we take the final 1406 tokens of our batch we get the same sum of padding tokens: . (xb[:, -1406:]==1).sum(1).cpu() . tensor([ 0, 22, 44, 27, 56, 6, 8, 20, 37, 66, 59, 1, 51, 68, 31, 33, 33, 50, 103, 133, 146, 159, 179, 209, 210, 229, 267, 286, 309, 309, 311, 316, 367, 396, 400, 402, 407, 412, 422, 426, 430, 430, 439, 448, 471, 483, 488, 495, 497, 498, 507, 507, 533, 534, 544, 547, 552, 556, 559, 560, 563, 575, 580, 581]) . What does the classifier look like? . The classifier takes the encoded sentence and produces a binary prediction: . lin.summary(enc_x) . PoolingLinearClassifier (Input shape: [&#34;[&#39;64 x 1406 x 400&#39;, &#39;64 x 1406&#39;]&#34;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ BatchNorm1d 64 x 1200 2,400 True ________________________________________________________________ Dropout 64 x 1200 0 False ________________________________________________________________ Linear 64 x 50 60,000 True ________________________________________________________________ ReLU 64 x 50 0 False ________________________________________________________________ BatchNorm1d 64 x 50 100 True ________________________________________________________________ Dropout 64 x 50 0 False ________________________________________________________________ Linear 64 x 2 100 True ________________________________________________________________ Total params: 62,600 Total trainable params: 62,600 Total non-trainable params: 0 . If we pass the encoded batch through the model, we get a binary output for each item in our batch: . lin(enc_x)[0].shape . torch.Size([64, 2]) . It&#39; not entirely clear how we go from from the 1406 dimension input to the first batchnorm layer that expects 1200 size input. But if we check the source code of the PoolingLinearClassifier?? module it indeed has a masked_concat_pool in it&#39;s forward method which changes the dimensionality of the input. . Making a prediction . Let&#39;s pass the batch we grabbed above through the entire model: . preds = learn.model(xb) [p.shape for p in preds] . [torch.Size([64, 2]), torch.Size([64, 1406, 400]), torch.Size([64, 1406, 400])] . Our model produces three things. The binary predictions for each item in the batch and the encoded sentences. Note that the two encoded batches of sentences are identical: . (preds[1]!=preds[2]).sum().item() . 0 . Since the model is untrained at this point, the prediction should be random. If we take the softmax of the final dimension we see that the model is mostly 50-50 on every prediction - so just a random guess at this point! . preds[0].softmax(-1)[:5].cpu() . tensor([[0.4973, 0.5027], [0.4912, 0.5088], [0.4805, 0.5195], [0.4879, 0.5121], [0.5008, 0.4992]], grad_fn=&lt;CopyBackwards&gt;) . A quick glance at the weights . Let&#39;s also take a quick glance at the various weights of the model: If we plot one of the weight matrices it&#39;s clear that the above weights matrix has been randomly and uniformly initialized, and thus has not been trained yet. . plt.hist(to_np(lin.state_dict()[&#39;layers.0.2.weight&#39;]).flatten()); . If we instead check the one of the encoder weights, we get a completely different pattern. Most weights a centered around 0: . plt.hist(to_np(enc.state_dict()[&#39;module.rnns.1.module.weight_ih_l0&#39;]).flatten(), bins=50); . What are the frozen parameter groups? . Fastai operates with a concept of frozen parameter groups. I.e. the model is split into groups which can be made trainable or not. We can check the status of the models parameter groups: . len(learn.opt.param_groups), learn.opt.frozen_idx . (5, 4) . That is a total of 5 parameter groups and currently the first 4 is frozen. This corresponds with what we saw from the model summary - only the top classifier layer was trainable. During training we can unfreeze parts of the model: . learn.freeze_to(-3), learn.opt.frozen_idx . (None, 2) . Let&#39;s reset the model before we train it: . learn.freeze_to(-1), learn.opt.frozen_idx . (None, 4) . The parameter groups aren&#39;t named, but if we check the shape of the weights from each group we recognize the various layers: . The embedding layer | LSTM 1 | LSTM 2 | LSTM 3 | Linear classifier | [group.get(&#39;params&#39;)[0].shape for group in learn.opt.param_groups] . [torch.Size([30002, 400]), torch.Size([4600, 1150]), torch.Size([4600, 1150]), torch.Size([1600, 400]), torch.Size([1200])] . Training the model . It&#39;s high time to actually train the model. We&#39;ll stick with standard fastai procedure for fine tuning a language sentiment classifier: . lr_find() to find a sensible learning rate | train with the one cycle policy | gradual unfreezing of the layers - train the top layers first | discriminative learning rates. That is train the lower layers with a smaller learning rate than the top layers. | . This procedure is well known from the course and documentation. It&#39;s also a very robust method in my experience. It seem to just work for mosts datasets. . learn.lr_find() . SuggestedLRs(lr_min=0.010000000149011612, lr_steep=0.0063095735386013985) . This pattern is typical for untrained networks: There is a part of the graph which shows substantial lower loss than the other parts. This makes sense since changing random weights should give us substantial improvements pretty fast. We&#39;ll start with a learning rate of 1e-2 and train for a few epochs. Note: We are only training the top layers in this step: . lr = 1e-2 . learn.fit_one_cycle(3, lr) . epoch train_loss valid_loss accuracy time . 0 | 0.619132 | 0.507724 | 0.752283 | 00:24 | . 1 | 0.579055 | 0.467570 | 0.768265 | 00:23 | . 2 | 0.540422 | 0.468025 | 0.779680 | 00:24 | . We&#39;re already at nearly 78% accuracy, and we haven&#39;t touched the actual language model yet! Let&#39;s unfreeze the first LSTM and run LR-find one more time: . learn.freeze_to(-2) learn.lr_find() . SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=3.0199516913853586e-05) . This pattern is what you would expect from a model that has trained for a while. There are no more random weights left anymore, and further progress is much harder to find. We&#39;ll set a lower base learning rate for the remainder of the training, and slice this with a scaling factor. This means that the lowest parameter groups gets a lower learning rate, and the top layers a higher. When we slice we&#39;ll use the magic scaling factor* of 2.64 ~ 45. The source of this scaling factor is fastai guru Jeremy Howard, which found it to empirically work well. I&#39;ve set this to 50 for sake of simplicity. . lr = 1e-3 scaling = 50 . learn.fit_one_cycle(1, slice(lr/scaling, lr)) . epoch train_loss valid_loss accuracy time . 0 | 0.519882 | 0.468881 | 0.779680 | 00:29 | . We continue to unfreeze the next layer, and train a bit more: . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(lr/scaling,lr)) . epoch train_loss valid_loss accuracy time . 0 | 0.468425 | 0.448788 | 0.797945 | 00:43 | . Our model is gradually improving, and we&#39;re at 8 % accuracy. Finally we unfreeze the entire model and train a final few epochs: . learn.unfreeze() learn.fit_one_cycle(5, slice(lr/scaling,lr)) . epoch train_loss valid_loss accuracy time . 0 | 0.440907 | 0.427563 | 0.809361 | 00:56 | . 1 | 0.374653 | 0.418673 | 0.819635 | 00:54 | . 2 | 0.329612 | 0.423127 | 0.831050 | 00:51 | . 3 | 0.279565 | 0.419359 | 0.828767 | 00:57 | . 4 | 0.262696 | 0.429828 | 0.825342 | 00:55 | . We end up with around 83% validation accuracy in our best epoch. This is much better than the 50-50 we would have gotten from a random guess: . df[&#39;sentiment&#39;].value_counts(normalize=True) . negative 0.511044 positive 0.488956 Name: sentiment, dtype: float64 . But is our model really a good one? This is kind of difficult to say, since I haven&#39;t seen any results for this particular dataset, and we don&#39;t have any other models as baselines to compare with. The English IMDb dataset gets over 95% accuracy with ULMFiT. But this task likely has access to a better language model and has 10x more data. On the other hand, the reviews considered in this dataset are written by relatively few journalists, and we would maybe expect their writing style to be more consistent. Finally it should also be noted that our sentiment classes probably are closer than in the IMDb example. That is, we based negative on a rating of 1-3 of 6, and positive as 5-6. The IMDb reviews are more polarized, so the task is perhaps a bit easier. . Note that we also have a test set for this model, so this is the actual data set to score our model on . We already passed the test dataset to our dataloader, so now we can access it directly. The test set has index 2 (0 is train, and 1 is validation). So we can just validate with this data to get the score since we&#39;re not actually interested in the particular predictions at this point. . learn.validate(ds_idx=2) . (#2) [0.42844530940055847,0.8227384090423584] . The test set accuracy is the same as the validation set accuracy. This is kind of what we hoped for. That is, if there was larger differences, we would have to go back to our model to see if we did something wrong, or if the test set was sampled from a different distribution. . A finally important point is to investigate the predictions of the model. This could increase our understanding of what the model actually learns, discover data leaks, and in general increase our confidence in the model. We&#39;ll come back to model interpretation in a later post! .",
            "url": "https://hallvagi.github.io/dl-explorer/nlp/fastai/ulmfit/sentiment/2020/06/12/ULMFiT_classifier.html",
            "relUrl": "/nlp/fastai/ulmfit/sentiment/2020/06/12/ULMFiT_classifier.html",
            "date": " • Jun 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Inspecting the embedding of an AWD-LSTM with UMAP",
            "content": "from fastai2.text.all import * . Load vocabulary and model weights . In the previous post we took a pretrained language model and fine tuned it on our movie review dataset. In this post we&#39;ll try to see if we can make sense of the weights that the models learned. Specifically we&#39;ll look at the weights of the initial embedding layer. This is the first layer of the model, and we would expect the weights to reflect patterns in the language. . In order to inspect the weights, we don&#39;t need to load the dataset or a learner object. We can simply load the saved weights directly. We will also need the vocabulary of the model, the itos, to map which weights belong to which token. . path = Path(&#39;/data/hgi/.fastai/data/norec/&#39;) Path.BASE_PATH = path (path/&#39;models&#39;).ls() . (#7) [Path(&#39;models/finetuned_model.pth&#39;),Path(&#39;models/norwegian_wgts.h5&#39;),Path(&#39;models/norwegian_enc.pth&#39;),Path(&#39;models/finetuned_encoder.pth&#39;),Path(&#39;models/norwegian.zip&#39;),Path(&#39;models/norwegian_enc.h5&#39;),Path(&#39;models/norwegian_itos.pkl&#39;)] . First we&#39;ll load the vocabulary of our model, the norwegian_itos.pkl: . with open(path/&#39;models/norwegian_itos.pkl&#39;, &#39;rb&#39;) as f: itos = pickle.load(f) len(itos) . 30002 . And then the weights of the finetuned model: . mod = torch.load(path/&#39;models/finetuned_model.pth&#39;) mod.keys() . dict_keys([&#39;model&#39;, &#39;opt&#39;]) . Let&#39;s check the model part of the dictionary: . [f&#39;{k:30}{v.shape}&#39; for k,v in mod[&#39;model&#39;].items()] . [&#39;0.encoder.weight torch.Size([30002, 400])&#39;, &#39;0.encoder_dp.emb.weight torch.Size([30002, 400])&#39;, &#39;0.rnns.0.weight_hh_l0_raw torch.Size([4600, 1150])&#39;, &#39;0.rnns.0.module.weight_ih_l0 torch.Size([4600, 400])&#39;, &#39;0.rnns.0.module.bias_ih_l0 torch.Size([4600])&#39;, &#39;0.rnns.0.module.bias_hh_l0 torch.Size([4600])&#39;, &#39;0.rnns.1.weight_hh_l0_raw torch.Size([4600, 1150])&#39;, &#39;0.rnns.1.module.weight_ih_l0 torch.Size([4600, 1150])&#39;, &#39;0.rnns.1.module.bias_ih_l0 torch.Size([4600])&#39;, &#39;0.rnns.1.module.bias_hh_l0 torch.Size([4600])&#39;, &#39;0.rnns.2.weight_hh_l0_raw torch.Size([1600, 400])&#39;, &#39;0.rnns.2.module.weight_ih_l0 torch.Size([1600, 1150])&#39;, &#39;0.rnns.2.module.bias_ih_l0 torch.Size([1600])&#39;, &#39;0.rnns.2.module.bias_hh_l0 torch.Size([1600])&#39;, &#39;1.decoder.weight torch.Size([30002, 400])&#39;, &#39;1.decoder.bias torch.Size([30002])&#39;] . We want the 0.encoder.weight layer. Note the shape of 30002 x 400 is vocab x embedding size. For sake of simplicity we will combine the weights and itos into a pandas dataframe with the token in the first column. . wts = pd.DataFrame(mod[&#39;model&#39;][&#39;0.encoder.weight&#39;]) wts.insert(0, &#39;token&#39;, itos) # create itos column as first column print(wts.shape) wts.head(3) . (30002, 401) . token 0 1 2 3 4 5 6 7 8 ... 390 391 392 393 394 395 396 397 398 399 . 0 _unk_ | 0.532715 | 0.289551 | 0.000947 | 0.555176 | 0.645508 | -0.158447 | -0.063538 | 0.475830 | -0.143799 | ... | 0.303711 | -0.447510 | 0.070312 | 0.091370 | 0.530273 | -0.200806 | -0.341064 | 0.335205 | 0.305908 | 0.035522 | . 1 _pad_ | -0.155029 | -0.045685 | 0.090393 | -0.244507 | -0.101379 | -0.004860 | -0.033386 | -0.083130 | 0.025513 | ... | -0.103577 | 0.100159 | -0.036377 | 0.010338 | -0.089478 | 0.059662 | 0.187744 | -0.114014 | -0.085083 | 0.013466 | . 2 . | 0.362061 | -0.982910 | -0.106384 | 0.452637 | 0.752930 | -0.117310 | 0.069397 | 0.145386 | 0.092346 | ... | 0.346191 | -0.233398 | 0.073853 | 0.242065 | 0.050323 | -0.436035 | -0.372314 | 0.303223 | 0.360596 | 0.108032 | . 3 rows × 401 columns . We now have a dataframe with one row per token in the vocab, and 400 columns with the embedding weights for that particular token. . Inspect the weights . We can see that our weights vary between approximately -3.8 to 2.8. Note that .values returns thee underlying numpy ndarray of our data frame. . wts.iloc[:, 1:].values.min(), wts.iloc[:, 1:].values.max() . (-3.8261719, 2.7851562) . We can also plot all the weights as a histogram. We add a .flatten() to our dataframe.values to create a single histogram for all the 30002*400 weights. . fix, ax = plt.subplots(1,1) ax.hist(wts.iloc[:, 1:].values.flatten(), bins=100); ax.set_xlabel(&#39;Embedding weight&#39;) . Text(0.5, 0, &#39;Embedding weight&#39;) . As expected the weights are centered around 0, with a few extreme values. . Good weights, bad weights . Let&#39;s look closer at a few select tokens, and see if we can make sense of the corresponding weights. We&#39;ll choose the words good(&#39;god&#39;) and bad(&#39;dårlig&#39;). We would expect the two tokens to have some similarities, but also in some respect to be opposites. . sample_tokens = [&#39;god&#39;, &#39;dårlig&#39;] # good, bad sample = wts.loc[wts[&#39;token&#39;].isin(sample_tokens),:] sample . token 0 1 2 3 4 5 6 7 8 ... 390 391 392 393 394 395 396 397 398 399 . 590 god | 0.410400 | 0.414062 | -0.260498 | 0.235474 | -0.162964 | -0.142700 | -0.017609 | 0.064209 | 0.533203 | ... | 0.276367 | 0.239380 | -0.054108 | 0.024933 | -0.006481 | 0.557617 | -0.384033 | 0.313965 | -0.033844 | -0.141357 | . 1015 dårlig | 0.404541 | 0.183838 | 0.001801 | 0.097351 | -0.022003 | -0.049194 | -0.019836 | -0.060455 | 0.001980 | ... | -0.227661 | 0.582031 | -0.130127 | 0.232788 | 0.145996 | 0.483887 | 0.069214 | -0.012825 | -0.185181 | -0.145630 | . 2 rows × 401 columns . Let&#39;s plot the first 30 weights sequentially. They seem to mostly follow each other: . fig, ax = plt.subplots(1,1, figsize = (12, 6)) for row in sample.values: ax.plot(row[1:30], label = row[0]) ax.legend() . We can also make a scatter plot to compare the weights of the two tokens. It looks like a linear relationship, but with some variation: . fig, ax = plt.subplots(1,1, figsize = (6, 6)) ax.scatter(x = sample.iloc[0,1:], y=sample.iloc[1,1:], alpha=0.5) ax.set_xlabel(&#39;Weights for &quot;God&quot;&#39;); ax.set_ylabel(&#39;Weights for &quot;Dårlig&quot;&#39;); . . Note: We are using the object oriented syntax for matplotlib in the above examples. I particularly like Chris Moffits tutorial on matplotlib. . Finally, we can also verify the relationship by asserting that the correlation coefficient is greater than 0: . np.corrcoef(sample.iloc[:, 1:])[0,1] . 0.5042995297412233 . But this approach is kind of unwieldy. It&#39;s fine for comparing pairs of tokens, but if we want to somehow compare all the tokens, we&#39;ll need another method. . UMAP . UMAP is a dimensionality reduction algorithm which can be helpful to visualize high dimensional data. It was introduced in a 2018 paper. The authors also made a python library. This makes it easy to use the algorithm. I installed the UMAP-learn library along with the suggested pynndescent. . In brief, UMAP can take a high dimensional data structure and turn it into fewer dimensions, while retaining some of the characteristics of the original data structure, such as clusters. We can thus take our 400 dimensional weights, and turn them into a new data structure of only two dimensions. This will be much easier to illustrate. . Note: The new data structure is called an embedding which is kind of confusing in this case where we are inspecting an embedding! . import umap reduced = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.3, random_state=42, metric=&#39;cosine&#39;).fit_transform(wts.values[:, 1:]) . The &#39;embedding&#39; produced by the UMAP algorithm, reduced, only has two dimensions: . wts.shape, reduced.shape . ((30002, 401), (30002, 2)) . That means we have reduced the number of &quot;weights&quot; from approximately 12 000 000 (30002x400) to around 60 000 (30002x2), a reduction of 99.5 %. But does the result make any sense? Let&#39;s look at a scatter plot: . fig, ax = plt.subplots(1,1, figsize = (6, 6)) ax.scatter(reduced[:, 0], reduced[:, 1], alpha=0.01); ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;); . Note that results from the UMAP algorithm can vary a lot with varying hyper parameters. I simply tried a couple of hyperparameters within the range of the recommended defaults. The above result seemed to be good enough, that is, it has several interesting clusters and shapes that we can investigate further. But there might be other embeddings that are &#39;better&#39; though. . We would like to inspect the actual tokens that correspond to each point above. But making an interactive chart with matplotlib isn&#39;t straight forward to my knowledge. But luckily there are other plotting libraries we can test! . Visualizing with altair . I haven&#39;t used altair before, but heard a lot about it. It&#39;s also supposed to play well with fastpages, the platform used to write this blog. About time to take it for a spin! . Altair prefers input data in the form of a data frame, so let&#39;s combine our reduced embedding with the vocabulary itos: . df = pd.DataFrame(reduced, columns=[&#39;x&#39;, &#39;y&#39;]) df.insert(0, &#39;token&#39;, itos) df.head() . token x y . 0 _unk_ | 12.963406 | 9.573566 | . 1 _pad_ | 5.633428 | 0.549199 | . 2 . | 3.142599 | 1.704447 | . 3 i | 14.942327 | 9.356459 | . 4 , | 3.190900 | 1.649735 | . Note that altair at the time of writing has a maximum limit of 5000 data points for such a plot, so we simply grab a random sample of 5000 rows from our data frame: . import altair as alt alt.Chart(df.sample(5000, random_state=42)).mark_circle(size=50, fillOpacity=0.2).encode( x=&#39;x&#39;, y=&#39;y&#39;, tooltip=[&#39;token&#39;] ).interactive() . We recognize the patterns of the scatter plot from the above plot. But with this plot we can inspect each token by hovering over a point. There are several interesting clusters of tokens: . x=-2, y=2: 4-digit numbers, probably years | x=9, y=15, 3-digit numbers (note the 2 digit numbers directly to the right) | x:17, y=0: infinitive form verbs (present form directly above) | x:10, y=-2: place names | x:6, y=6: names of people | . It&#39;s kind of remarkable how meaningful and easy to interpret the clustering is. I&#39;m sure there are many other relationships that can be discovered given a closer inspection. It certainly seems like our model has learned a meaningful representation of the language. . But it&#39;s difficult to take the above visualization and diagnose our model in a specific way. It&#39;s not clear if we get any actionable insights from it. But seeing that things &#39;make sense&#39; definitively give us some confidence in our model! .",
            "url": "https://hallvagi.github.io/dl-explorer/nlp/fastai/lstm/umap/2020/04/28/UMAP-embedding.html",
            "relUrl": "/nlp/fastai/lstm/umap/2020/04/28/UMAP-embedding.html",
            "date": " • Apr 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fine-tuning a Norwegian language model for ULMFiT",
            "content": "from fastai2.text.all import * . ULMFiT recap . In a previous post we explored the Norec Norwegian language corpus of film and TV reviews. In this post I want to use the ULMFiT method to predict the sentiment of the reviews based on the text. ULMFiT has three main steps: . Train a language model on a large general purpose corpus such as Wikipedia | Fine-tune the language model on the text your are working with - the style is most likely different than a Wikipedia article | Use the encoder of the fine-tuned language to transform text to feature vectors, and finally add a linear classifier on top to predict the class of the review. | In this post we&#39;ll focus on step 1 and 2, the language model and the fine-tuning. Training a language model from scratch is a bit of work. First you have to get the data to train it, and the training will also take a long time. Luckily the the fast.ai language model zoo already lists a pretrained language model for Norwegian. Note that this is a ULMFiT model zoo, so we expect to find weights for a AWD-LSTM. See this post to better understand how to customize an AWD-LSTM with fastai. . Let&#39;s first grab the dataset from a previous post. It&#39;s available as a csv from github: . df = pd.read_csv(&#39;https://raw.githubusercontent.com/hallvagi/dl-explorer/master/uploads/norec.csv&#39;) df.head(3) . filename rating title split sentiment text . 0 html/train/000000.html | 6 | Rome S02 | train | positive | Den andre og siste sesongen av Rome er ute på DVD i Norge. Om du så sesong 1, vet du at du har noe stort i vente. Har du aldri sett Rome før, stikk ut og kjøp begge sesongene. Dette er nemlig en av verdens beste tv-serier, og etter å ha sett de fire første episodene av sesong 2, konstaterer jeg at kvaliteten ser ut til å holde seg på et nesten overraskende høyt nivå! Sesong 2 starter nøyaktig der sesong 1 sluttet. Julius Cæsar ligger myrdet i Senatet og Lucius Vorenus hulker over liket av Neobie. Så blir historien enda mørkere. Marcus Antonius tar over styringen av Roma, men utfordres fra ... | . 1 html/train/000001.html | 6 | Twin Peaks - definitive gold box edition | train | positive | Tv-serien Twin Peaks, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen. Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men Twin Peaks sikrer dem evig liv. Serien handler om et mordmysterium i den lille byen Twin Peaks, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funn... | . 2 html/train/000002.html | 6 | The Wire (sesong 1-4) | train | positive | I neste uke kommer sesong 5 av tv-serien ”The Wire” på DVD. 2008 har for meg vært sterkt preget av denne serien. Hjemme hos oss begynte vi med sesong 1 i vår. Da hadde jeg i lengre tid hørt panegyriske lovord om serien fra både venner og media. Vi ble også fanget av skildringene av purk og skurk i Baltimore, og pløyde oss igjennom alt til og med sesong 4 på sensommeren. Jeg vil ikke gå så langt som å kalle det ”verdens beste serie”, som noen har gjort, men det er ingen tvil om at dette er noe av det bedre som er blitt vist på tv! Serien forteller om en gruppe politietterforskere som samles... | . Downloading weights . The pretrained weights we want to use is located in this repo. There is some information listed here: . The weights were trained on 90% of all text in the corresponding language wikipedia as per 3. July 2018. The remaining 10% was used for validation. | Norwegian: Trained on 80,284,231 tokens, and validated on 8,920,387 tokens. We achieve a perplexity of 26.31 | . And file descriptions: . enc.h5 Contains the weights in &#39;Hierarchical Data Format&#39; | enc.pth Contains the weights in &#39;Pytorch model format&#39; | itos.pkl (Integers to Strings) contains the vocabulary mapping from ids (0 - 30000) to strings | . It looks like we will need the enc.pth (fastai is built on top of PyTorch) and the vocabulary (itos.pkl). But how do we actually load the model? The repo doesn&#39;t really specify this part, so let&#39;s see if we can figure it out. First we&#39;ll download and extract the data to a desired location and have a look at the files: . path = Path(&#39;~/.fastai/data/norec&#39;) # choses a path of your liking! os.makedirs(path/&#39;models&#39;, exist_ok=True) model_url = &#39;https://www.dropbox.com/s/lwr5kvbxri1gvv9/norwegian.zip&#39; !wget {model_url} -O {path/&#39;models/norwegian.zip&#39;} -q !unzip -q {path/&#39;models/norwegian.zip&#39;} -d {path/&#39;models&#39;} . Path.BASE_PATH = path # paths are printed relative to the BASE_PATH (path/&#39;models&#39;).ls() . (#5) [Path(&#39;models/norwegian_wgts.h5&#39;),Path(&#39;models/norwegian_enc.pth&#39;),Path(&#39;models/norwegian.zip&#39;),Path(&#39;models/norwegian_enc.h5&#39;),Path(&#39;models/norwegian_itos.pkl&#39;)] . Vocabulary . The first file we want to check out is the norwegian_itos.pkl. This the vocabulary of the model, that is, the words and tokens it&#39;s able to recognize. itos means integer-to-string. The index of a particular token in the list is the key to that token. . with open(path/&#39;models/norwegian_itos.pkl&#39;, &#39;rb&#39;) as f: itos = pickle.load(f) itos[:10], itos[-5:], len(itos) . ([&#39;_unk_&#39;, &#39;_pad_&#39;, &#39;.&#39;, &#39;i&#39;, &#39;,&#39;, &#39;og&#39;, &#39; n n&#39;, &#39;av&#39;, &#39;som&#39;, &#39;en&#39;], [&#39;learning&#39;, &#39;initiativtager&#39;, &#39;forskningsleder&#39;, &#39;devils&#39;, &#39;graeme&#39;], 30002) . The very first token, i.e. index 0, is _unk_ or unknown. The other tokens in the first part of the list are common words such as &#39;i&#39; (in) and &#39;og&#39; (and). Among the final tokens there are even some English words. This is not really surprising since Norwegian has &quot;borrowed&quot; several words from English. It seems, however, that the special tokens for unknown and padding (_unk_ and _pad_) are different than the fastai defaults: . print(defaults.text_spec_tok) . [&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;xxbos&#39;, &#39;xxeos&#39;, &#39;xxfld&#39;, &#39;xxrep&#39;, &#39;xxwrep&#39;, &#39;xxup&#39;, &#39;xxmaj&#39;] . Will this cause issues later? . The weights . Secondly, let&#39;s have a look at the weights. We&#39;ll load it with pyTorch. . enc = torch.load(path/&#39;models/norwegian_enc.pth&#39;) enc.keys() . odict_keys([&#39;encoder.weight&#39;, &#39;encoder_dp.emb.weight&#39;, &#39;rnns.0.weight_hh_l0_raw&#39;, &#39;rnns.0.module.weight_ih_l0&#39;, &#39;rnns.0.module.weight_hh_l0&#39;, &#39;rnns.0.module.bias_ih_l0&#39;, &#39;rnns.0.module.bias_hh_l0&#39;, &#39;rnns.1.weight_hh_l0_raw&#39;, &#39;rnns.1.module.weight_ih_l0&#39;, &#39;rnns.1.module.weight_hh_l0&#39;, &#39;rnns.1.module.bias_ih_l0&#39;, &#39;rnns.1.module.bias_hh_l0&#39;, &#39;rnns.2.weight_hh_l0_raw&#39;, &#39;rnns.2.module.weight_ih_l0&#39;, &#39;rnns.2.module.weight_hh_l0&#39;, &#39;rnns.2.module.bias_ih_l0&#39;, &#39;rnns.2.module.bias_hh_l0&#39;]) . It&#39;s a dictionary with keys, and the keys are the names of the layers of the model. We can see that is has an embedding layer (named &#39;encoder&#39;), and three RNNs(LSTMs more precisely) with various descriptions. We recognize the three layer LSTM from the AWD-LSTM and the ULMFiT paper. So we must make sure that the model we set up matches matches this. Let&#39;s have a look at the dimensions of the weights. . for k,v in enc.items(): print(k,&quot; t&quot;, v.shape) . encoder.weight torch.Size([30002, 400]) encoder_dp.emb.weight torch.Size([30002, 400]) rnns.0.weight_hh_l0_raw torch.Size([4600, 1150]) rnns.0.module.weight_ih_l0 torch.Size([4600, 400]) rnns.0.module.weight_hh_l0 torch.Size([4600, 1150]) rnns.0.module.bias_ih_l0 torch.Size([4600]) rnns.0.module.bias_hh_l0 torch.Size([4600]) rnns.1.weight_hh_l0_raw torch.Size([4600, 1150]) rnns.1.module.weight_ih_l0 torch.Size([4600, 1150]) rnns.1.module.weight_hh_l0 torch.Size([4600, 1150]) rnns.1.module.bias_ih_l0 torch.Size([4600]) rnns.1.module.bias_hh_l0 torch.Size([4600]) rnns.2.weight_hh_l0_raw torch.Size([1600, 400]) rnns.2.module.weight_ih_l0 torch.Size([1600, 1150]) rnns.2.module.weight_hh_l0 torch.Size([1600, 400]) rnns.2.module.bias_ih_l0 torch.Size([1600]) rnns.2.module.bias_hh_l0 torch.Size([1600]) . We notice that the hidden size is different than the fastai default of 1152, but apart from that everything looks fine. Let&#39;s save a few weights from the embedding layer to compare with our final model. . sample_weights = enc[&#39;encoder.weight&#39;][0][:5] sample_weights . tensor([0.5711, 0.2321, 0.2601, 0.9425, 0.0901]) . Load the weights into our model . First we have to make sure that our data loader uses our custom vocabulary instead of doing tokenization on its own, so we pass text_vocab = itos. We also set is_lm = True since we want a language model and not a classifier. We use the basic factory method, since we have no need of customization at this point. . dls_lm = TextDataLoaders.from_df(df, text_col=&#39;text&#39;, text_vocab=itos, is_lm=True, valid_pct=0.1) dls_lm.show_batch(max_n=3) . text text_ . 0 _unk_ _unk_ film : i likhet med _unk_ alejandro _unk_ gonzález _unk_ _unk_ « _unk_ » er _unk_ olivier _unk_ _unk_ &#39; « _unk_ _unk_ maria » et forsøk på å _unk_ spørsmålet om hva som skjer med _unk_ i en tid der de ambisiøse _unk_ _unk_ til tv - skjermen og amerikanske _unk_ dominerer både _unk_ og _unk_ . _unk_ men der « _unk_ » er en high _unk_ - film | _unk_ film : i likhet med _unk_ alejandro _unk_ gonzález _unk_ _unk_ « _unk_ » er _unk_ olivier _unk_ _unk_ &#39; « _unk_ _unk_ maria » et forsøk på å _unk_ spørsmålet om hva som skjer med _unk_ i en tid der de ambisiøse _unk_ _unk_ til tv - skjermen og amerikanske _unk_ dominerer både _unk_ og _unk_ . _unk_ men der « _unk_ » er en high _unk_ - film med | . 1 på først : _unk_ fordi den franske byen _unk_ calais ligger ved kysten på det stedet hvor _unk_ den engelske kanal er på sitt smaleste , har den blitt invadert av illegale innvandrere fra land som _unk_ afghanistan og _unk_ irak , som etter en _unk_ ferd fra hjemlandet vil forsøke å krysse kanalen og nå _unk_ storbritannia . _unk_ ofte skjuler de seg i lastebiler , noen forsøker å svømme over | først : _unk_ fordi den franske byen _unk_ calais ligger ved kysten på det stedet hvor _unk_ den engelske kanal er på sitt smaleste , har den blitt invadert av illegale innvandrere fra land som _unk_ afghanistan og _unk_ irak , som etter en _unk_ ferd fra hjemlandet vil forsøke å krysse kanalen og nå _unk_ storbritannia . _unk_ ofte skjuler de seg i lastebiler , noen forsøker å svømme over . | . 2 et stort og _unk_ glimt i øyet . _unk_ de spiller aldri hovedrollen selv . _unk_ det er noen år siden herrene _unk_ nils _unk_ _unk_ og _unk_ ronny _unk_ kristoffersen gjorde « _unk_ » . _unk_ nå er de tilbake igjen . _unk_ godt er det . i ni programmer følger vi _unk_ nils og _unk_ ronny gjennom _unk_ colombia , _unk_ gaza , _unk_ england og usas _unk_ _unk_ _unk_ | stort og _unk_ glimt i øyet . _unk_ de spiller aldri hovedrollen selv . _unk_ det er noen år siden herrene _unk_ nils _unk_ _unk_ og _unk_ ronny _unk_ kristoffersen gjorde « _unk_ » . _unk_ nå er de tilbake igjen . _unk_ godt er det . i ni programmer følger vi _unk_ nils og _unk_ ronny gjennom _unk_ colombia , _unk_ gaza , _unk_ england og usas _unk_ _unk_ _unk_ _unk_ | . This looks pretty good. We can recognize our _unk token for example. We also see that the label column, that is the &quot;text&quot; column, is offset by 1 token from the input. This makes sense since the goal of a language model is to predict the next word in a sequence. . The next step is to configure the AWD-LSTM architecture. Let&#39;s have a look at the default config: . awd_lstm_lm_config . {&#39;emb_sz&#39;: 400, &#39;n_hid&#39;: 1152, &#39;n_layers&#39;: 3, &#39;pad_token&#39;: 1, &#39;bidir&#39;: False, &#39;output_p&#39;: 0.1, &#39;hidden_p&#39;: 0.15, &#39;input_p&#39;: 0.25, &#39;embed_p&#39;: 0.02, &#39;weight_p&#39;: 0.2, &#39;tie_weights&#39;: True, &#39;out_bias&#39;: True} . Most of these look good, but we will change the n_hid to 1150. Note also the pad_token=1. This is the index of the padding token, and from our itos above we see that itos[1] = _pad_ . awd_lstm_lm_config[&#39;n_hid&#39;] = 1150 . Now we can pass the config to our learner object. Notice that we set pretrained=False, we want to load our own weights. The final .to_fp16() means that the model is trained with mixed precision (16 bit floating point) which can often speed up training quite a bit. . learn_lm = language_model_learner(dls_lm, arch=AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, config=awd_lstm_lm_config, pretrained=False).to_fp16() . The model summary now looks correct: . learn_lm.model . SequentialRNN( (0): AWD_LSTM( (encoder): Embedding(30002, 400, padding_idx=1) (encoder_dp): EmbeddingDropout( (emb): Embedding(30002, 400, padding_idx=1) ) (rnns): ModuleList( (0): WeightDropout( (module): LSTM(400, 1150, batch_first=True) ) (1): WeightDropout( (module): LSTM(1150, 1150, batch_first=True) ) (2): WeightDropout( (module): LSTM(1150, 400, batch_first=True) ) ) (input_dp): RNNDropout() (hidden_dps): ModuleList( (0): RNNDropout() (1): RNNDropout() (2): RNNDropout() ) ) (1): LinearDecoder( (decoder): Linear(in_features=400, out_features=30002, bias=True) (output_dp): RNNDropout() ) ) . The weights of our model have been initialized randomly, so they should not match at the moment. Let&#39;s compare our sample weights from the above section with those from our language model: . learn_lm.model.state_dict()[&#39;0.encoder.weight&#39;][0][:5].cpu(), sample_weights . (tensor([-0.0005, -0.0117, 0.0255, 0.0342, 0.0455]), tensor([0.5711, 0.2321, 0.2601, 0.9425, 0.0901])) . But now we should be able to load the encoder: . learn_lm.load_encoder(path/&#39;models/norwegian_enc&#39;) . &lt;fastai2.text.learner.LMLearner at 0x7f518b3bfc10&gt; . It worked! We can also see that the weights match: . learn_lm.model.state_dict()[&#39;0.encoder.weight&#39;][0][:5].cpu(), sample_weights . (tensor([0.5711, 0.2321, 0.2601, 0.9425, 0.0901]), tensor([0.5711, 0.2321, 0.2601, 0.9425, 0.0901])) . But are we able to predict any useful text? . learn_lm.predict(&#39;Hovedstaden i Norge er&#39;) # the captical of norway is . ValueError Traceback (most recent call last) &lt;ipython-input-36-d3bf635eccab&gt; in &lt;module&gt; -&gt; 1 learn_lm.predict(&#39;Hovedstaden i Norge er&#39;) ~/git/fastai2/fastai2/text/learner.py in predict(self, text, n_words, no_unk, temperature, min_p, no_bar, decoder, only_last_word) 159 self.model.reset() 160 idxs = idxs_all = self.dls.test_dl([text]).items[0].to(self.dls.device) --&gt; 161 if no_unk: unk_idx = self.dls.vocab.index(UNK) 162 for _ in (range(n_words) if no_bar else progress_bar(range(n_words), leave=False)): 163 with self.no_bar(): preds,_ = self.get_preds(dl=[(idxs[None],)]) ValueError: &#39;xxunk&#39; is not in list . What is the problem now? We see that predict() by default has no_unk=True. The error message tells us that the library tries to get the index of the UNK token. The UNK token is as we noted earlier different in our itos (vocabulary) than that what the library expects . UNK, itos[0] . (&#39;xxunk&#39;, &#39;_unk_&#39;) . This is not really a problem for our model. The models only sees the underlying numbers and indexes, and they are still correct. But if we want to use most of the convenience functions of the fastai2 library, we either have to customise the code, or simpler still, change the vocab. . Replace special tokens in vocab . So let&#39;s look through our itos and see if we can find any special tokens: . print(defaults.text_spec_tok) # fastai defaults . [&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;xxbos&#39;, &#39;xxeos&#39;, &#39;xxfld&#39;, &#39;xxrep&#39;, &#39;xxwrep&#39;, &#39;xxup&#39;, &#39;xxmaj&#39;] . Let&#39;s look for tokens that contains an underscore _ . _toks = [token for token in itos if &#39;_&#39; in token] _toks[:5] . [&#39;_unk_&#39;, &#39;_pad_&#39;, &#39;t_up&#39;, &#39;tk_rep&#39;, &#39;formula_1&#39;] . And then for tokens that begin with an x. We use a simple regex to check for x in the beginning of the token. . x_toks = [token for token in itos if re.match(r&#39;^x&#39;, token) != None] x_toks[:5] . [&#39;xfld&#39;, &#39;xbos&#39;, &#39;x&#39;, &#39;xii&#39;, &#39;xi&#39;] . &#39;unk&#39;, &#39;pad&#39;, &#39;xfld&#39;, &#39;xbos&#39; seems pretty obvious. But I&#39;m less sure of eg. &#39;t_up&#39; and &#39;tk_rep&#39;. So we replace a bit conservatively: . to_replace = _toks[:2]+x_toks[:2] to_replace . [&#39;_unk_&#39;, &#39;_pad_&#39;, &#39;xfld&#39;, &#39;xbos&#39;] . replace_with = defaults.text_spec_tok[:2]+defaults.text_spec_tok[7:8]+defaults.text_spec_tok[2:3] replace_with . [&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;xxup&#39;, &#39;xxbos&#39;] . Then we loop trough our itos and replace the selected tokens: . for tok_remove, tok_insert in zip(to_replace, replace_with): idx = itos.index(tok_remove) itos[idx] = tok_insert . To verify that we did things correct: . idxs = [itos.index(token) for token in replace_with] idxs . [0, 1, 31, 32] . [itos[idx] for idx in idxs] . [&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;xxup&#39;, &#39;xxbos&#39;] . Language model: final cut . Let&#39;s make yet another version of our dataloader and language learner. . dls_lm = TextDataLoaders.from_df(df, text_col=&#39;text&#39;, text_vocab=itos, is_lm=True, valid_pct=0.1) learn_lm = language_model_learner(dls_lm, arch=AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, config=awd_lstm_lm_config, pretrained=False).to_fp16() learn_lm.load_encoder(path/&#39;models/norwegian_enc&#39;) . &lt;fastai2.text.learner.LMLearner at 0x7f511dc73b50&gt; . TEXT = &quot;Hovedstaden i Norge er&quot; # the capital of norway is preds = [learn_lm.predict(TEXT, 40, temperature=0.75) for _ in range(3)] preds . [&#39;hovedstaden i norge er en by med stor politisk status fra 1891 til 1955 . n n 1 havene n n havene ( engelsk &#34; american mountains &#34; , engelsk : &#34; mountain peninsula &#34; eller &#34; t_up ngc &#34; eller &#34; øvre&#39;, &#39;hovedstaden i norge er n n 1 hans - herman n n hans - günther ( født 28 . desember 1812 i rostock , død 6 . desember 1888 i leipzig ) var en tysk økonom , som var general og generaldirektør for&#39;, &#39;hovedstaden i norge er oppkalt etter ham . i tillegg er det ett av norges høyeste byer og omegn . i norge er det også en rekke andre land : n n historie . n det er i dag i norge ikke lenger en kommunal&#39;] . Well, that kind of makes sense. LSTMs are less capable at generating text than the more complex transformer architectures, but our concern in this particular case is how well we eventually do sentiment classification. . Fine tune model . Now we are finally ready to fine tune our language model. We&#39;ll use the &quot;standard&quot; training regime from the documentation and the fast.ai courses. That is 1 epoch where only the linear layers in the head of the model are trainable, and finally 10 epochs with all layers unfrozen. . learn_lm.lr_find() . SuggestedLRs(lr_min=0.02089296132326126, lr_steep=1.3182567499825382e-06) . A learning rate of 1e-2 seem to be a safe choice: . learn_lm.fit_one_cycle(1, 1e-2) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.189573 | 3.936918 | 0.315999 | 51.260372 | 02:35 | . Next we will unfreeze all layers and train for 10 epochs at a reduced learning rate. The idea is that once we unfreeze the lower layers of our model we should make smaller changes to avoid catastrophic forgetting. We will go for a leraning rate of 1e-3. One can always test if longer training improves results, but in this case we will simply assumes that 10 epochs is good enough. . learn_lm.unfreeze() learn_lm.lr_find() . SuggestedLRs(lr_min=0.0003019951749593019, lr_steep=6.309573450380412e-07) . learn_lm.fit_one_cycle(10, 1e-3) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.997230 | 3.822397 | 0.327288 | 45.713673 | 02:47 | . 1 | 3.954266 | 3.777179 | 0.330856 | 43.692589 | 02:48 | . 2 | 3.878042 | 3.725606 | 0.337251 | 41.496380 | 02:38 | . 3 | 3.814440 | 3.686008 | 0.340614 | 39.885315 | 02:47 | . 4 | 3.785215 | 3.656684 | 0.344700 | 38.732697 | 02:41 | . 5 | 3.729483 | 3.635671 | 0.346845 | 37.927280 | 02:43 | . 6 | 3.698734 | 3.618694 | 0.348383 | 37.288837 | 02:46 | . 7 | 3.654537 | 3.607326 | 0.350624 | 36.867321 | 02:48 | . 8 | 3.635801 | 3.602736 | 0.351033 | 36.698505 | 02:49 | . 9 | 3.618312 | 3.601928 | 0.351133 | 36.668865 | 02:50 | . . Note: We&#8217;re only training for about 30 minutes on a single GPU. . We will save the model and the encoder for future use. The save() method save objects to path/&#39;models&#39; by default. The encoder will be used in our classifier in the next post. . learn_lm.save(&#39;finetuned_model&#39;) learn_lm.save_encoder(&#39;finetuned_encoder&#39;) . Is the model any good at predicting text? . TEXT = &quot;Denne filmen er et godt eksempel på&quot; # this film is a good example of preds = [learn_lm.predict(TEXT, 40, temperature=0.75) for _ in range(3)] preds . [&#39;xxunk denne filmen er et godt eksempel på hvordan serien er blitt laget med en amerikansk komedie . i filmen står det en tydelig og effektiv følelse av at en serie med tretten mennesker , som har mistet kontrollen over de to første episodene , har fått en&#39;, &#39;xxunk denne filmen er et godt eksempel på hva som skjer når den kommer i nærheten av en tsunami . i verste fall skjer det med den virkelige verden , hvor det som skjer , er det som gjør at den blir rammet av et stadig forvirrende ,&#39;, &#39;xxunk denne filmen er et godt eksempel på at det er vanskelig å forestille seg hva som egentlig foregår . i visse scener er dette en underholdende og ytterst underholdende film , i norsk forstand . JON GORE « the THAT IN&#39;] . This is by no means as impressive as the recent transformer models, but the model certainly understands language fairly well. Also, our particular use case isn&#39;t really text generation, but sentiment classification. Transformers only do marginally better than ULMFiT according to Papers with code on the similar IMDB classification task. Classification will be the topic of an upcoming post. .",
            "url": "https://hallvagi.github.io/dl-explorer/nlp/fastai/lstm/ulmfit/2020/04/20/ULMFiT_langmod.html",
            "relUrl": "/nlp/fastai/lstm/ulmfit/2020/04/20/ULMFiT_langmod.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "A code-first inspection of the AWD-LSTM",
            "content": "from fastai2.text.all import * . ULMFiT . In the previous post we explored the Norec Norwegian language corpus. We grabbed the reviews for films and TV-shows, parsed the html-text and created labels based on the ratings. In the next few posts I want to use ULMFiT and other methods to predict the sentiment of the reviews based on the text. ULMFiT has three main steps: . Train a language model on a large general purpose corpus such as Wikipedia | Fine-tune the language model on the text your are working with - the style is most likely different than a Wikipedia article | Combine the encoder of the fine-tuned language model with a linear classifier to predict the class of your text | The core of the ULMFiT method is a type of Recurrent neural network (RNN) called AWD-LSTM. AWD-LSTM is a special kind of Recurrent neural network (RNN) with tuned dropout parameters among other. We need to look into this architecture before we continue with our modeling. For an explanation of what an LSTM actually is i suggest checking out this blog post by Chris Olah. In general, most of Chris&#39; posts and papers are worth reading! . How to set up an AWD-LSTM with fastai . Let&#39;s first start by inspecting fastai&#39;s language_model_learner. It&#39;s a learner class designed to be used for language models, and holds both dataloaders and the architecture along with various hyperparameters. We can use the doc() method to show us the documentation: . doc(language_model_learner) . The documentation tells us that we can pass arch = AWD-LSTM and modify the awd_lstm_lm_config to customize the architecture. The config dictionary specifies various hyperparameters and settings inspired by the aforementioned AWD-LSTM paper. By changing this dictionary we can customize our AWD-LSTM to fit our specific needs: . awd_lstm_lm_config . {&#39;emb_sz&#39;: 400, &#39;n_hid&#39;: 1152, &#39;n_layers&#39;: 3, &#39;pad_token&#39;: 1, &#39;bidir&#39;: False, &#39;output_p&#39;: 0.1, &#39;hidden_p&#39;: 0.15, &#39;input_p&#39;: 0.25, &#39;embed_p&#39;: 0.02, &#39;weight_p&#39;: 0.2, &#39;tie_weights&#39;: True, &#39;out_bias&#39;: True} . Let&#39;s check the documentation and source code of the AWD-LSTM class. You can check the source code directly in the notebook by appending a ?? behind the method name: . AWD_LSTM?? . The source code shows us a few interesting lines we&#39;ll look more into in the next few sections: . self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token) | self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir, bidir, weight_p, l) for l in range(n_layers)]) | self.input_dp = RNNDropout(input_p) | self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)]) | . Note: the embedding is called encoder in the code above. The name encoder is also fastai lingo for the entire RNN-part of the architecture. The linear layers added on top for the classifier is called decoder. Neither the ULMFiT or AWD-LSTM paper uses the term encoder or decoder though. . But what is an embedding? . Once again I&#39;ll be lazy and rather refer to another blog that explains embeddings in detail. The blog is by Jay Alammar and has explanations of many deep learning and NLP concepts. The essence is that we&#39;ll turn each token in our vocabulary into a vector of some size that represents various aspects of that token. The weights of this vector will be trainable and gives our neural network a lot of flexibility in assigning various properties to each token. . The embedding is created by: self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token) Here we see that fastai is built on top of pyTorch and relies on pyTorch&#39;s fundamental methods in its own code. The encoder layer is a call to nn.Embedding, see documentation. Let&#39;s create an embedding of size 10x3 with padding_idx = 0: . embedding = nn.Embedding(num_embeddings=10, embedding_dim=3, padding_idx=0) embedding.weight . Parameter containing: tensor([[ 0.0000, 0.0000, 0.0000], [ 1.6721, -1.3130, 0.6414], [ 1.1675, 0.1174, 1.8511], [-0.3341, -1.0047, -0.8467], [-0.7737, -0.3947, -1.5273], [-1.1472, -0.0429, -0.0994], [-1.0594, 1.3725, 0.3796], [ 0.1682, 0.7212, 0.9494], [ 1.2791, 0.1334, -0.5075], [ 0.4486, 0.4936, 0.2588]], requires_grad=True) . The embedding now has 10 vectors of length 3 with randomly initialized weights. Note that the first one (index 0) is all 0. This is because 0 is our padding index. Next we&#39;ll pass some some sample data inp, and inspect the result. We can think of our input as the index of words in a dictionary. E.g. 1=&#39;this&#39;, 7=&#39;is&#39;, 4=&#39;not&#39; and 3=&#39;easy&#39;. 0 will be our padding token. The padding token is a special token that is used to ensure that some text has a certain length. This is useful when stacking various pieces of text into a batch where sizes needs to match. . inp = torch.LongTensor([1,7,4,3,0,0]) emb = embedding(inp) emb . tensor([[ 1.6721, -1.3130, 0.6414], [ 0.1682, 0.7212, 0.9494], [-0.7737, -0.3947, -1.5273], [-0.3341, -1.0047, -0.8467], [ 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000]], grad_fn=&lt;EmbeddingBackward&gt;) . We see that the embedding produced by feeding the input corresponds to the weights of our original embedding. That is, index 1 of inp is the item &#39;7&#39;. So emb[1] is basically a lookup for embedding.weight[7]. . embedding.weight[7], emb[1] . (tensor([0.1682, 0.7212, 0.9494], grad_fn=&lt;SelectBackward&gt;), tensor([0.1682, 0.7212, 0.9494], grad_fn=&lt;SelectBackward&gt;)) . To summarize:We&#39;ll need an embedding with the number of embeddings equal to our vocabulary size, and embedding size of 400 and a padding token-id which corresponds to whichever token has been used as padding in our vocabulary. . Compostion of the RNN-layers . Secondly we create a list of RNN-layers with various dimensions: . self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir, bidir, weight_p, l) for l in(n_layers)]) . The code stacks RNN-layers of embedding size x hidden size for the first layer, and hidden size x embedding size for the final. Let&#39;s verify that this works for various number of layers: . AWD_LSTM(vocab_sz=10_000, emb_sz=400, n_hid=1152, n_layers=2) . AWD_LSTM( (encoder): Embedding(10000, 400, padding_idx=1) (encoder_dp): EmbeddingDropout( (emb): Embedding(10000, 400, padding_idx=1) ) (rnns): ModuleList( (0): WeightDropout( (module): LSTM(400, 1152, batch_first=True) ) (1): WeightDropout( (module): LSTM(1152, 400, batch_first=True) ) ) (input_dp): RNNDropout() (hidden_dps): ModuleList( (0): RNNDropout() (1): RNNDropout() ) ) . AWD_LSTM(vocab_sz=10_000, emb_sz=400, n_hid=1152, n_layers=5) . AWD_LSTM( (encoder): Embedding(10000, 400, padding_idx=1) (encoder_dp): EmbeddingDropout( (emb): Embedding(10000, 400, padding_idx=1) ) (rnns): ModuleList( (0): WeightDropout( (module): LSTM(400, 1152, batch_first=True) ) (1): WeightDropout( (module): LSTM(1152, 1152, batch_first=True) ) (2): WeightDropout( (module): LSTM(1152, 1152, batch_first=True) ) (3): WeightDropout( (module): LSTM(1152, 1152, batch_first=True) ) (4): WeightDropout( (module): LSTM(1152, 400, batch_first=True) ) ) (input_dp): RNNDropout() (hidden_dps): ModuleList( (0): RNNDropout() (1): RNNDropout() (2): RNNDropout() (3): RNNDropout() (4): RNNDropout() ) ) . We see the first and final layers have similar dimensions in the two examples. . To summarize:We&#39;ll use a 3 layer network with input and output dimensions of (400, 1152), (1152, 1152) and (1152, 400) as in the AWD-LSTM paper. This should be handled automatically by the library. . nn.LSTM . In the module list above, the layers are actually WeightDropout layers. We can verify this from the hidden constructor method that is called when the RNNs are being created. First, a regular nn.LSTM layer is created before being passed to the WeightDropout module. . def _one_rnn(self, n_in, n_out, bidir, weight_p, l): &quot;Return one of the inner rnn&quot; rnn = nn.LSTM(n_in, n_out, 1, batch_first=True, bidirectional=bidir) return WeightDropout(rnn, weight_p) . Lets have a look at an example from the nn.LSTM documentation, also see source code here. We&#39;ll make a 1 layer LSTM with input size of 10 and hidden size of 20. Note that in the AWD-LSTM case the input size is equal to the embedding size (400 by default). . inp_s = 10 # input size hid_s = 20 # hidden size . lstm = nn.LSTM(input_size = inp_s, hidden_size = hid_s, num_layers=1) . The documentation details that the LSTM expects input in the form of input(seq_len, batch, input_size). Seq_len is the length of the part of the text the model will see in each iteration (seq_len = 72 by default in fastais language_model_learner, that is 72 tokens). The batch size is the number of documents the models sees in each iteration. . h0 and c0 are the inital hidden and cell states (set to 0 if not provided). The documentation specifiy their shapes as: (num_layers * num_directions, batch, hidden_size). . bs = 16 n_l, n_d = 1, 1 # we are testing a 1 layer and 1 direction lstm seq_len = 5 . inp = torch.randn(seq_len, bs, inp_s) h0 = torch.randn(n_l*n_d, bs, hid_s) c0 = torch.randn(n_l*n_d, bs, hid_s) . inp.shape, h0.shape, c0.shape . (torch.Size([5, 16, 10]), torch.Size([1, 16, 20]), torch.Size([1, 16, 20])) . The output from the LSTM should be a tuple of output, (h_n, c_n)where output has shape given by: (seq_len, batch, num_directions * hidden_size): . out, (hn, cn) = lstm(inp) out.shape . torch.Size([5, 16, 20]) . Let&#39;s also check the actual shape of our weights by looping through the state_dict(): . [(key, lstm.state_dict()[key].shape) for key in lstm.state_dict().keys()] . [(&#39;weight_ih_l0&#39;, torch.Size([80, 10])), (&#39;weight_hh_l0&#39;, torch.Size([80, 20])), (&#39;bias_ih_l0&#39;, torch.Size([80])), (&#39;bias_hh_l0&#39;, torch.Size([80]))] . Here we recognize the input size of 10 and hidden size of 20, but where does the 80 come from? The documentation specifies that the weights will be of dimension (4*hidden_size, input_size). The &#39;4&#39; is called gate_size, and we can find this in the source code for the base RNN module also: . if mode == &#39;LSTM&#39;: gate_size = 4 * hidden_size . To summarize:we expect two sets of weights and biases per LSTM: weight_ih_l0 with a shape of (4hidden_size, input_size) . weight_hh_l0 with a shape of (4*hidden_size, hidden_size) | bias_ih_l0 with a shape of (4*hidden_size) | bias_hh_l0 with a shape of (4*hidden_size) | . WeightDropout . We can see from the _one_rnn that the nn.LSTM is transformed to a WeightDropout module. The documentation describes the module as &#39;A module that warps another layer in which some weights will be replaced by 0 during training&#39;. From the source code we can see that it&#39;s the weight_hh_l0 weights that will be modified, and that these weights are duplicated with suffix &#39;raw&#39; in the WeightDropout module: self.register_parameter(f&#39;{layer}_raw&#39;, nn.Parameter(w.data)). . Let&#39;s see if we can verify this by first checking the weights from the lstm from the above section. Of the 80*20 = 1600 weights in the hh_l0 layer, none are 0: . orig_wts = getattr(lstm, &#39;weight_hh_l0&#39;) orig_wts.shape, (orig_wts == 0.).sum() . (torch.Size([80, 20]), tensor(0)) . But if pass the lstm through the WeightDroput module, approximately half of the 1600 weights are set to 0. Note that we have to call the model on the input since the weights are only reset during the forward pass. The weights are also only reset for the WeighDropout&#39;s internal LSTM module, while a copy with suffix &#39;_raw&#39; retains the original weights. . wd = WeightDropout(lstm, weight_p=0.5) _,_ = wd(inp) # we don&#39;t need the output in this case . The .module attribute of the wd object is our original LSTM: . wd.module . LSTM(10, 20) . And about half its weights have been set to 0: . (getattr(wd.module, &#39;weight_hh_l0&#39;)==0.0).sum() . tensor(826) . The original weights from the lstm matches the &#39;_raw&#39; weights of the WeightDropout module: . test_eq(orig_wts, getattr(wd, &#39;weight_hh_l0_raw&#39;)) . The new layers are as expected: . wd.state_dict().keys() . odict_keys([&#39;weight_hh_l0_raw&#39;, &#39;module.weight_ih_l0&#39;, &#39;module.bias_ih_l0&#39;, &#39;module.bias_hh_l0&#39;]) . RNN dropout . Finally several RNNDropout layers are being created - one for the input and one for each LSTM. This dropout is applied to the input embedding and on the output of each LSTM. We can test the functionality with inp from the above section. . dp = RNNDropout(0.5) dp_out = dp(inp) inp.shape, dp_out.shape . (torch.Size([5, 16, 10]), torch.Size([5, 16, 10])) . The documentation also says: &#39;Dropout with probability p that is consistent on the seq_len dimension.&#39; In our input from the above section, seq_len is the first dimension (index 0), and if we check for items equaling 0 and sum along the second dimension (index 1) we see that the same tokens are dropped out for the entire batch (our sample batch size is 16) consistently in approximately half of the instances. . (dp_out == 0).sum((1)) . tensor([[ 0, 0, 16, 16, 16, 16, 0, 0, 16, 0], [ 0, 16, 16, 16, 16, 0, 0, 16, 16, 0], [ 0, 0, 0, 16, 0, 0, 0, 0, 16, 16], [ 0, 0, 16, 16, 0, 16, 0, 16, 0, 16], [ 0, 16, 0, 16, 0, 16, 16, 0, 16, 16]]) . IMDb inspection . Let&#39;s take a look at a minimal IMDB example from the fastai documentation to verify our understanding of the AWD-LSTM architecture. . imdb_path = untar_data(URLs.IMDB_SAMPLE) df = pd.read_csv(imdb_path/&#39;texts.csv&#39;) dls = TextDataLoaders.from_df(df, path=imdb_path, text_col=&#39;text&#39;, is_lm=True, valid_col=&#39;is_valid&#39;) learn = language_model_learner(dls, AWD_LSTM) . The vocab is of length 7080 and vocab index 1 is &#39;xxpad&#39;: . dls.vocab[:5], len(dls.vocab) . ([&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;xxbos&#39;, &#39;xxeos&#39;, &#39;xxfld&#39;], 7080) . In our model we recognize Embedding(7080, 400, padding_idx=1) as vocab_size x embedding size with the correct padding token. We also see that the (input, output) dimensions of our LSTM-layers are as expected, and with the expected dropout layers added. . learn.model . SequentialRNN( (0): AWD_LSTM( (encoder): Embedding(7080, 400, padding_idx=1) (encoder_dp): EmbeddingDropout( (emb): Embedding(7080, 400, padding_idx=1) ) (rnns): ModuleList( (0): WeightDropout( (module): LSTM(400, 1152, batch_first=True) ) (1): WeightDropout( (module): LSTM(1152, 1152, batch_first=True) ) (2): WeightDropout( (module): LSTM(1152, 400, batch_first=True) ) ) (input_dp): RNNDropout() (hidden_dps): ModuleList( (0): RNNDropout() (1): RNNDropout() (2): RNNDropout() ) ) (1): LinearDecoder( (decoder): Linear(in_features=400, out_features=7080, bias=True) (output_dp): RNNDropout() ) ) . The model summary shows us the default batch size of 64 and seq_len of 72. . learn.summary() . SequentialRNN (Input shape: [&#39;64 x 72&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ RNNDropout 64 x 72 x 400 0 False ________________________________________________________________ RNNDropout 64 x 72 x 1152 0 False ________________________________________________________________ RNNDropout 64 x 72 x 1152 0 False ________________________________________________________________ Linear 64 x 72 x 7080 2,839,080 True ________________________________________________________________ RNNDropout 64 x 72 x 400 0 False ________________________________________________________________ Total params: 2,839,080 Total trainable params: 2,839,080 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fa03d7cbdd0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group number 3 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback - ModelReseter - RNNRegularizer . And finally, the layer names and shapes also is consistent with a gate size of 4 (1152*4 = 4608). Note the enumeration of the layers: 0. is the encoder part of the architecture (including the embedding called encoder) and 1. is the decoder. . for key in learn.model.state_dict().keys(): print(key, &#39; t&#39;, learn.model.state_dict()[key].shape) . 0.encoder.weight torch.Size([7080, 400]) 0.encoder_dp.emb.weight torch.Size([7080, 400]) 0.rnns.0.weight_hh_l0_raw torch.Size([4608, 1152]) 0.rnns.0.module.weight_ih_l0 torch.Size([4608, 400]) 0.rnns.0.module.bias_ih_l0 torch.Size([4608]) 0.rnns.0.module.bias_hh_l0 torch.Size([4608]) 0.rnns.1.weight_hh_l0_raw torch.Size([4608, 1152]) 0.rnns.1.module.weight_ih_l0 torch.Size([4608, 1152]) 0.rnns.1.module.bias_ih_l0 torch.Size([4608]) 0.rnns.1.module.bias_hh_l0 torch.Size([4608]) 0.rnns.2.weight_hh_l0_raw torch.Size([1600, 400]) 0.rnns.2.module.weight_ih_l0 torch.Size([1600, 1152]) 0.rnns.2.module.bias_ih_l0 torch.Size([1600]) 0.rnns.2.module.bias_hh_l0 torch.Size([1600]) 1.decoder.weight torch.Size([7080, 400]) 1.decoder.bias torch.Size([7080]) .",
            "url": "https://hallvagi.github.io/dl-explorer/nlp/fastai/rnn/lstm/2020/04/17/AWD_LSTM.html",
            "relUrl": "/nlp/fastai/rnn/lstm/2020/04/17/AWD_LSTM.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Finding a Norwegian language dataset for sentiment analysis",
            "content": "I&#39;m currently taking the fast.ai online course practical deep learning for coders (to be released publicly in July 2020). As part of the studies I want to explore the fastai2 deep learning library. I will do so by testing various NLP methods. . Over the next few posts I&#39;ll got through the entire process of finding and processing data, training various models and also interpret the results. I&#39;ll try to highlight the various things I&#39;ve been struggling with or confused about. I think writing short blog posts such as these are a great way of learning new material. . The NOREC dataset . I wanted to find a dataset in my native Norwegian to analyze. NLP for languages other than English is often challenging, even though many new techniques are addressing this. After a bit of searching I found the Norec dataset. It contains Norwegian language reviews of various films, music etc. The dataset even comes with a paper that explains the setup of the data! This seems like a great case study, and is similar to the IMDB movie review sentiment analyses, which is one of the built in datasets of the fastai2 library. . The repo includes a utility library and a download.sh script. This is what you normally would want to use to ease the process of actually getting the data, but in this case I would like to do things manually. We&#39;ll be using the fastai2 library (which upon release will be renamed to fastai). . Note that import * is usually not encouraged, but the fastai2 library has defined it&#39;s __all__ variables properly, so this won&#39;t be a problem. See this file for the various imports that is part of the library. . from fastai2.text.all import * . I&#39;m using fastai2 &#39;0.0.17&#39; and fastcore &#39;0.1.17&#39;. To check the version you can uncomment the following lines: . #collapse # import fastai2, fastcore # fastai2.__version__, fastcore.__version__ . . First we&#39;ll download and extract the data from the url given in the download.sh file in the github-repo to a dest with archive_name. Note the !command syntax which runs the corresponding wget shell command. The wget command only has to be run once. . url = f&#39;http://folk.uio.no/eivinabe/norec-1.0.1.tar.gz&#39; data_path = Path(&#39;~/.fastai&#39;) dest = data_path/&#39;archive&#39; archive_name = &#39;norec.tar.gz&#39; !wget {url} -O {dest/archive_name} -q . Then we&#39;ll extract the archive with the tarfile library (imported via the fastai2 import at the beginning of the notebook). We&#39;ll extract from the archive location to data. Finally we set a new path that points to this location. . tarfile.open(dest/archive_name).extractall(data_path/&#39;data&#39;) path = data_path/&#39;data/norec&#39; Path.BASE_PATH=path # this avoids printing the entire file-path when we list files in a directory path.ls() . (#4) [Path(&#39;metadata.json&#39;),Path(&#39;conllu.tar.gz&#39;),Path(&#39;html.tar.gz&#39;),Path(&#39;README.txt&#39;)] . The archive contains a .json file with metadata, and among other, a html.tar.gz archive with our desired raw texts. We&#39;ll extract this archive too. The conllu.tar.gz contains tokenized and filtered text, and we don&#39;t need this for the time being. . tarfile.open(path/&#39;html.tar.gz&#39;).extractall(path) path.ls() . (#5) [Path(&#39;metadata.json&#39;),Path(&#39;conllu.tar.gz&#39;),Path(&#39;html.tar.gz&#39;),Path(&#39;html&#39;),Path(&#39;README.txt&#39;)] . The extracted archive is now in the data/norec/html folder, which contains the train, dev (we&#39;ll call it validation) and test split. . (path/&#39;html&#39;).ls() . (#3) [Path(&#39;html/train&#39;),Path(&#39;html/test&#39;),Path(&#39;html/dev&#39;)] . Inspect the metadata . Let&#39;s inspect the metatdata file and see if we are able to make sense of the data. But what does this json file look like? We can use the head command to have a look at the raw file contents before we attempt to read it into a pandas dataframe: . !head -n 20 {path/&#39;metadata.json&#39;} . { &#34;000000&#34;: { &#34;authors&#34;: [ &#34;Birger Vestmo&#34; ], &#34;category&#34;: &#34;screen&#34;, &#34;day&#34;: 27, &#34;excerpt&#34;: &#34;Toppen innen tv-drama akkurat nå!&#34;, &#34;id&#34;: 0, &#34;language&#34;: &#34;nb&#34;, &#34;month&#34;: 9, &#34;rating&#34;: 6, &#34;source&#34;: &#34;p3&#34;, &#34;source-category&#34;: &#34;tv&#34;, &#34;source-id&#34;: 74781, &#34;source-tags&#34;: [], &#34;split&#34;: &#34;train&#34;, &#34;tags&#34;: [ &#34;tv&#34; ], . Let&#39;s try to read the data with pandas default read_json() . pd.read_json(path/&#39;metadata.json&#39;).head(3) . 0 1 2 3 4 5 6 7 8 9 ... 705136 705137 705138 705139 705140 705141 705142 705143 705144 705145 . authors [Birger Vestmo] | [Birger Vestmo] | [Birger Vestmo] | [Birger Vestmo] | [Birger Vestmo] | [Torfinn Borkhus] | [Torfinn Borkhus] | [Torfinn Borkhus] | [Torfinn Borkhus] | [Torfinn Borkhus] | ... | [Arnfinn Bø-Rygg] | [Leif Tore Lindø] | [Leif Tore Lindø] | [Kine Hult] | [Arnfinn Bø-Rygg] | [Leif Tore Lindø] | [Arnfinn Bø-Rygg] | [Leif Tore Lindø] | [Leif Tore Lindø] | [Elisabeth Bie] | . category screen | screen | screen | screen | screen | screen | screen | screen | screen | screen | ... | music | music | music | music | music | music | music | music | music | music | . day 27 | 22 | 22 | 3 | 9 | 25 | 25 | 26 | 1 | 8 | ... | 8 | 16 | 13 | 13 | 3 | 8 | 4 | 17 | 25 | 9 | . 3 rows × 35189 columns . Not quite what we looked for! From the read_json documentation we see that we can change the orientation of the data with the orientoption. &#39;index&#39; seems to be what we look for. Note that we also could have transposed the data frame for a similar result. . There is another problem that is harder to spot: The index of our dataframe should be the string representation of the json key. This is the actual file name of the corresponding review, but it is cast to an int. This turns &#39;000000&#39; into 0. So we&#39;ll set convert_axes to False. We&#39;ll also reset the index and rename it to filename. . df = pd.read_json(path/&#39;metadata.json&#39;, orient=&#39;index&#39;, convert_axes=False) df = df.reset_index().rename(columns={&#39;index&#39;: &#39;filename&#39;}) df.head(3) . filename authors category day excerpt id language month rating source source-category source-id source-tags split tags title url year . 0 000000 | [Birger Vestmo] | screen | 27 | Toppen innen tv-drama akkurat nå! | 0 | nb | 9 | 6 | p3 | tv | 74781 | [] | train | [tv] | Rome S02 | http://p3.no/filmpolitiet/rome-02 | 2007 | . 1 000001 | [Birger Vestmo] | screen | 22 | Gull for &lt;em&gt;Twin Peaks&lt;/em&gt;-fans! | 1 | nb | 11 | 6 | p3 | tv | 74065 | [] | train | [tv] | Twin Peaks - definitive gold box edition | http://p3.no/filmpolitiet/twin-peaks-definitive-gold-box-edition | 2007 | . 2 000002 | [Birger Vestmo] | screen | 22 | The Wire vil gjøre deg avhengig, men på en god måte. | 2 | nb | 2 | 6 | p3 | tv | 73886 | [] | train | [tv] | The Wire (sesong 1-4) | http://p3.no/filmpolitiet/the-wire-sesong-1-4 | 2008 | . There are several category-like columns as explained in the paper. We will use the category column. . df[&#39;category&#39;].value_counts() . screen 13085 music 12410 literature 3526 products 3120 games 1765 restaurants 534 stage 530 sports 117 misc 102 Name: category, dtype: int64 . I&#39;m also curious about the distribution of bokmål (nb) vs nynorsk (nn). It seems the vast majority of reviews are in bokmål. . df[&#39;language&#39;].value_counts() . nb 34656 nn 533 Name: language, dtype: int64 . Get a subset of the data . We&#39;ll proceed with a subset of the data. We&#39;ll look at the screen sub category and bokmål (nb) language. The screen category contains both movie and TV-reviews. The data frame also contains several columns we won&#39;t be using now so let&#39;s select the relevant columns. We&#39;re left with ~ 13000 reviews. . screen = df.loc[(df[&#39;category&#39;]==&#39;screen&#39;) &amp; (df[&#39;language&#39;]==&#39;nb&#39;)] screen = screen.loc[:, [&#39;filename&#39;, &#39;rating&#39;, &#39;title&#39;, &#39;split&#39;]] print(screen.shape) screen.head(3) . (12924, 4) . filename rating title split . 0 000000 | 6 | Rome S02 | train | . 1 000001 | 6 | Twin Peaks - definitive gold box edition | train | . 2 000002 | 6 | The Wire (sesong 1-4) | train | . Let&#39;s also change the filename so that is gives the path to our review files. . screen[&#39;filename&#39;] = &#39;html/&#39;+screen[&#39;split&#39;]+&#39;/&#39;+screen[&#39;filename&#39;]+&#39;.html&#39; screen.sample(3) . filename rating title split . 21438 html/train/301590.html | 4 | På den smale sykkelsti | train | . 28588 html/train/600841.html | 4 | Lykken er en olivenlund | train | . 32395 html/test/702352.html | 5 | Sterk, sanselig skjønnhet fra Aldomóvar | test | . The ratings of the screen category has a slight positive skew. . screen[&#39;rating&#39;].value_counts().sort_index().plot(kind=&#39;bar&#39;); . We&#39;ll encode a rating of 1 to 3 as negative, and 5 and 6 as positive. Reviews rated 4 will be removed. We&#39;ll lose a bit of data this way, but this means that the positive and negative review are a bit more distinct, and will make down stream classification a bit simpler. This leaves us with ~8600 reviews. . screen = screen.loc[screen[&#39;rating&#39;]!=4].reset_index(drop=True) screen[&#39;sentiment&#39;]=screen[&#39;rating&#39;].apply(lambda k: &#39;positive&#39; if k&gt;=4 else &#39;negative&#39;) print(screen.shape) screen.head(3) . (8613, 5) . filename rating title split sentiment . 0 html/train/000000.html | 6 | Rome S02 | train | positive | . 1 html/train/000001.html | 6 | Twin Peaks - definitive gold box edition | train | positive | . 2 html/train/000002.html | 6 | The Wire (sesong 1-4) | train | positive | . The train, validation (dev) and test split is ok: . screen[&#39;split&#39;].value_counts(normalize=True) . train 0.803321 dev 0.101707 test 0.094973 Name: split, dtype: float64 . And the dataset is well balanced, i.e. similar amount of labels for each class. . screen[&#39;sentiment&#39;].value_counts(normalize=True) . negative 0.512713 positive 0.487287 Name: sentiment, dtype: float64 . Add text to the data frame . Let&#39;s also add the full text to the dataframe for convenience. This step is not strictly necessary, and doesn&#39;t scale to big data. The html/train folder contains html files, and the data frame gives us our filenames. . (path/&#39;html/train&#39;).ls() . (#28158) [Path(&#39;html/train/201232.html&#39;),Path(&#39;html/train/601043.html&#39;),Path(&#39;html/train/108030.html&#39;),Path(&#39;html/train/700390.html&#39;),Path(&#39;html/train/702069.html&#39;),Path(&#39;html/train/100023.html&#39;),Path(&#39;html/train/701430.html&#39;),Path(&#39;html/train/201765.html&#39;),Path(&#39;html/train/302012.html&#39;),Path(&#39;html/train/400836.html&#39;)...] . screen.head(3) . filename rating title split sentiment . 0 html/train/000000.html | 6 | Rome S02 | train | positive | . 1 html/train/000001.html | 6 | Twin Peaks - definitive gold box edition | train | positive | . 2 html/train/000002.html | 6 | The Wire (sesong 1-4) | train | positive | . Let&#39;s inspect the second file in the data frame. We expect it should be a review of Twin Peaks. We&#39;ll open the file and print the raw contents. . fn = screen.loc[1, &#39;filename&#39;] item = (path/fn) item . Path(&#39;html/train/000001.html&#39;) . with open(item) as f: review = f.read() review[:1500] . &#39;&lt;h1&gt;Twin Peaks - definitive gold box edition&lt;/h1&gt; n n&lt;p&gt;Tv-serien &lt;em&gt;Twin Peaks&lt;/em&gt;, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen.&lt;/p&gt; n n&lt;p&gt;Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men &lt;em&gt;Twin Peaks&lt;/em&gt; sikrer dem evig liv.&lt;/p&gt; n n&lt;h5&gt;Mørke hemmeligheter&lt;/h5&gt; n n&lt;p&gt;Serien handler om et mordmysterium i den lille byen &lt;em&gt;Twin Peaks&lt;/em&gt;, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funnet drept, og FBI-etterforsker Dale Cooper kommer til byen for å oppklare saken.&lt;/p&gt; n n&lt;p&gt;Her blir han betatt av det tilsynelatende enkle livet i den lille byen mellom den endeløse skogen og de høye fjellene. Men han oppdager også at byen skjuler mørke, farlige hemmeligheter.&lt;/p&gt; n n&lt;REMOVE&gt;&lt;small&gt;Anmeldelsen fortsetter under bildet.&lt;/small&gt;&lt;/REMOVE&gt; n n&lt;REMOVE&gt;[caption id=&#34;attachment_207347&#34; align=&#34;alignnone&#34; width=&#34;750&#34;]&lt;a href=&#34;http://p3.no/filmpolitiet/wp-content/uploads/2015/01/twinpeaks.jpg&#34;&gt;&lt;img src=&#34;http://p3.no/filmpolitiet/wp-content/uploads/2015/01/twinpeaks-750x421.jpg&#34; alt=&#34;Kyle MacLachlan som Dale Cooper og Sherilyn Fenn som Audrey Horne i Twin Peaks. (Foto: Lynch/Frost Productions)&#34; width=&#34;750&#34; height=&#34;421&#34; class=&#34;size-medium wp-image-207347&#34; /&gt;&lt;/a&gt; Kyle Mac&#39; . The data contains normal text but also several html-tags. The REMOVE tag was added by the authors of the dataset to tag unwanted text such as image captions. In general we want to keep our text as intact as possible, but some text is clearly noise. So we&#39;ll proceed to get rid of the REMOVE tags and titles. The norec repo contains a method to do this with the lxml library. We&#39;ll change the code slightly to also remove headers. . from lxml.html import fragments_fromstring def html_to_text(html): return &#39; &#39;.join(elem.text_content() for elem in fragments_fromstring(html) if elem.tag == &#39;p&#39;) . html_to_text(review)[:1500] . &#39;Tv-serien Twin Peaks, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen. Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men Twin Peaks sikrer dem evig liv. Serien handler om et mordmysterium i den lille byen Twin Peaks, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funnet drept, og FBI-etterforsker Dale Cooper kommer til byen for å oppklare saken. Her blir han betatt av det tilsynelatende enkle livet i den lille byen mellom den endeløse skogen og de høye fjellene. Men han oppdager også at byen skjuler mørke, farlige hemmeligheter. Twin Peaks holder seg overraskende godt. Historien glir nok litt ut i det absurde i sesong 2, mens sesong 1 er en udiskutabel klassiker. Serien kjører en tilnærmet tidløs stil som idoliserer 50-talls-looken som David Lynch også dyrket i filmen ”Wild at heart” fra samme periode. Handlingen er fremdeles mørk og truende, samtidig som den har en merkelig form for humor, som regel fortalt gjennom Dale Cooper, glimrende spilt av en opplagt Kyle MacLachlan. Twin Peaks har et omfattende galleri av merkelige figurer. Men de er klart definerte, og spenner over et vidt spekter, fra søte Donna, til mystiske Log Lady til yppige Audrey Horn. F&#39; . That looks much better! Now lets combine the two methods to make a function to easily get html reviews from a file name . def get_review(fn): with open(fn) as f: return(html_to_text(f.read())) . get_review(item)[:1500] . &#39;Tv-serien Twin Peaks, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen. Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men Twin Peaks sikrer dem evig liv. Serien handler om et mordmysterium i den lille byen Twin Peaks, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funnet drept, og FBI-etterforsker Dale Cooper kommer til byen for å oppklare saken. Her blir han betatt av det tilsynelatende enkle livet i den lille byen mellom den endeløse skogen og de høye fjellene. Men han oppdager også at byen skjuler mørke, farlige hemmeligheter. Twin Peaks holder seg overraskende godt. Historien glir nok litt ut i det absurde i sesong 2, mens sesong 1 er en udiskutabel klassiker. Serien kjører en tilnærmet tidløs stil som idoliserer 50-talls-looken som David Lynch også dyrket i filmen ”Wild at heart” fra samme periode. Handlingen er fremdeles mørk og truende, samtidig som den har en merkelig form for humor, som regel fortalt gjennom Dale Cooper, glimrende spilt av en opplagt Kyle MacLachlan. Twin Peaks har et omfattende galleri av merkelige figurer. Men de er klart definerte, og spenner over et vidt spekter, fra søte Donna, til mystiske Log Lady til yppige Audrey Horn. F&#39; . Finally we append the review text to our data frame, and save it for future use. . screen[&#39;text&#39;] = screen[&#39;filename&#39;].apply(lambda o: get_review(path/o)) screen.to_feather(path/&#39;norec_df&#39;) screen.head(3) . filename rating title split sentiment text . 0 html/train/000000.html | 6 | Rome S02 | train | positive | Den andre og siste sesongen av Rome er ute på DVD i Norge. Om du så sesong 1, vet du at du har noe stort i vente. Har du aldri sett Rome før, stikk ut og kjøp begge sesongene. Dette er nemlig en av verdens beste tv-serier, og etter å ha sett de fire første episodene av sesong 2, konstaterer jeg at kvaliteten ser ut til å holde seg på et nesten overraskende høyt nivå! Sesong 2 starter nøyaktig der sesong 1 sluttet. Julius Cæsar ligger myrdet i Senatet og Lucius Vorenus hulker over liket av Neobie. Så blir historien enda mørkere. Marcus Antonius tar over styringen av Roma, men utfordres fra ... | . 1 html/train/000001.html | 6 | Twin Peaks - definitive gold box edition | train | positive | Tv-serien Twin Peaks, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen. Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men Twin Peaks sikrer dem evig liv. Serien handler om et mordmysterium i den lille byen Twin Peaks, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funn... | . 2 html/train/000002.html | 6 | The Wire (sesong 1-4) | train | positive | I neste uke kommer sesong 5 av tv-serien ”The Wire” på DVD. 2008 har for meg vært sterkt preget av denne serien. Hjemme hos oss begynte vi med sesong 1 i vår. Da hadde jeg i lengre tid hørt panegyriske lovord om serien fra både venner og media. Vi ble også fanget av skildringene av purk og skurk i Baltimore, og pløyde oss igjennom alt til og med sesong 4 på sensommeren. Jeg vil ikke gå så langt som å kalle det ”verdens beste serie”, som noen har gjort, men det er ingen tvil om at dette er noe av det bedre som er blitt vist på tv! Serien forteller om en gruppe politietterforskere som samles... | . I will use this dataset in future posts to explore various NLP techniques. In the upcoming post we will see if we are able to train a ULMFiT classifier on this dataset, and see how it compares to the results for the similar english IMDB-dataset. I also hope to test MultiFiT in a later post. .",
            "url": "https://hallvagi.github.io/dl-explorer/nlp/fastai/2020/04/06/get-data.html",
            "relUrl": "/nlp/fastai/2020/04/06/get-data.html",
            "date": " • Apr 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Hallvar. I’m really interested in deep learning and data science, and in this blog I’ll try to explain the concepts and methods I’m currently working on. My main reason for blogging is that I believe writing is a very useful part of the learning process. Writing forces you to be honest about your limits of understanding and knowledge. .",
          "url": "https://hallvagi.github.io/dl-explorer/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}