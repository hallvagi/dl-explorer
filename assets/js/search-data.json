{
  
    
        "post0": {
            "title": "A code first inspection of the AWD-LSTM",
            "content": "from fastai2.text.all import * . ULMFiT . In the previous post we explored the Norec Norwegian language corpus. We grabbed the reviews for films and TV-shows, parsed the html-text and created labels based on the ratings. In the next few posts I want to use ULMFiT and other methods to predict the sentiment of the reviews based on the text. ULMFiT has three main steps: . Train a language model on a large general purpose corpus such as Wikipedia | Fine-tune the language model on the text your are working with - the style is most likely different than a Wikipedia article | Combine the encoder of the fine-tuned language model with a linear classifier to predict the class of your text | The core of the ULMFiT method is a type of Recurrent neural network (RNN) called AWD-LSTM. AWD-LSTM is a special kind of Recurrent neural network (RNN) with tuned dropout parameters among other. We need to look into this architecture before we continue with our modeling. For an explanation of what an LSTM actually is i suggest checking out this blog post by Chris Olah. In general, most of Chris&#39; posts and papers are worth reading! . How to set up an AWD-LSTM with fastai . Let&#39;s first start by inspecting fastai&#39;s language_model_learner. It&#39;s a learner class designed to be used for language models, and holds both dataloaders and the architecture along with various hyperparameters. We can use the doc() method to show us the documentation: . doc(language_model_learner) . The documentation tells us that we can pass arch = AWD-LSTM and modify the awd_lstm_lm_config to customize the architecture. The config dictionary specifies various hyperparameters and settings inspired by the aforementioned AWD-LSTM paper. By changing this dictionary we can customize our AWD-LSTM to fit our specific needs: . awd_lstm_lm_config . {&#39;emb_sz&#39;: 400, &#39;n_hid&#39;: 1152, &#39;n_layers&#39;: 3, &#39;pad_token&#39;: 1, &#39;bidir&#39;: False, &#39;output_p&#39;: 0.1, &#39;hidden_p&#39;: 0.15, &#39;input_p&#39;: 0.25, &#39;embed_p&#39;: 0.02, &#39;weight_p&#39;: 0.2, &#39;tie_weights&#39;: True, &#39;out_bias&#39;: True} . Let&#39;s check the documentation and source code of the AWD-LSTM class. You can check the source code directly in the notebook by appending a ?? behind the method name: . AWD_LSTM?? . The source code shows us a few interesting lines we&#39;ll look more into in the next few sections: . self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token) | self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir, bidir, weight_p, l) for l in range(n_layers)]) | self.input_dp = RNNDropout(input_p) | self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)]) | . Note: the embedding is called encoder in the code above. The name encoder is also fastai lingo for the entire RNN-part of the architecture. The linear layers added on top for the classifier is called decoder. Neither the ULMFiT or AWD-LSTM paper uses the term encoder or decoder though. . But what is an embedding? . Once again I&#39;ll be lazy and rather refer to another blog that explains embeddings in detail. The blog is by Jay Alammar and has explanations of many deep learning and NLP concepts. The essence is that we&#39;ll turn each token in our vocabulary into a vector of some size that represents various aspects of that token. The weights of this vector will be trainable and gives our neural network a lot of flexibility in assigning various properties to each token. . The embedding is created by: self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token) Here we see that fastai is built on top of pyTorch and relies on pyTorch&#39;s fundamental methods in its own code. The encoder layer is a call to nn.Embedding, see documentation. Let&#39;s create an embedding of size 10x3 with padding_idx = 0: . embedding = nn.Embedding(num_embeddings=10, embedding_dim=3, padding_idx=0) embedding.weight . Parameter containing: tensor([[ 0.0000, 0.0000, 0.0000], [ 1.6721, -1.3130, 0.6414], [ 1.1675, 0.1174, 1.8511], [-0.3341, -1.0047, -0.8467], [-0.7737, -0.3947, -1.5273], [-1.1472, -0.0429, -0.0994], [-1.0594, 1.3725, 0.3796], [ 0.1682, 0.7212, 0.9494], [ 1.2791, 0.1334, -0.5075], [ 0.4486, 0.4936, 0.2588]], requires_grad=True) . The embedding now has 10 vectors of length 3 with randomly initialized weights. Note that the first one (index 0) is all 0. This is because 0 is our padding index. Next we&#39;ll pass some some sample data inp, and inspect the result. We can think of our input as the index of words in a dictionary. E.g. 1=&#39;this&#39;, 7=&#39;is&#39;, 4=&#39;not&#39; and 3=&#39;easy&#39;. 0 will be our padding token. The padding token is a special token that is used to ensure that some text has a certain length. This is useful when stacking various pieces of text into a batch where sizes needs to match. . inp = torch.LongTensor([1,7,4,3,0,0]) emb = embedding(inp) emb . tensor([[ 1.6721, -1.3130, 0.6414], [ 0.1682, 0.7212, 0.9494], [-0.7737, -0.3947, -1.5273], [-0.3341, -1.0047, -0.8467], [ 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000]], grad_fn=&lt;EmbeddingBackward&gt;) . We see that the embedding produced by feeding the input corresponds to the weights of our original embedding. That is, index 1 of inp is the item &#39;7&#39;. So emb[1] is basically a lookup for embedding.weight[7]. . embedding.weight[7], emb[1] . (tensor([0.1682, 0.7212, 0.9494], grad_fn=&lt;SelectBackward&gt;), tensor([0.1682, 0.7212, 0.9494], grad_fn=&lt;SelectBackward&gt;)) . To summarize:We&#39;ll need an embedding with the number of embeddings equal to our vocabulary size, and embedding size of 400 and a padding token-id which corresponds to whichever token has been used as padding in our vocabulary. . Compostion of the RNN-layers . Secondly we create a list of RNN-layers with various dimensions: . self.rnns = nn.ModuleList([self._one_rnn(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir, bidir, weight_p, l) for l in(n_layers)]) . The code stacks RNN-layers of embedding size x hidden size for the first layer, and hidden size x embedding size for the final. Let&#39;s verify that this works for various number of layers: . #collapse-show AWD_LSTM(vocab_sz=10_000, emb_sz=400, n_hid=1152, n_layers=2) . . AWD_LSTM( (encoder): Embedding(10000, 400, padding_idx=1) (encoder_dp): EmbeddingDropout( (emb): Embedding(10000, 400, padding_idx=1) ) (rnns): ModuleList( (0): WeightDropout( (module): LSTM(400, 1152, batch_first=True) ) (1): WeightDropout( (module): LSTM(1152, 400, batch_first=True) ) ) (input_dp): RNNDropout() (hidden_dps): ModuleList( (0): RNNDropout() (1): RNNDropout() ) ) . #collapse-output AWD_LSTM(vocab_sz=10_000, emb_sz=400, n_hid=1152, n_layers=5) . AWD_LSTM( (encoder): Embedding(10000, 400, padding_idx=1) (encoder_dp): EmbeddingDropout( (emb): Embedding(10000, 400, padding_idx=1) ) (rnns): ModuleList( (0): WeightDropout( (module): LSTM(400, 1152, batch_first=True) ) (1): WeightDropout( (module): LSTM(1152, 1152, batch_first=True) ) (2): WeightDropout( (module): LSTM(1152, 1152, batch_first=True) ) (3): WeightDropout( (module): LSTM(1152, 1152, batch_first=True) ) (4): WeightDropout( (module): LSTM(1152, 400, batch_first=True) ) ) (input_dp): RNNDropout() (hidden_dps): ModuleList( (0): RNNDropout() (1): RNNDropout() (2): RNNDropout() (3): RNNDropout() (4): RNNDropout() ) ) . We see the first and final layers have similar dimensions in the two examples. . To summarize:We&#39;ll use a 3 layer network with input and output dimensions of (400, 1152), (1152, 1152) and (1152, 400) as in the AWD-LSTM paper. This should be handled automatically by the library. . nn.LSTM . In the module list above, the layers are actually WeightDropout layers. We can verify this from the hidden constructor method that is called when the RNNs are being created. First, a regular nn.LSTM layer is created before being passed to the WeightDropout module. . def _one_rnn(self, n_in, n_out, bidir, weight_p, l): &quot;Return one of the inner rnn&quot; rnn = nn.LSTM(n_in, n_out, 1, batch_first=True, bidirectional=bidir) return WeightDropout(rnn, weight_p) . Lets have a look at an example from the nn.LSTM documentation, also see source code here. We&#39;ll make a 1 layer LSTM with input size of 10 and hidden size of 20. Note that in the AWD-LSTM case the input size is equal to the embedding size (400 by default). . inp_s = 10 # input size hid_s = 20 # hidden size . lstm = nn.LSTM(input_size = inp_s, hidden_size = hid_s, num_layers=1) . The documentation details that the LSTM expects input in the form of input(seq_len, batch, input_size). Seq_len is the length of the part of the text the model will see in each iteration (seq_len = 72 by default in fastais language_model_learner, that is 72 tokens). The batch size is the number of documents the models sees in each iteration. . h0 and c0 are the inital hidden and cell states (set to 0 if not provided). The documentation specifiy their shapes as: (num_layers * num_directions, batch, hidden_size). . bs = 16 n_l, n_d = 1, 1 # we are testing a 1 layer and 1 direction lstm seq_len = 5 . inp = torch.randn(seq_len, bs, inp_s) h0 = torch.randn(n_l*n_d, bs, hid_s) c0 = torch.randn(n_l*n_d, bs, hid_s) . inp.shape, h0.shape, c0.shape . (torch.Size([5, 16, 10]), torch.Size([1, 16, 20]), torch.Size([1, 16, 20])) . The output from the LSTM should be a tuple of output, (h_n, c_n)where output has shape given by: (seq_len, batch, num_directions * hidden_size): . out, (hn, cn) = lstm(inp) out.shape . torch.Size([5, 16, 20]) . Let&#39;s also check the actual shape of our weights by looping through the state_dict(): . [(key, lstm.state_dict()[key].shape) for key in lstm.state_dict().keys()] . [(&#39;weight_ih_l0&#39;, torch.Size([80, 10])), (&#39;weight_hh_l0&#39;, torch.Size([80, 20])), (&#39;bias_ih_l0&#39;, torch.Size([80])), (&#39;bias_hh_l0&#39;, torch.Size([80]))] . Here we recognize the input size of 10 and hidden size of 20, but where does the 80 come from? The documentation specifiyes that the weights will be of dimension (4*hidden_size, input_size). The &#39;4&#39; is called gate_size, and we can find this in the source code for the base RNN module also: . if mode == &#39;LSTM&#39;: gate_size = 4 * hidden_size . To summarize:we expect two sets of weights and biases per LSTM: weight_ih_l0 with a shape of (4hidden_size, input_size) . weight_hh_l0 with a shape of (4*hidden_size, hidden_size) | bias_ih_l0 with a shape of (4*hidden_size) | bias_hh_l0 with a shape of (4*hidden_size) | . WeightDropout . We can see from the _one_rnn that the nn.LSTM is transformed to a WeightDropout module. The documentation describes the module as &#39;A module that warps another layer in which some weights will be replaced by 0 during training&#39;. From the source code we can see that it&#39;s the weight_hh_l0 weights that will be modified, and that these weights are duplicated with suffix &#39;raw&#39; in the WeightDropout module: self.register_parameter(f&#39;{layer}_raw&#39;, nn.Parameter(w.data)). . Let&#39;s see if we can verify this by first checking the weights from the lstm from the above section. Of the 80*20 = 1600 weights in the hh_l0 layer, none are 0: . orig_wts = getattr(lstm, &#39;weight_hh_l0&#39;) orig_wts.shape, (orig_wts == 0.).sum() . (torch.Size([80, 20]), tensor(0)) . But if pass the lstm through the WeightDroput module, approximately half of the 1600 weights are set to 0. Note that we have to call the model on the input since the weights are only reset during the forward pass. The weights are also only reset for the WeighDropout&#39;s internal LSTM module, while a copy with suffix &#39;_raw&#39; retains the original weights. . wd = WeightDropout(lstm, weight_p=0.5) _,_ = wd(inp) # we don&#39;t need the output in this case . The .module attribute of the wd object is our original LSTM: . wd.module . LSTM(10, 20) . And about half its weights have been set to 0: . (getattr(wd.module, &#39;weight_hh_l0&#39;)==0.0).sum() . tensor(826) . The original weights from the lstm matches the &#39;_raw&#39; weights of the WeightDropout module: . test_eq(orig_wts, getattr(wd, &#39;weight_hh_l0_raw&#39;)) . The new layers are as expected: . wd.state_dict().keys() . odict_keys([&#39;weight_hh_l0_raw&#39;, &#39;module.weight_ih_l0&#39;, &#39;module.bias_ih_l0&#39;, &#39;module.bias_hh_l0&#39;]) . RNN dropout . Finally several RNNDropout layers are being created - one for the input and one for each LSTM. This dropout is applied to the input embedding and on the output of each LSTM. We can test the functionality with inp from the above section. . dp = RNNDropout(0.5) dp_out = dp(inp) inp.shape, dp_out.shape . (torch.Size([5, 16, 10]), torch.Size([5, 16, 10])) . The documentation also says: &#39;Dropout with probability p that is consistent on the seq_len dimension.&#39; In our input from the above section, seq_len is the first dimension (index 0), and if we check for items equaling 0 and sum along the second dimension (index 1) we see that the same tokens are dropped out for the entire batch (our sample batch size is 16) consistently in approximately half of the instances. . (dp_out == 0).sum((1)) . tensor([[ 0, 0, 16, 16, 16, 16, 0, 0, 16, 0], [ 0, 16, 16, 16, 16, 0, 0, 16, 16, 0], [ 0, 0, 0, 16, 0, 0, 0, 0, 16, 16], [ 0, 0, 16, 16, 0, 16, 0, 16, 0, 16], [ 0, 16, 0, 16, 0, 16, 16, 0, 16, 16]]) . IMDb inspection . Let&#39;s take a look at a minimal IMDB example from the fastai documentation to verify our understanding of the AWD-LSTM architecture. . imdb_path = untar_data(URLs.IMDB_SAMPLE) df = pd.read_csv(imdb_path/&#39;texts.csv&#39;) dls = TextDataLoaders.from_df(df, path=imdb_path, text_col=&#39;text&#39;, is_lm=True, valid_col=&#39;is_valid&#39;) learn = language_model_learner(dls, AWD_LSTM) . The vocab is of length 7080 and vocab index 1 is &#39;xxpad&#39;: . dls.vocab[:5], len(dls.vocab) . ([&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;xxbos&#39;, &#39;xxeos&#39;, &#39;xxfld&#39;], 7080) . In our model we recognize Embedding(7080, 400, padding_idx=1) as vocab_size x embedding size with the correct padding token. We also see that the (input, output) dimensions of our LSTM-layers are as expected, and with the expected dropout layers added. . learn.model . SequentialRNN( (0): AWD_LSTM( (encoder): Embedding(7080, 400, padding_idx=1) (encoder_dp): EmbeddingDropout( (emb): Embedding(7080, 400, padding_idx=1) ) (rnns): ModuleList( (0): WeightDropout( (module): LSTM(400, 1152, batch_first=True) ) (1): WeightDropout( (module): LSTM(1152, 1152, batch_first=True) ) (2): WeightDropout( (module): LSTM(1152, 400, batch_first=True) ) ) (input_dp): RNNDropout() (hidden_dps): ModuleList( (0): RNNDropout() (1): RNNDropout() (2): RNNDropout() ) ) (1): LinearDecoder( (decoder): Linear(in_features=400, out_features=7080, bias=True) (output_dp): RNNDropout() ) ) . The model summary shows us the default batch size of 64 and seq_len of 72. . learn.summary() . SequentialRNN (Input shape: [&#39;64 x 72&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ RNNDropout 64 x 72 x 400 0 False ________________________________________________________________ RNNDropout 64 x 72 x 1152 0 False ________________________________________________________________ RNNDropout 64 x 72 x 1152 0 False ________________________________________________________________ Linear 64 x 72 x 7080 2,839,080 True ________________________________________________________________ RNNDropout 64 x 72 x 400 0 False ________________________________________________________________ Total params: 2,839,080 Total trainable params: 2,839,080 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fa03d7cbdd0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group number 3 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback - ModelReseter - RNNRegularizer . And finally, the layer names and shapes also is concistent with a gate size of 4 (1152*4 = 4608). Note the enumeration of the layers: 0. is the encoder part of the architecture (including the embedding called encoder) and 1. is the decoder. The encoder is specific to the language model, and the decoder is specific to the classifier. . for key in learn.model.state_dict().keys(): print(key, &#39; t&#39;, learn.model.state_dict()[key].shape) . 0.encoder.weight torch.Size([7080, 400]) 0.encoder_dp.emb.weight torch.Size([7080, 400]) 0.rnns.0.weight_hh_l0_raw torch.Size([4608, 1152]) 0.rnns.0.module.weight_ih_l0 torch.Size([4608, 400]) 0.rnns.0.module.bias_ih_l0 torch.Size([4608]) 0.rnns.0.module.bias_hh_l0 torch.Size([4608]) 0.rnns.1.weight_hh_l0_raw torch.Size([4608, 1152]) 0.rnns.1.module.weight_ih_l0 torch.Size([4608, 1152]) 0.rnns.1.module.bias_ih_l0 torch.Size([4608]) 0.rnns.1.module.bias_hh_l0 torch.Size([4608]) 0.rnns.2.weight_hh_l0_raw torch.Size([1600, 400]) 0.rnns.2.module.weight_ih_l0 torch.Size([1600, 1152]) 0.rnns.2.module.bias_ih_l0 torch.Size([1600]) 0.rnns.2.module.bias_hh_l0 torch.Size([1600]) 1.decoder.weight torch.Size([7080, 400]) 1.decoder.bias torch.Size([7080]) .",
            "url": "https://hallvagi.github.io/dl-explorer/nlp/fastai/rnn/lstm/2020/04/17/AWD_LSTM.html",
            "relUrl": "/nlp/fastai/rnn/lstm/2020/04/17/AWD_LSTM.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Finding a Norwegian language dataset for sentiment analysis",
            "content": "I&#39;m currently taking the fast.ai online course practical deep learning for coders (to be released publicly in July 2020). As part of the studies I want to explore the fastai2 deep learning library. I will do so by testing various NLP methods. . Over the next few posts I&#39;ll got through the entire process of finding and processing data, training various models and also interpret the results. I&#39;ll try to highlight the various things I&#39;ve been struggling with or confused about. I think writing short blog posts such as these are a great way of learning new material. . The NOREC dataset . I wanted to find a dataset in my native Norwegian to analyze. NLP for languages other than English is often challenging, even though many new techniques are addressing this. After a bit of searching I found the Norec dataset. It contains Norwegian language reviews of various films, music etc. The dataset even comes with a paper that explains the setup of the data! This seems like a great case study, and is similar to the IMDB movie review sentiment analyses, which is one of the built in datasets of the fastai2 library. . The repo includes a utility library and a download.sh script. This is what you normally would want to use to ease the process of actually getting the data, but in this case I would like to do things manually. We&#39;ll be using the fastai2 library (which upon release will be renamed to fastai). . Note that import * is usually not encouraged, but the fastai2 library has defined it&#39;s __all__ variables properly, so this won&#39;t be a problem. See this file for the various imports that is part of the library. . from fastai2.text.all import * . I&#39;m using fastai2 &#39;0.0.17&#39; and fastcore &#39;0.1.17&#39;. To check the version you can uncomment the following lines: . #collapse # import fastai2, fastcore # fastai2.__version__, fastcore.__version__ . . First we&#39;ll download and extract the data from the url given in the download.sh file in the github-repo to a dest with archive_name. Note the !command syntax which runs the corresponding wget shell command. The wget command only has to be run once. . url = f&#39;http://folk.uio.no/eivinabe/norec-1.0.1.tar.gz&#39; data_path = Path(&#39;~/.fastai&#39;) dest = data_path/&#39;archive&#39; archive_name = &#39;norec.tar.gz&#39; !wget {url} -O {dest/archive_name} -q . Then we&#39;ll extract the archive with the tarfile library (imported via the fastai2 import at the beginning of the notebook). We&#39;ll extract from the archive location to data. Finally we set a new path that points to this location. . tarfile.open(dest/archive_name).extractall(data_path/&#39;data&#39;) path = data_path/&#39;data/norec&#39; Path.BASE_PATH=path # this avoids printing the entire file-path when we list files in a directory path.ls() . (#4) [Path(&#39;metadata.json&#39;),Path(&#39;conllu.tar.gz&#39;),Path(&#39;html.tar.gz&#39;),Path(&#39;README.txt&#39;)] . The archive contains a .json file with metadata, and among other, a html.tar.gz archive with our desired raw texts. We&#39;ll extract this archive too. The conllu.tar.gz contains tokenized and filtered text, and we don&#39;t need this for the time being. . tarfile.open(path/&#39;html.tar.gz&#39;).extractall(path) path.ls() . (#5) [Path(&#39;metadata.json&#39;),Path(&#39;conllu.tar.gz&#39;),Path(&#39;html.tar.gz&#39;),Path(&#39;html&#39;),Path(&#39;README.txt&#39;)] . The extracted archive is now in the data/norec/html folder, which contains the train, dev (we&#39;ll call it validation) and test split. . (path/&#39;html&#39;).ls() . (#3) [Path(&#39;html/train&#39;),Path(&#39;html/test&#39;),Path(&#39;html/dev&#39;)] . Inspect the metadata . Let&#39;s inspect the metatdata file and see if we are able to make sense of the data. But what does this json file look like? We can use the head command to have a look at the raw file contents before we attempt to read it into a pandas dataframe: . !head -n 20 {path/&#39;metadata.json&#39;} . { &#34;000000&#34;: { &#34;authors&#34;: [ &#34;Birger Vestmo&#34; ], &#34;category&#34;: &#34;screen&#34;, &#34;day&#34;: 27, &#34;excerpt&#34;: &#34;Toppen innen tv-drama akkurat nå!&#34;, &#34;id&#34;: 0, &#34;language&#34;: &#34;nb&#34;, &#34;month&#34;: 9, &#34;rating&#34;: 6, &#34;source&#34;: &#34;p3&#34;, &#34;source-category&#34;: &#34;tv&#34;, &#34;source-id&#34;: 74781, &#34;source-tags&#34;: [], &#34;split&#34;: &#34;train&#34;, &#34;tags&#34;: [ &#34;tv&#34; ], . Let&#39;s try to read the data with pandas default read_json() . pd.read_json(path/&#39;metadata.json&#39;).head(3) . 0 1 2 3 4 5 6 7 8 9 ... 705136 705137 705138 705139 705140 705141 705142 705143 705144 705145 . authors [Birger Vestmo] | [Birger Vestmo] | [Birger Vestmo] | [Birger Vestmo] | [Birger Vestmo] | [Torfinn Borkhus] | [Torfinn Borkhus] | [Torfinn Borkhus] | [Torfinn Borkhus] | [Torfinn Borkhus] | ... | [Arnfinn Bø-Rygg] | [Leif Tore Lindø] | [Leif Tore Lindø] | [Kine Hult] | [Arnfinn Bø-Rygg] | [Leif Tore Lindø] | [Arnfinn Bø-Rygg] | [Leif Tore Lindø] | [Leif Tore Lindø] | [Elisabeth Bie] | . category screen | screen | screen | screen | screen | screen | screen | screen | screen | screen | ... | music | music | music | music | music | music | music | music | music | music | . day 27 | 22 | 22 | 3 | 9 | 25 | 25 | 26 | 1 | 8 | ... | 8 | 16 | 13 | 13 | 3 | 8 | 4 | 17 | 25 | 9 | . 3 rows × 35189 columns . Not quite what we looked for! From the read_json documentation we see that we can change the orientation of the data with the orientoption. &#39;index&#39; seems to be what we look for. Note that we also could have transposed the data frame for a similar result. . There is another problem that is harder to spot: The index of our dataframe should be the string representation of the json key. This is the actual file name of the corresponding review, but it is cast to an int. This turns &#39;000000&#39; into 0. So we&#39;ll set convert_axes to False. We&#39;ll also reset the index and rename it to filename. . df = pd.read_json(path/&#39;metadata.json&#39;, orient=&#39;index&#39;, convert_axes=False) df = df.reset_index().rename(columns={&#39;index&#39;: &#39;filename&#39;}) df.head(3) . filename authors category day excerpt id language month rating source source-category source-id source-tags split tags title url year . 0 000000 | [Birger Vestmo] | screen | 27 | Toppen innen tv-drama akkurat nå! | 0 | nb | 9 | 6 | p3 | tv | 74781 | [] | train | [tv] | Rome S02 | http://p3.no/filmpolitiet/rome-02 | 2007 | . 1 000001 | [Birger Vestmo] | screen | 22 | Gull for &lt;em&gt;Twin Peaks&lt;/em&gt;-fans! | 1 | nb | 11 | 6 | p3 | tv | 74065 | [] | train | [tv] | Twin Peaks - definitive gold box edition | http://p3.no/filmpolitiet/twin-peaks-definitive-gold-box-edition | 2007 | . 2 000002 | [Birger Vestmo] | screen | 22 | The Wire vil gjøre deg avhengig, men på en god måte. | 2 | nb | 2 | 6 | p3 | tv | 73886 | [] | train | [tv] | The Wire (sesong 1-4) | http://p3.no/filmpolitiet/the-wire-sesong-1-4 | 2008 | . There are several category-like columns as explained in the paper. We will use the category column. . df[&#39;category&#39;].value_counts() . screen 13085 music 12410 literature 3526 products 3120 games 1765 restaurants 534 stage 530 sports 117 misc 102 Name: category, dtype: int64 . I&#39;m also curious about the distribution of bokmål (nb) vs nynorsk (nn). It seems the vast majority of reviews are in bokmål. . df[&#39;language&#39;].value_counts() . nb 34656 nn 533 Name: language, dtype: int64 . Get a subset of the data . We&#39;ll proceed with a subset of the data. We&#39;ll look at the screen sub category and bokmål (nb) language. The screen category contains both movie and TV-reviews. The data frame also contains several columns we won&#39;t be using now so let&#39;s select the relevant columns. We&#39;re left with ~ 13000 reviews. . screen = df.loc[(df[&#39;category&#39;]==&#39;screen&#39;) &amp; (df[&#39;language&#39;]==&#39;nb&#39;)] screen = screen.loc[:, [&#39;filename&#39;, &#39;rating&#39;, &#39;title&#39;, &#39;split&#39;]] print(screen.shape) screen.head(3) . (12924, 4) . filename rating title split . 0 000000 | 6 | Rome S02 | train | . 1 000001 | 6 | Twin Peaks - definitive gold box edition | train | . 2 000002 | 6 | The Wire (sesong 1-4) | train | . Let&#39;s also change the filename so that is gives the path to our review files. . screen[&#39;filename&#39;] = &#39;html/&#39;+screen[&#39;split&#39;]+&#39;/&#39;+screen[&#39;filename&#39;]+&#39;.html&#39; screen.sample(3) . filename rating title split . 21438 html/train/301590.html | 4 | På den smale sykkelsti | train | . 28588 html/train/600841.html | 4 | Lykken er en olivenlund | train | . 32395 html/test/702352.html | 5 | Sterk, sanselig skjønnhet fra Aldomóvar | test | . The ratings of the screen category has a slight positive skew. . screen[&#39;rating&#39;].value_counts().sort_index().plot(kind=&#39;bar&#39;); . We&#39;ll encode a rating of 1 to 3 as negative, and 5 and 6 as positive. Reviews rated 4 will be removed. We&#39;ll lose a bit of data this way, but this means that the positive and negative review are a bit more distinct, and will make down stream classification a bit simpler. This leaves us with ~8600 reviews. . screen = screen.loc[screen[&#39;rating&#39;]!=4].reset_index(drop=True) screen[&#39;sentiment&#39;]=screen[&#39;rating&#39;].apply(lambda k: &#39;positive&#39; if k&gt;=4 else &#39;negative&#39;) print(screen.shape) screen.head(3) . (8613, 5) . filename rating title split sentiment . 0 html/train/000000.html | 6 | Rome S02 | train | positive | . 1 html/train/000001.html | 6 | Twin Peaks - definitive gold box edition | train | positive | . 2 html/train/000002.html | 6 | The Wire (sesong 1-4) | train | positive | . The train, validation (dev) and test split is ok: . screen[&#39;split&#39;].value_counts(normalize=True) . train 0.803321 dev 0.101707 test 0.094973 Name: split, dtype: float64 . And the dataset is well balanced, i.e. similar amount of labels for each class. . screen[&#39;sentiment&#39;].value_counts(normalize=True) . negative 0.512713 positive 0.487287 Name: sentiment, dtype: float64 . Add text to the data frame . Let&#39;s also add the full text to the dataframe for convenience. This step is not strictly necessary, and doesn&#39;t scale to big data. The html/train folder contains html files, and the data frame gives us our filenames. . (path/&#39;html/train&#39;).ls() . (#28158) [Path(&#39;html/train/201232.html&#39;),Path(&#39;html/train/601043.html&#39;),Path(&#39;html/train/108030.html&#39;),Path(&#39;html/train/700390.html&#39;),Path(&#39;html/train/702069.html&#39;),Path(&#39;html/train/100023.html&#39;),Path(&#39;html/train/701430.html&#39;),Path(&#39;html/train/201765.html&#39;),Path(&#39;html/train/302012.html&#39;),Path(&#39;html/train/400836.html&#39;)...] . screen.head(3) . filename rating title split sentiment . 0 html/train/000000.html | 6 | Rome S02 | train | positive | . 1 html/train/000001.html | 6 | Twin Peaks - definitive gold box edition | train | positive | . 2 html/train/000002.html | 6 | The Wire (sesong 1-4) | train | positive | . Let&#39;s inspect the second file in the data frame. We expect it should be a review of Twin Peaks. We&#39;ll open the file and print the raw contents. . fn = screen.loc[1, &#39;filename&#39;] item = (path/fn) item . Path(&#39;html/train/000001.html&#39;) . with open(item) as f: review = f.read() review[:1500] . &#39;&lt;h1&gt;Twin Peaks - definitive gold box edition&lt;/h1&gt; n n&lt;p&gt;Tv-serien &lt;em&gt;Twin Peaks&lt;/em&gt;, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen.&lt;/p&gt; n n&lt;p&gt;Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men &lt;em&gt;Twin Peaks&lt;/em&gt; sikrer dem evig liv.&lt;/p&gt; n n&lt;h5&gt;Mørke hemmeligheter&lt;/h5&gt; n n&lt;p&gt;Serien handler om et mordmysterium i den lille byen &lt;em&gt;Twin Peaks&lt;/em&gt;, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funnet drept, og FBI-etterforsker Dale Cooper kommer til byen for å oppklare saken.&lt;/p&gt; n n&lt;p&gt;Her blir han betatt av det tilsynelatende enkle livet i den lille byen mellom den endeløse skogen og de høye fjellene. Men han oppdager også at byen skjuler mørke, farlige hemmeligheter.&lt;/p&gt; n n&lt;REMOVE&gt;&lt;small&gt;Anmeldelsen fortsetter under bildet.&lt;/small&gt;&lt;/REMOVE&gt; n n&lt;REMOVE&gt;[caption id=&#34;attachment_207347&#34; align=&#34;alignnone&#34; width=&#34;750&#34;]&lt;a href=&#34;http://p3.no/filmpolitiet/wp-content/uploads/2015/01/twinpeaks.jpg&#34;&gt;&lt;img src=&#34;http://p3.no/filmpolitiet/wp-content/uploads/2015/01/twinpeaks-750x421.jpg&#34; alt=&#34;Kyle MacLachlan som Dale Cooper og Sherilyn Fenn som Audrey Horne i Twin Peaks. (Foto: Lynch/Frost Productions)&#34; width=&#34;750&#34; height=&#34;421&#34; class=&#34;size-medium wp-image-207347&#34; /&gt;&lt;/a&gt; Kyle Mac&#39; . The data contains normal text but also several html-tags. The REMOVE tag was added by the authors of the dataset to tag unwanted text such as image captions. In general we want to keep our text as intact as possible, but some text is clearly noise. So we&#39;ll proceed to get rid of the REMOVE tags and titles. The norec repo contains a method to do this with the lxml library. We&#39;ll change the code slightly to also remove headers. . from lxml.html import fragments_fromstring def html_to_text(html): return &#39; &#39;.join(elem.text_content() for elem in fragments_fromstring(html) if elem.tag == &#39;p&#39;) . html_to_text(review)[:1500] . &#39;Tv-serien Twin Peaks, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen. Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men Twin Peaks sikrer dem evig liv. Serien handler om et mordmysterium i den lille byen Twin Peaks, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funnet drept, og FBI-etterforsker Dale Cooper kommer til byen for å oppklare saken. Her blir han betatt av det tilsynelatende enkle livet i den lille byen mellom den endeløse skogen og de høye fjellene. Men han oppdager også at byen skjuler mørke, farlige hemmeligheter. Twin Peaks holder seg overraskende godt. Historien glir nok litt ut i det absurde i sesong 2, mens sesong 1 er en udiskutabel klassiker. Serien kjører en tilnærmet tidløs stil som idoliserer 50-talls-looken som David Lynch også dyrket i filmen ”Wild at heart” fra samme periode. Handlingen er fremdeles mørk og truende, samtidig som den har en merkelig form for humor, som regel fortalt gjennom Dale Cooper, glimrende spilt av en opplagt Kyle MacLachlan. Twin Peaks har et omfattende galleri av merkelige figurer. Men de er klart definerte, og spenner over et vidt spekter, fra søte Donna, til mystiske Log Lady til yppige Audrey Horn. F&#39; . That looks much better! Now lets combine the two methods to make a function to easily get html reviews from a file name . def get_review(fn): with open(fn) as f: return(html_to_text(f.read())) . get_review(item)[:1500] . &#39;Tv-serien Twin Peaks, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen. Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men Twin Peaks sikrer dem evig liv. Serien handler om et mordmysterium i den lille byen Twin Peaks, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funnet drept, og FBI-etterforsker Dale Cooper kommer til byen for å oppklare saken. Her blir han betatt av det tilsynelatende enkle livet i den lille byen mellom den endeløse skogen og de høye fjellene. Men han oppdager også at byen skjuler mørke, farlige hemmeligheter. Twin Peaks holder seg overraskende godt. Historien glir nok litt ut i det absurde i sesong 2, mens sesong 1 er en udiskutabel klassiker. Serien kjører en tilnærmet tidløs stil som idoliserer 50-talls-looken som David Lynch også dyrket i filmen ”Wild at heart” fra samme periode. Handlingen er fremdeles mørk og truende, samtidig som den har en merkelig form for humor, som regel fortalt gjennom Dale Cooper, glimrende spilt av en opplagt Kyle MacLachlan. Twin Peaks har et omfattende galleri av merkelige figurer. Men de er klart definerte, og spenner over et vidt spekter, fra søte Donna, til mystiske Log Lady til yppige Audrey Horn. F&#39; . Finally we append the review text to our data frame, and save it for future use. . screen[&#39;text&#39;] = screen[&#39;filename&#39;].apply(lambda o: get_review(path/o)) screen.to_feather(path/&#39;norec_df&#39;) screen.head(3) . filename rating title split sentiment text . 0 html/train/000000.html | 6 | Rome S02 | train | positive | Den andre og siste sesongen av Rome er ute på DVD i Norge. Om du så sesong 1, vet du at du har noe stort i vente. Har du aldri sett Rome før, stikk ut og kjøp begge sesongene. Dette er nemlig en av verdens beste tv-serier, og etter å ha sett de fire første episodene av sesong 2, konstaterer jeg at kvaliteten ser ut til å holde seg på et nesten overraskende høyt nivå! Sesong 2 starter nøyaktig der sesong 1 sluttet. Julius Cæsar ligger myrdet i Senatet og Lucius Vorenus hulker over liket av Neobie. Så blir historien enda mørkere. Marcus Antonius tar over styringen av Roma, men utfordres fra ... | . 1 html/train/000001.html | 6 | Twin Peaks - definitive gold box edition | train | positive | Tv-serien Twin Peaks, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen. Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men Twin Peaks sikrer dem evig liv. Serien handler om et mordmysterium i den lille byen Twin Peaks, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funn... | . 2 html/train/000002.html | 6 | The Wire (sesong 1-4) | train | positive | I neste uke kommer sesong 5 av tv-serien ”The Wire” på DVD. 2008 har for meg vært sterkt preget av denne serien. Hjemme hos oss begynte vi med sesong 1 i vår. Da hadde jeg i lengre tid hørt panegyriske lovord om serien fra både venner og media. Vi ble også fanget av skildringene av purk og skurk i Baltimore, og pløyde oss igjennom alt til og med sesong 4 på sensommeren. Jeg vil ikke gå så langt som å kalle det ”verdens beste serie”, som noen har gjort, men det er ingen tvil om at dette er noe av det bedre som er blitt vist på tv! Serien forteller om en gruppe politietterforskere som samles... | . I will use this dataset in future posts to explore various NLP techniques. In the upcoming post we will see if we are able to train a ULMFiT classifier on this dataset, and see how it compares to the results for the similar english IMDB-dataset. I also hope to test MultiFiT in a later post. .",
            "url": "https://hallvagi.github.io/dl-explorer/nlp/fastai/2020/04/06/get-data.html",
            "relUrl": "/nlp/fastai/2020/04/06/get-data.html",
            "date": " • Apr 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Hallvar. I’m really interested in deep learning and data science, and in this blog I’ll try to explain the concepts and methods I’m currently working on. My main reason for blogging is that I believe writing is a very useful part of the learning process. Writing forces you to be honest about your limits of understanding and knowledge. .",
          "url": "https://hallvagi.github.io/dl-explorer/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}