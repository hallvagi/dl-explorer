<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Classification of movie review sentiment using ULMFiT | Deep learning explorer</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Classification of movie review sentiment using ULMFiT" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We test how well ULMFIiT manages to predict Norwegian language movie review using a fine-tuned language model." />
<meta property="og:description" content="We test how well ULMFIiT manages to predict Norwegian language movie review using a fine-tuned language model." />
<link rel="canonical" href="https://hallvagi.github.io/dl-explorer/nlp/fastai/ulmfit/sentiment/2020/06/12/ULMFiT_classifier.html" />
<meta property="og:url" content="https://hallvagi.github.io/dl-explorer/nlp/fastai/ulmfit/sentiment/2020/06/12/ULMFiT_classifier.html" />
<meta property="og:site_name" content="Deep learning explorer" />
<meta property="og:image" content="https://hallvagi.github.io/dl-explorer/images/some_folder/your_image.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-12T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"We test how well ULMFIiT manages to predict Norwegian language movie review using a fine-tuned language model.","headline":"Classification of movie review sentiment using ULMFiT","@type":"BlogPosting","dateModified":"2020-06-12T00:00:00-05:00","datePublished":"2020-06-12T00:00:00-05:00","image":"https://hallvagi.github.io/dl-explorer/images/some_folder/your_image.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://hallvagi.github.io/dl-explorer/nlp/fastai/ulmfit/sentiment/2020/06/12/ULMFiT_classifier.html"},"url":"https://hallvagi.github.io/dl-explorer/nlp/fastai/ulmfit/sentiment/2020/06/12/ULMFiT_classifier.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/dl-explorer/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://hallvagi.github.io/dl-explorer/feed.xml" title="Deep learning explorer" /><link rel="shortcut icon" type="image/x-icon" href="/dl-explorer/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Classification of movie review sentiment using ULMFiT | Deep learning explorer</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Classification of movie review sentiment using ULMFiT" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We test how well ULMFIiT manages to predict Norwegian language movie review using a fine-tuned language model." />
<meta property="og:description" content="We test how well ULMFIiT manages to predict Norwegian language movie review using a fine-tuned language model." />
<link rel="canonical" href="https://hallvagi.github.io/dl-explorer/nlp/fastai/ulmfit/sentiment/2020/06/12/ULMFiT_classifier.html" />
<meta property="og:url" content="https://hallvagi.github.io/dl-explorer/nlp/fastai/ulmfit/sentiment/2020/06/12/ULMFiT_classifier.html" />
<meta property="og:site_name" content="Deep learning explorer" />
<meta property="og:image" content="https://hallvagi.github.io/dl-explorer/images/some_folder/your_image.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-12T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"We test how well ULMFIiT manages to predict Norwegian language movie review using a fine-tuned language model.","headline":"Classification of movie review sentiment using ULMFiT","@type":"BlogPosting","dateModified":"2020-06-12T00:00:00-05:00","datePublished":"2020-06-12T00:00:00-05:00","image":"https://hallvagi.github.io/dl-explorer/images/some_folder/your_image.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://hallvagi.github.io/dl-explorer/nlp/fastai/ulmfit/sentiment/2020/06/12/ULMFiT_classifier.html"},"url":"https://hallvagi.github.io/dl-explorer/nlp/fastai/ulmfit/sentiment/2020/06/12/ULMFiT_classifier.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://hallvagi.github.io/dl-explorer/feed.xml" title="Deep learning explorer" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/dl-explorer/">Deep learning explorer</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/dl-explorer/about/">About</a><a class="page-link" href="/dl-explorer/search/">Search</a><a class="page-link" href="/dl-explorer/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Classification of movie review sentiment using ULMFiT</h1><p class="page-description">We test how well ULMFIiT manages to predict Norwegian language movie review using a fine-tuned language model.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-12T00:00:00-05:00" itemprop="datePublished">
        Jun 12, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      24 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/dl-explorer/categories/#NLP">NLP</a>
        &nbsp;
      
        <a class="category-tags-link" href="/dl-explorer/categories/#fastai">fastai</a>
        &nbsp;
      
        <a class="category-tags-link" href="/dl-explorer/categories/#ULMFiT">ULMFiT</a>
        &nbsp;
      
        <a class="category-tags-link" href="/dl-explorer/categories/#sentiment">sentiment</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/hallvagi/dl-explorer/tree/master/_notebooks/2020-06-12-ULMFiT_classifier.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/dl-explorer/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/hallvagi/dl-explorer/master?filepath=_notebooks%2F2020-06-12-ULMFiT_classifier.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/dl-explorer/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/hallvagi/dl-explorer/blob/master/_notebooks/2020-06-12-ULMFiT_classifier.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/dl-explorer/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Load-data-and-language-model">Load data and language model </a></li>
<li class="toc-entry toc-h1"><a href="#Setup-dataloader">Setup dataloader </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Inspect-a-batch">Inspect a batch </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Padding">Padding </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Creating-our-model">Creating our model </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Inspecting-the-model">Inspecting the model </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Looking-inside-our-model">Looking inside our model </a>
<ul>
<li class="toc-entry toc-h2"><a href="#What-does-the-SentenceEncoder-do?">What does the SentenceEncoder do? </a></li>
<li class="toc-entry toc-h2"><a href="#What-does-the-classifier-look-like?">What does the classifier look like? </a></li>
<li class="toc-entry toc-h2"><a href="#Making-a-prediction">Making a prediction </a></li>
<li class="toc-entry toc-h2"><a href="#A-quick-glance-at-the-weights">A quick glance at the weights </a></li>
<li class="toc-entry toc-h2"><a href="#What-are-the-frozen-parameter-groups?">What are the frozen parameter groups? </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Training-the-model">Training the model </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-12-ULMFiT_classifier.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai2.text.all</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Load-data-and-language-model">
<a class="anchor" href="#Load-data-and-language-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Load data and language model<a class="anchor-link" href="#Load-data-and-language-model"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this post we'll try to predict movie reviews from a Norwegian language dataset explored in this <a href="https://hallvagi.github.io/dl-explorer/nlp/fastai/2020/04/06/get-data.html">post</a>. It's taken a couple of steps to get here:</p>
<ul>
<li>first we <a href="https://hallvagi.github.io/dl-explorer/nlp/fastai/rnn/lstm/2020/04/17/AWD_LSTM.html">explored</a> the underlying AWD-LSTM architecture</li>
<li>then we <a href="https://hallvagi.github.io/dl-explorer/nlp/fastai/lstm/ulmfit/2020/04/20/ULMFiT_langmod.html">fine-tuned</a> a pretrained Norwegian language model</li>
<li>and tried to <a href="https://hallvagi.github.io/dl-explorer/nlp/fastai/lstm/ulmfit/2020/04/20/ULMFiT_langmod.html">interpret</a> what the model learn with dimensionality reduction techniques.</li>
</ul>
<p>In this post we want to do actual classification, the final step of the <a href="https://arxiv.org/abs/1801.06146">ULMFiT</a> method using <a href="https://dev.fast.ai/">fastai2</a> deep learning library.</p>
<p>First we'll grab the dataframe with reviews and labels from this <a href="https://hallvagi.github.io/dl-explorer/nlp/fastai/2020/04/06/get-data.html">post</a> (available directly from github), the vocabulary from the language model, and the encoder from the fine-tuned language model (see this <a href="https://hallvagi.github.io/dl-explorer/nlp/fastai/lstm/ulmfit/2020/04/20/ULMFiT_langmod.html">post</a>).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'https://raw.githubusercontent.com/hallvagi/dl-explorer/master/uploads/norec.csv'</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>filename</th>
      <th>rating</th>
      <th>title</th>
      <th>split</th>
      <th>sentiment</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>html/train/000000.html</td>
      <td>6</td>
      <td>Rome S02</td>
      <td>train</td>
      <td>positive</td>
      <td>Den andre og siste sesongen av Rome er ute på DVD i Norge. Om du så sesong 1, vet du at du har noe stort i vente. Har du aldri sett Rome før, stikk ut og kjøp begge sesongene. Dette er nemlig en av verdens beste tv-serier, og etter å ha sett de fire første episodene av sesong 2, konstaterer jeg at kvaliteten ser ut til å holde seg på et nesten overraskende høyt nivå! Sesong 2 starter nøyaktig der sesong 1 sluttet. Julius Cæsar ligger myrdet i Senatet og Lucius Vorenus hulker over liket av Neobie. Så blir historien enda mørkere. Marcus Antonius tar over styringen av Roma, men utfordres fra ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>html/train/000001.html</td>
      <td>6</td>
      <td>Twin Peaks - definitive gold box edition</td>
      <td>train</td>
      <td>positive</td>
      <td>Tv-serien Twin Peaks, skapt av David Lynch og Mark Frost, trollbandt publikum på starten av 1990-tallet. Nå er begge sesongene samlet på DVD i en såkalt ”definitive gold box edition” som viser at serien ikke har mistet noe av appellen. Det eneste som egentlig røper alderen, er at serien ikke er i widescreen, og at flere av skuespillerne fremdeles er unge og vakre. 17 år etter premieren har de falmet, som mennesker gjør, men Twin Peaks sikrer dem evig liv. Serien handler om et mordmysterium i den lille byen Twin Peaks, et sted langs USAs grense til Canada. Unge, vakre Laura Palmer blir funn...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>html/train/000002.html</td>
      <td>6</td>
      <td>The Wire (sesong 1-4)</td>
      <td>train</td>
      <td>positive</td>
      <td>I neste uke kommer sesong 5 av tv-serien ”The Wire” på DVD. 2008 har for meg vært sterkt preget av denne serien. Hjemme hos oss begynte vi med sesong 1 i vår. Da hadde jeg i lengre tid hørt panegyriske lovord om serien fra både venner og media. Vi ble også fanget av skildringene av purk og skurk i Baltimore, og pløyde oss igjennom alt til og med sesong 4 på sensommeren. Jeg vil ikke gå så langt som å kalle det ”verdens beste serie”, som noen har gjort, men det er ingen tvil om at dette er noe av det bedre som er blitt vist på tv! Serien forteller om en gruppe politietterforskere som samles...</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are mainly interested in the text, split and sentiment columns. Let's take a quick check for missing values:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 8613 entries, 0 to 8612
Data columns (total 6 columns):
 #   Column     Non-Null Count  Dtype 
---  ------     --------------  ----- 
 0   filename   8613 non-null   object
 1   rating     8613 non-null   int64 
 2   title      8613 non-null   object
 3   split      8613 non-null   object
 4   sentiment  8613 non-null   object
 5   text       8557 non-null   object
dtypes: int64(1), object(5)
memory usage: 403.9+ KB
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It seems there are a few missing values in our text columns, so let's get rid of them and reset the index:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll load the language model vocabulary <code>lm_itos.pkl</code> that we made in a previous post:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s1">'~/.fastai/data/norec/'</span><span class="p">)</span>
<span class="n">Path</span><span class="o">.</span><span class="n">BASE_PATH</span> <span class="o">=</span> <span class="n">path</span>
<span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">'models'</span><span class="p">)</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#8) [Path('models/finetuned_model.pth'),Path('models/norwegian_wgts.h5'),Path('models/norwegian_enc.pth'),Path('models/lm_itos.pkl'),Path('models/finetuned_encoder.pth'),Path('models/norwegian.zip'),Path('models/norwegian_enc.h5'),Path('models/norwegian_itos.pkl')]</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">'models/lm_itos.pkl'</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">itos</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">itos</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="n">itos</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:],</span> <span class="nb">len</span><span class="p">(</span><span class="n">itos</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(['xxunk', 'xxpad', '.', 'i', ',', 'og', '\n\n', 'av', 'som', 'en'],
 ['learning', 'initiativtager', 'forskningsleder', 'devils', 'graeme'],
 30002)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll load the encoder, <code>finetuned_encoder.pth</code>, at a later stage when our classifier is set up.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Setup-dataloader">
<a class="anchor" href="#Setup-dataloader" aria-hidden="true"><span class="octicon octicon-link"></span></a>Setup dataloader<a class="anchor-link" href="#Setup-dataloader"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before we can make a model we need a <strong>dataloader</strong>. The dataloader is responsible for keeping track of the data, labels and dataset splits among other. The data loader will then feed batches to our model during training. We will test the <a href="https://dev.fast.ai/tutorial.datablock">data block api</a>, instead of a simpler factory  method. The data block represents the mid-level of the fastai api and is a flexible way to set up our data loader. For the ickiest datasets, one might have to drop down the lowest level of the api.</p>
<p>A data block is basically an assembly line that we will send our data through. That mean we set up a string of functions that will take our data frame and extract the relevant information the data loader needs:</p>
<ol>
<li>define which types (block) our dependent and independent variables are. In our case that is text and category.</li>
<li>create a tokenizer and a vocabulary. We need to turn our text in to a numerical representation according to some vocabulary.</li>
<li>define how to get the text: in our case we read it from the dataframe column 'text'</li>
<li>define how to get the labels: in our case we'll read it form the dataframe column 'sentiment'</li>
<li>split the data in a train, validation and test dataset. In our case we have to locate the indexes of rows in the column 'split' depending on the value of split: 'train', 'dev' or 'test'.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>In fastai and kaggle lingo the <strong>test set</strong> is an unlabeled dataset on which we test our model. In our case, though, we also have labels for the test set. So we can assess the performance on both the validation and test set. 
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First let's set up a basic function that splits the data according to which split it belongs to. The datablock expects a set of indexes for the training, validation and test dataset:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">train_idx</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'split'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
    <span class="n">valid_idx</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'split'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'dev'</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
    <span class="n">test_idx</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">'split'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">L</span><span class="p">(</span><span class="n">train_idx</span><span class="p">),</span> <span class="n">L</span><span class="p">(</span><span class="n">valid_idx</span><span class="p">),</span> <span class="n">L</span><span class="p">(</span><span class="n">test_idx</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we pass our <code>df</code> through this function it simply returns the indexes of the various splits:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">split</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((#6863) [0,1,2,3,4,5,6,7,8,9...],
 (#876) [136,137,138,140,141,142,143,144,145,146...],
 (#818) [183,184,186,187,188,189,190,191,192,193...])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can use the built-in class 'ColReader' to read the actual texts and labels. The ColReader class has a <strong>__call__</strong> method that reads a particular column from a data frame that is passed:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reader</span> <span class="o">=</span> <span class="n">ColReader</span><span class="p">(</span><span class="n">cols</span><span class="o">=</span><span class="s1">'sentiment'</span><span class="p">)</span>
<span class="n">reader</span><span class="p">(</span><span class="n">df</span><span class="p">)[:</span><span class="mi">3</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0    positive
1    positive
2    positive
Name: sentiment, dtype: object</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now have all we need to set up our data block:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>The seq_len has to be the same as we trained our language model with!
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">reviews</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">TextBlock</span><span class="o">.</span><span class="n">from_df</span><span class="p">(</span><span class="n">text_cols</span><span class="o">=</span><span class="s1">'text'</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">itos</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="mi">72</span><span class="p">),</span> <span class="n">CategoryBlock</span><span class="p">),</span>
                    <span class="n">get_x</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">'text'</span><span class="p">),</span>
                    <span class="n">get_y</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">'sentiment'</span><span class="p">),</span>
                    <span class="n">splitter</span><span class="o">=</span><span class="n">split</span>
                   <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A grate way of debugging your data block with is the <code>summary(df)</code> method. It's takes you through all the steps in the pipeline. The output is very long though, so I left it out in this notebook.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#reviews.summary(df)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally let's create the actual data loader from our source dataframe:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">reviews</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>xxbos html , body { border : xxunk ; } — xxunk det enkle er grunnlaget for all \n xxunk , sa xxunk xxunk xxunk xxunk til xxunk xxunk xxunk xxunk i 1923 . xxunk filmen « xxunk xxunk xxunk \n &amp; xxunk igor xxunk xxunk » , som handler om xxunk xxunk korte xxunk med den \n russiske komponisten xxunk igor xxunk xxunk , baserer seg på samme dekret . xxunk xxunk selv \n skjærer som en mørk xxunk inn i scenene , med sine smale , svarte antrekk . xxunk store \n deler av filmen foregår på xxunk xxunk landsted , dit den velstående xxunk \n inviterer den fattige komponisten og familien hans for at han skal få jobbe , og \n de svarte og hvite linjene i xxunk xxunk xxunk xxunk seg xxunk gjennom regissør \n xxunk jan xxunk xxunk bilder slik xxunk xxunk selv skjærer gjennom</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>1</th>
      <td>xxbos xxunk vakre , xxunk xxunk christine xxunk brown sover trygt i sin xxunk xxunk , med kjæresten ved sin side . xxunk ei flue flyr inn gjennom vinduet , summer xxunk gjennom rommet og lander på xxunk , før den setter kursen mot vår blonde xxunk . xxunk musikken er xxunk og xxunk . xxunk xxunk likeså og xxunk inn i xxunk browns ene xxunk . xxunk og ut igjen fra det andre . xxunk før den forsvinner inn i xxunk hennes og vekker henne til hennes livs verste mareritt , ei xxunk , xxunk xxunk , som xxunk over henne med brune xxunk , xxunk for å bite henne til døde . xxup scenen xxup fra « drag xxunk me to xxunk hell » representerer det meste xxunk sam xxunk xxunk står for som filmskaper . xxunk den er xxunk , xxunk og preget av humor . xxunk</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>2</th>
      <td>xxbos xxunk lawrence of xxunk arabia er en film for de store lerret . xxunk den bør helst oppleves i en xxunk , fortrinnsvis i sitt opprinnelige 70 mm format . xxunk men siden det er en xxunk i 2012 , er den nye xxunk blu - ray - utgivelsen det nest beste . xxunk den er faktisk dobbelt restaurert . xxunk den digitale xxunk er nemlig basert på en xxunk xxunk fra 1988 , da den også ble rekonstruert til sin opprinnelige lengde . xxunk og på xxunk blu - ray har xxunk lawrence of xxunk arabia en fantastisk klarhet og dybde som forsterker følelsen av xxunk xxunk . xxunk det er helt utrolig at en 50 år gammel film kan se så bra ut ! xxunk david xxunk xxunk mesterverk skildrer en mann som lar sin indre kamp komme til uttrykk i ytre handlinger . xxunk når en</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>3</th>
      <td>xxbos xxunk 3 | • xxunk dagbladets reporter xxunk i xxunk cannes . xxunk les hans rapport her . xxunk xxup film : xxunk vi har lest om xxunk og \n skandale . xxunk om sex , blod , xxunk og vold . xxunk om xxunk av xxunk og \n xxunk xxunk som xxunk blod . xxunk om journalister som har xxunk og \n kritikere som har slaktet og xxunk om hverandre . xxunk xxunk for en tid tilbake befant regissør von xxunk trier seg i en dyp \n depresjon . xxunk den svært xxunk dansken har fortalt om psykiske problemer før , \n men ifølge ham selv var tidligere depresjoner ingen ting sammenliknet med mørket \n som rammet ham nå . i lang tid var han ikke i stand til å gjøre annet enn å ligge \n og xxunk tomt ut i lufta . xxunk som en form for</td>
      <td>positive</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that our data block <code>reviews</code> is specific to the source we plan to use it with. If we instead pass the underlying numpy values of the dataframe to it, the reader and splitter functions won't work:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#reviews.dataloaders(df.values)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Inspect-a-batch">
<a class="anchor" href="#Inspect-a-batch" aria-hidden="true"><span class="octicon octicon-link"></span></a>Inspect a batch<a class="anchor-link" href="#Inspect-a-batch"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What does our data actually look like? Grabbing a batch and inspecting it is often a useful way of understanding what the data actually looks like for our model:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">yb</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(torch.Size([64, 2990]), torch.Size([64]))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The dependent variable xb represents the numericalized text of the reviews. We have a batch size of 64 and the longest text in this particular batch has a length of 2990 tokens - but this will vary from batch to batch. We can get the vocab from our data loader, <code>dls</code>, to check what the numbers represent. We have two vocabs, one for the text and one for the labels of our data:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span><span class="p">,</span> <span class="n">classes</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">vocab</span>
<span class="n">tokens</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">classes</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(['xxunk', 'xxpad', '.', 'i', ','], (#2) ['negative','positive'])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's first grab a few numericalized tokens from our batch:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">nums</span> <span class="o">=</span> <span class="n">xb</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">10</span><span class="p">:</span><span class="mi">15</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">nums</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([2631,    0,   18, 2770,   10])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can use the fastai L-class to look up those indexes in the vocab. Remember that token 2631 is simply the token at index 2631 in our vocab:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span><span class="p">[</span><span class="mi">2631</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>'—'</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">L</span><span class="p">(</span><span class="n">tokens</span><span class="p">)[</span><span class="n">nums</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#5) ['—','xxunk','det','enkle','er']</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>L is a fastai version of the python List datatype with some added functionality such as slicing from a list of indexes
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our dependent variable is as expected, either positive or negative. Once again we can use the vocab to get the actual classes. 1 is positive and 0 is negative:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">yb</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">L</span><span class="p">(</span><span class="n">classes</span><span class="p">)[</span><span class="n">yb</span><span class="p">[:</span><span class="mi">5</span><span class="p">]]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([1, 1, 0, 1, 1]),
 (#5) ['positive','positive','negative','positive','positive'])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Padding">
<a class="anchor" href="#Padding" aria-hidden="true"><span class="octicon octicon-link"></span></a>Padding<a class="anchor-link" href="#Padding"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The longest text in the batch decides the shape of the enitre batch. What happens with the shorter texts? They are padded with our padding token, in our case token id 1:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>'xxpad'</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we look at the final text in the batch we see that it's padded on both sides:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">xb</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="nb">len</span><span class="p">(</span><span class="n">xb</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([1, 1, 1,  ..., 1, 1, 1]), 2990)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can count the padding of each text in the batch:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">xb</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>TensorText([   0,   94,  404, 1179, 1352, 1374, 1376, 1388, 1405, 1434, 1499, 1513,
        1563, 1580, 1615, 1617, 1617, 1634, 1687, 1717, 1730, 1743, 1763, 1793,
        1794, 1813, 1851, 1870, 1893, 1893, 1895, 1900, 1951, 1980, 1984, 1986,
        1991, 1996, 2006, 2010, 2014, 2014, 2023, 2032, 2055, 2067, 2072, 2079,
        2081, 2082, 2091, 2091, 2117, 2118, 2128, 2131, 2136, 2140, 2143, 2144,
        2147, 2159, 2164, 2165])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We see the longest review is placed first, and the batch has progressively shorter texts.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Creating-our-model">
<a class="anchor" href="#Creating-our-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Creating our model<a class="anchor-link" href="#Creating-our-model"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Setting up the model is pretty straight forward, but since our language model had a hidden size of 1150, we have to change the default in the config dictionary. Note also that we pass <code>pretrained=False</code>. We'll load our pretrained encoder manually instead.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">awd_lstm_clas_config</span><span class="p">[</span><span class="s1">'n_hid'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1150</span>

<span class="n">learn</span> <span class="o">=</span> <span class="n">text_classifier_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">arch</span><span class="o">=</span><span class="n">AWD_LSTM</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">,</span> 
                                <span class="n">config</span><span class="o">=</span><span class="n">awd_lstm_clas_config</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">to_fp16</span><span class="p">()</span>

<span class="n">learn</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">'models/finetuned_encoder'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;fastai2.text.learner.TextLearner at 0x7fb1d16c5350&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Inspecting-the-model">
<a class="anchor" href="#Inspecting-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Inspecting the model<a class="anchor-link" href="#Inspecting-the-model"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When looking at the model we can see that the first layers of the model are <strong>frozen</strong>, i.e. the trainable column says False. It's only the randomly initialized linear classification layers that are trainable. This makes sense. We first want to calibrate these layers as much as possible before we proceed to fine tune the language model. We'll come back to freezing and unfreezing of the model later in the post.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SequentialRNN (Input shape: ['64 x 2990'])
================================================================
Layer (type)         Output Shape         Param #    Trainable 
================================================================
RNNDropout           64 x 38 x 400        0          False     
________________________________________________________________
RNNDropout           64 x 38 x 1150       0          False     
________________________________________________________________
RNNDropout           64 x 38 x 1150       0          False     
________________________________________________________________
BatchNorm1d          64 x 1200            2,400      True      
________________________________________________________________
Dropout              64 x 1200            0          False     
________________________________________________________________
Linear               64 x 50              60,000     True      
________________________________________________________________
ReLU                 64 x 50              0          False     
________________________________________________________________
BatchNorm1d          64 x 50              100        True      
________________________________________________________________
Dropout              64 x 50              0          False     
________________________________________________________________
Linear               64 x 2               100        True      
________________________________________________________________

Total params: 62,600
Total trainable params: 62,600
Total non-trainable params: 0

Optimizer used: &lt;function Adam at 0x7fb1fc4a0f80&gt;
Loss function: FlattenedLoss of CrossEntropyLoss()

Model frozen up to parameter group number 4

Callbacks:
  - ModelReseter
  - RNNRegularizer
  - ModelToHalf
  - TrainEvalCallback
  - Recorder
  - ProgressCallback
  - MixedPrecision</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We also recognize most of the other dimensions in the architecture:</p>
<ul>
<li>batch size of 64</li>
<li>embedding size of 400</li>
<li>hidden size of LSTMs is 1150</li>
<li>1200 and 50 output channels from the linear classifier layers</li>
<li>but why is seq_len 38? Didn't we set it to 72 for the data loader? We'll investigate this later in the post.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SequentialRNN(
  (0): SentenceEncoder(
    (module): AWD_LSTM(
      (encoder): Embedding(30002, 400, padding_idx=1)
      (encoder_dp): EmbeddingDropout(
        (emb): Embedding(30002, 400, padding_idx=1)
      )
      (rnns): ModuleList(
        (0): WeightDropout(
          (module): LSTM(400, 1150, batch_first=True)
        )
        (1): WeightDropout(
          (module): LSTM(1150, 1150, batch_first=True)
        )
        (2): WeightDropout(
          (module): LSTM(1150, 400, batch_first=True)
        )
      )
      (input_dp): RNNDropout()
      (hidden_dps): ModuleList(
        (0): RNNDropout()
        (1): RNNDropout()
        (2): RNNDropout()
      )
    )
  )
  (1): PoolingLinearClassifier(
    (layers): Sequential(
      (0): LinBnDrop(
        (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Dropout(p=0.2, inplace=False)
        (2): Linear(in_features=1200, out_features=50, bias=False)
        (3): ReLU(inplace=True)
      )
      (1): LinBnDrop(
        (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Dropout(p=0.1, inplace=False)
        (2): Linear(in_features=50, out_features=2, bias=False)
      )
    )
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Looking-inside-our-model">
<a class="anchor" href="#Looking-inside-our-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Looking inside our model<a class="anchor-link" href="#Looking-inside-our-model"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before we do the actual classification, let's have a look at what the model actually does at each step.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-does-the-SentenceEncoder-do?">
<a class="anchor" href="#What-does-the-SentenceEncoder-do?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What does the SentenceEncoder do?<a class="anchor-link" href="#What-does-the-SentenceEncoder-do?"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In short, the SentenceEncoder takes text and outputs a vector representation of it. Let's split the model into the SentenceEncoder and the classifier and inspect both:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">enc</span><span class="p">,</span> <span class="n">lin</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">children</span><span class="p">()</span>
<span class="n">enc</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SentenceEncoder(
  (module): AWD_LSTM(
    (encoder): Embedding(30002, 400, padding_idx=1)
    (encoder_dp): EmbeddingDropout(
      (emb): Embedding(30002, 400, padding_idx=1)
    )
    (rnns): ModuleList(
      (0): WeightDropout(
        (module): LSTM(400, 1150, batch_first=True)
      )
      (1): WeightDropout(
        (module): LSTM(1150, 1150, batch_first=True)
      )
      (2): WeightDropout(
        (module): LSTM(1150, 400, batch_first=True)
      )
    )
    (input_dp): RNNDropout()
    (hidden_dps): ModuleList(
      (0): RNNDropout()
      (1): RNNDropout()
      (2): RNNDropout()
    )
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">enc</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SentenceEncoder (Input shape: ['64 x 2990'])
================================================================
Layer (type)         Output Shape         Param #    Trainable 
================================================================
RNNDropout           64 x 38 x 400        0          False     
________________________________________________________________
RNNDropout           64 x 38 x 1150       0          False     
________________________________________________________________
RNNDropout           64 x 38 x 1150       0          False     
________________________________________________________________

Total params: 0
Total trainable params: 0
Total non-trainable params: 0
</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we see a seq_len of 38 again. This is an artifact of the model summary which is specific to the batch. The LSTM processes a batch in chunks of 72 - the actual seq_len. The summary shows the final chunk, where it simply happens to be 38 items left:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="mi">2990</span><span class="o">%</span><span class="k">72</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>38</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What comes out of the encoder if we pass it a batch of data?</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">enc_x</span> <span class="o">=</span> <span class="n">enc</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="p">[</span><span class="n">e</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">enc_x</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[torch.Size([64, 1406, 400]), torch.Size([64, 1406])]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The SentenceEncoder outputs two things, and if we check the source code, <code>SentenceEncoder??</code>, we find that it returns: <code>return outs,mask</code>.</p>
<p>The first output is our encoded text. The shape out the output is bs, len, embedding size. But where does the 1406 size come
from? This is also specific to the batch we pass in. The get_text_classifier is called with a default max_len of 72*20 = 1440. So the SentenceEncoder encodes the final sequence of tokens up to a maximum length of 1440. It also makes sure to include the last sequence of the batch:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>SentenceEncoder<span class="o">??</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our batch has size 2990, much longer than the maximum. So we will fill up 19 chunks of seq_len 72 and then add the remainder of 38 of the final chunk as the last part of the encoded sentence:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="mi">2990</span><span class="o">%</span><span class="k">72</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>38</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So 19 full chunks and the remainder gives us:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="mi">72</span><span class="o">*</span><span class="mi">19</span> <span class="o">+</span> <span class="mi">38</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>1406</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>mask</code> output is a padding mask, that tells us where the padding tokens are in the processed text:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">enc_x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([  0,  22,  44,  27,  56,   6,   8,  20,  37,  66,  59,   1,  51,  68,
         31,  33,  33,  50, 103, 133, 146, 159, 179, 209, 210, 229, 267, 286,
        309, 309, 311, 316, 367, 396, 400, 402, 407, 412, 422, 426, 430, 430,
        439, 448, 471, 483, 488, 495, 497, 498, 507, 507, 533, 534, 544, 547,
        552, 556, 559, 560, 563, 575, 580, 581])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we take the final 1406 tokens of our batch we get the same sum of padding tokens:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">xb</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1406</span><span class="p">:]</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([  0,  22,  44,  27,  56,   6,   8,  20,  37,  66,  59,   1,  51,  68,
         31,  33,  33,  50, 103, 133, 146, 159, 179, 209, 210, 229, 267, 286,
        309, 309, 311, 316, 367, 396, 400, 402, 407, 412, 422, 426, 430, 430,
        439, 448, 471, 483, 488, 495, 497, 498, 507, 507, 533, 534, 544, 547,
        552, 556, 559, 560, 563, 575, 580, 581])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-does-the-classifier-look-like?">
<a class="anchor" href="#What-does-the-classifier-look-like?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What does the classifier look like?<a class="anchor-link" href="#What-does-the-classifier-look-like?"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The classifier takes the encoded sentence and produces a binary prediction:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lin</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">enc_x</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>PoolingLinearClassifier (Input shape: ["['64 x 1406 x 400', '64 x 1406']"])
================================================================
Layer (type)         Output Shape         Param #    Trainable 
================================================================
BatchNorm1d          64 x 1200            2,400      True      
________________________________________________________________
Dropout              64 x 1200            0          False     
________________________________________________________________
Linear               64 x 50              60,000     True      
________________________________________________________________
ReLU                 64 x 50              0          False     
________________________________________________________________
BatchNorm1d          64 x 50              100        True      
________________________________________________________________
Dropout              64 x 50              0          False     
________________________________________________________________
Linear               64 x 2               100        True      
________________________________________________________________

Total params: 62,600
Total trainable params: 62,600
Total non-trainable params: 0
</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we pass the encoded batch through the model, we get a binary output for each item in our batch:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lin</span><span class="p">(</span><span class="n">enc_x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([64, 2])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It' not entirely clear how we go from from the 1406 dimension input to the first batchnorm layer that expects 1200 size input. But if we check the source code of the <code>PoolingLinearClassifier??</code> module it indeed has a <code>masked_concat_pool</code> in it's forward method which changes the dimensionality of the input.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Making-a-prediction">
<a class="anchor" href="#Making-a-prediction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Making a prediction<a class="anchor-link" href="#Making-a-prediction"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's pass the batch we grabbed above through the entire model:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
<span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">preds</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[torch.Size([64, 2]), torch.Size([64, 1406, 400]), torch.Size([64, 1406, 400])]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our model produces three things. The binary predictions for each item in the batch and the encoded sentences. Note that the two encoded batches of sentences are identical:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">!=</span><span class="n">preds</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since the model is untrained at this point, the prediction should be random. If we take the softmax of the final dimension we see that the model is mostly 50-50 on every prediction - so just a random guess at this point!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.4973, 0.5027],
        [0.4912, 0.5088],
        [0.4805, 0.5195],
        [0.4879, 0.5121],
        [0.5008, 0.4992]], grad_fn=&lt;CopyBackwards&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="A-quick-glance-at-the-weights">
<a class="anchor" href="#A-quick-glance-at-the-weights" aria-hidden="true"><span class="octicon octicon-link"></span></a>A quick glance at the weights<a class="anchor-link" href="#A-quick-glance-at-the-weights"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's also take a quick glance at the various weights of the model: If we plot one of the weight matrices it's clear that the above weights matrix has been randomly and uniformly initialized, and thus has not been trained yet.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">to_np</span><span class="p">(</span><span class="n">lin</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="s1">'layers.0.2.weight'</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">());</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we instead check the one of the encoder weights, we get a completely different pattern. Most weights a centered around 0:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">to_np</span><span class="p">(</span><span class="n">enc</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="s1">'module.rnns.1.module.weight_ih_l0'</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-are-the-frozen-parameter-groups?">
<a class="anchor" href="#What-are-the-frozen-parameter-groups?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What are the frozen parameter groups?<a class="anchor-link" href="#What-are-the-frozen-parameter-groups?"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Fastai operates with a concept of frozen parameter groups. I.e. the model is split into groups which can be made trainable or not. We can check the status of the models parameter groups:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">),</span> <span class="n">learn</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">frozen_idx</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(5, 4)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That is a total of 5 parameter groups and currently the first 4 is frozen. This corresponds with what we saw from the model summary - only the top classifier layer was trainable. During training we can unfreeze parts of the model:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">),</span> <span class="n">learn</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">frozen_idx</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(None, 2)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's reset the model before we train it:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">learn</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">frozen_idx</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(None, 4)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The parameter groups aren't named, but if we check the shape of the weights from each group we recognize the various layers:</p>
<ol>
<li>The embedding layer  </li>
<li>LSTM 1</li>
<li>LSTM 2</li>
<li>LSTM 3</li>
<li>Linear classifier</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[</span><span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'params'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">learn</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[torch.Size([30002, 400]),
 torch.Size([4600, 1150]),
 torch.Size([4600, 1150]),
 torch.Size([1600, 400]),
 torch.Size([1200])]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Training-the-model">
<a class="anchor" href="#Training-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training the model<a class="anchor-link" href="#Training-the-model"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's high time to actually train the model. We'll stick with standard fastai procedure for fine tuning a language sentiment classifier:</p>
<ul>
<li>lr_find() to find a sensible learning rate</li>
<li>train with the one cycle policy</li>
<li>gradual unfreezing of the layers - train the top layers first</li>
<li>discriminative learning rates. That is train the lower layers with a smaller learning rate than the top layers.</li>
</ul>
<p>This procedure is well known from the <a href="https://course.fast.ai/">course</a> and <a href="https://dev.fast.ai/tutorial.text#Fine-tuning-a-language-model-on-IMDb">documentation</a>. It's also a very robust method in my experience. It seem to just work for mosts datasets.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(lr_min=0.010000000149011612, lr_steep=0.0063095735386013985)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This pattern is typical for untrained networks: There is a part of the graph which shows substantial lower loss than the other parts. This makes sense since changing random weights should give us substantial improvements pretty fast. We'll start with a learning rate of 1e-2 and train for a few epochs. Note: We are only training the top layers in this step:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-2</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.619132</td>
      <td>0.507724</td>
      <td>0.752283</td>
      <td>00:24</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.579055</td>
      <td>0.467570</td>
      <td>0.768265</td>
      <td>00:23</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.540422</td>
      <td>0.468025</td>
      <td>0.779680</td>
      <td>00:24</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We're already at nearly 78% accuracy, and we haven't touched the actual language model yet! Let's unfreeze the first LSTM and run LR-find one more time:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=3.0199516913853586e-05)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This pattern is what you would expect from a model that has trained for a while. There are no more random weights left anymore, and further progress is much harder to find. We'll set a lower base learning rate for the remainder of the training, and slice this with a scaling factor. This means that the lowest parameter groups gets a lower learning rate, and the top layers a higher. When we slice we'll use the <strong>magic scaling factor* of 2.6</strong>4 ~ 45. The source of this scaling factor is fastai guru <a href="https://twitter.com/jeremyphoward">Jeremy Howard</a>, which found it to empirically work well. I've set this to 50 for sake of simplicity.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">scaling</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="n">lr</span><span class="o">/</span><span class="n">scaling</span><span class="p">,</span> <span class="n">lr</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.519882</td>
      <td>0.468881</td>
      <td>0.779680</td>
      <td>00:29</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We continue to unfreeze the next layer, and train a bit more:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="n">lr</span><span class="o">/</span><span class="n">scaling</span><span class="p">,</span><span class="n">lr</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.468425</td>
      <td>0.448788</td>
      <td>0.797945</td>
      <td>00:43</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our model is gradually improving, and we're at 8 % accuracy. Finally we unfreeze the entire model and train a final few epochs:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="n">lr</span><span class="o">/</span><span class="n">scaling</span><span class="p">,</span><span class="n">lr</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.440907</td>
      <td>0.427563</td>
      <td>0.809361</td>
      <td>00:56</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.374653</td>
      <td>0.418673</td>
      <td>0.819635</td>
      <td>00:54</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.329612</td>
      <td>0.423127</td>
      <td>0.831050</td>
      <td>00:51</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.279565</td>
      <td>0.419359</td>
      <td>0.828767</td>
      <td>00:57</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.262696</td>
      <td>0.429828</td>
      <td>0.825342</td>
      <td>00:55</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We end up with around 83% validation accuracy in our best epoch. This is much better than the 50-50 we would have gotten from a random guess:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">'sentiment'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>negative    0.511044
positive    0.488956
Name: sentiment, dtype: float64</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But is our model really a good one? This is kind of difficult to say, since I haven't seen any results for this particular dataset, and we don't have any other models as baselines to compare with. The English IMDb dataset gets over <a href="https://paperswithcode.com/sota/sentiment-analysis-on-imdb">95% accuracy</a> with ULMFiT. But this task likely has access to a better language model and has 10x more data. On the other hand, the reviews considered in this dataset are written by relatively few journalists, and we would maybe expect their writing style to be more consistent. Finally it should also be noted that our sentiment classes probably are closer than in the IMDb example. That is, we based negative on a rating of 1-3 of 6, and positive as 5-6. The IMDb reviews are more polarized, so the task is perhaps a bit easier.</p>
<blockquote>
<p>Note that we also have a test set for this model, so this is the actual data set to score our model on</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We already passed the test dataset to our dataloader, so now we can access it directly. The test set has index 2 (0 is train, and 1 is validation). So we can just validate with this data to get the score since we're not actually interested in the particular predictions at this point.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="n">ds_idx</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#2) [0.42844530940055847,0.8227384090423584]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The test set accuracy is the same as the validation set accuracy. This is kind of what we hoped for. That is, if there was larger differences, we would have to go back to our model to see if we did something wrong, or if the test set was sampled from a different distribution.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A finally important point is to investigate the predictions of the model. This could increase our understanding of what the model actually learns, discover data leaks, and in general increase our confidence in the model. We'll come back to model interpretation in a later post!</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="hallvagi/dl-explorer"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/dl-explorer/nlp/fastai/ulmfit/sentiment/2020/06/12/ULMFiT_classifier.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/dl-explorer/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/dl-explorer/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/dl-explorer/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Exploring deep learning and data science concepts.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/hallvagi" target="_blank" title="hallvagi"><svg class="svg-icon grey"><use xlink:href="/dl-explorer/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/HallvarGisnas" target="_blank" title="HallvarGisnas"><svg class="svg-icon grey"><use xlink:href="/dl-explorer/assets/minima-social-icons.svg#twitter"></use></svg></a></li><li><a rel="me" href="https://www.youtube.com/c%2FHallvarGisn%C3%A5s" target="_blank" title="c/HallvarGisnås"><svg class="svg-icon grey"><use xlink:href="/dl-explorer/assets/minima-social-icons.svg#youtube"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
